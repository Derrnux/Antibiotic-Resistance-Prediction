{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Out and Compare Models based on predefined Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup Overhead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This was run using the following split:\n",
    "- Test: 10/10\n",
    "- Val: 10/10\n",
    "- Train: 20/90  <- few 0 examples, bad generalisation capability???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_manager import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_seed = 44\n",
    "best_model = None\n",
    "best_model_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training and Comparison of Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10.0}, '1': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 10.0}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 20.0}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 20.0}}\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 1, attention_layers = 3, batch_size = 32,  balancing_method = 'none', embed_dim = 128, dim_feedforward = 2048, seed = global_seed)\n",
    "model_score = custom_model.evaluate()\n",
    "if (model_score > best_model_score):\n",
    "    best_model = custom_model\n",
    "    best_model_score = model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.0580\n",
      "Epoch 2/10, Loss: 3.0018\n",
      "Epoch 3/10, Loss: 2.7102\n",
      "Epoch 4/10, Loss: 2.7868\n",
      "Epoch 5/10, Loss: 2.7215\n",
      "Epoch 6/10, Loss: 2.6784\n",
      "Epoch 7/10, Loss: 2.6964\n",
      "Epoch 8/10, Loss: 2.6495\n",
      "Epoch 9/10, Loss: 2.5883\n",
      "Epoch 10/10, Loss: 2.6234\n",
      "{'0': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 10.0}, '1': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 10.0}, 'accuracy': 0.8, 'macro avg': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 20.0}, 'weighted avg': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 20.0}}\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 1, attention_layers = 3, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 128, dim_feedforward = 2048, seed = global_seed)\n",
    "custom_model.train(epochs = 10, learning_rate = 1e-4)\n",
    "model_score = custom_model.evaluate()\n",
    "if (model_score > best_model_score):\n",
    "    best_model = custom_model\n",
    "    best_model_score = model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.9484\n",
      "Epoch 2/50, Loss: 2.8928\n",
      "Epoch 3/50, Loss: 2.7602\n",
      "Epoch 4/50, Loss: 2.7736\n",
      "Epoch 5/50, Loss: 2.7492\n",
      "Epoch 6/50, Loss: 2.7164\n",
      "Epoch 7/50, Loss: 2.7257\n",
      "Epoch 8/50, Loss: 2.6969\n",
      "Epoch 9/50, Loss: 2.6521\n",
      "Epoch 10/50, Loss: 2.6761\n",
      "Epoch 11/50, Loss: 2.6506\n",
      "Epoch 12/50, Loss: 2.6222\n",
      "Epoch 13/50, Loss: 2.6679\n",
      "Epoch 14/50, Loss: 2.6257\n",
      "Epoch 15/50, Loss: 2.5982\n",
      "Epoch 16/50, Loss: 2.6037\n",
      "Epoch 17/50, Loss: 2.6262\n",
      "Epoch 18/50, Loss: 2.6276\n",
      "Epoch 19/50, Loss: 2.6103\n",
      "Epoch 20/50, Loss: 2.5439\n",
      "Epoch 21/50, Loss: 2.6175\n",
      "Epoch 22/50, Loss: 2.5378\n",
      "Epoch 23/50, Loss: 2.5551\n",
      "Epoch 24/50, Loss: 2.6460\n",
      "Epoch 25/50, Loss: 2.5130\n",
      "Epoch 26/50, Loss: 2.5934\n",
      "Epoch 27/50, Loss: 2.5613\n",
      "Epoch 28/50, Loss: 2.6663\n",
      "Epoch 29/50, Loss: 2.5625\n",
      "Epoch 30/50, Loss: 2.7337\n",
      "Epoch 31/50, Loss: 2.5666\n",
      "Epoch 32/50, Loss: 2.6563\n",
      "Epoch 33/50, Loss: 2.7175\n",
      "Epoch 34/50, Loss: 2.5812\n",
      "Epoch 35/50, Loss: 2.6377\n",
      "Epoch 36/50, Loss: 2.6036\n",
      "Epoch 37/50, Loss: 2.6595\n",
      "Epoch 38/50, Loss: 2.5320\n",
      "Epoch 39/50, Loss: 2.6824\n",
      "Epoch 40/50, Loss: 2.5350\n",
      "Epoch 41/50, Loss: 2.6285\n",
      "Epoch 42/50, Loss: 2.4994\n",
      "Epoch 43/50, Loss: 2.5769\n",
      "Epoch 44/50, Loss: 2.4897\n",
      "Epoch 45/50, Loss: 2.4851\n",
      "Epoch 46/50, Loss: 2.5929\n",
      "Epoch 47/50, Loss: 2.5108\n",
      "Epoch 48/50, Loss: 2.4783\n",
      "Epoch 49/50, Loss: 2.6507\n",
      "Epoch 50/50, Loss: 2.4833\n",
      "{'0': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 10.0}, '1': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 10.0}, 'accuracy': 0.8, 'macro avg': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 20.0}, 'weighted avg': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 20.0}}\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 3, attention_layers = 3, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 128, dim_feedforward = 2048, seed = global_seed)\n",
    "custom_model.train(epochs = 50, learning_rate = 1e-4)\n",
    "model_score = custom_model.evaluate()\n",
    "if (model_score > best_model_score):\n",
    "    best_model = custom_model\n",
    "    best_model_score = model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 3.2191\n",
      "Epoch 2/200, Loss: 3.0933\n",
      "Epoch 3/200, Loss: 2.8493\n",
      "Epoch 4/200, Loss: 2.7714\n",
      "Epoch 5/200, Loss: 2.7452\n",
      "Epoch 6/200, Loss: 2.7585\n",
      "Epoch 7/200, Loss: 2.7433\n",
      "Epoch 8/200, Loss: 2.6618\n",
      "Epoch 9/200, Loss: 2.7879\n",
      "Epoch 10/200, Loss: 2.6852\n",
      "Epoch 11/200, Loss: 2.6829\n",
      "Epoch 12/200, Loss: 2.7201\n",
      "Epoch 13/200, Loss: 2.6190\n",
      "Epoch 14/200, Loss: 2.5974\n",
      "Epoch 15/200, Loss: 2.5890\n",
      "Epoch 16/200, Loss: 2.6880\n",
      "Epoch 17/200, Loss: 2.6116\n",
      "Epoch 18/200, Loss: 2.5341\n",
      "Epoch 19/200, Loss: 2.6172\n",
      "Epoch 20/200, Loss: 2.5570\n",
      "Epoch 21/200, Loss: 2.5942\n",
      "Epoch 22/200, Loss: 2.6176\n",
      "Epoch 23/200, Loss: 2.6505\n",
      "Epoch 24/200, Loss: 2.5401\n",
      "Epoch 25/200, Loss: 2.7468\n",
      "Epoch 26/200, Loss: 2.6125\n",
      "Epoch 27/200, Loss: 2.6457\n",
      "Epoch 28/200, Loss: 2.7025\n",
      "Epoch 29/200, Loss: 2.5192\n",
      "Epoch 30/200, Loss: 2.5323\n",
      "Epoch 31/200, Loss: 2.6807\n",
      "Epoch 32/200, Loss: 2.6751\n",
      "Epoch 33/200, Loss: 2.5770\n",
      "Epoch 34/200, Loss: 2.6545\n",
      "Epoch 35/200, Loss: 2.7030\n",
      "Epoch 36/200, Loss: 2.4753\n",
      "Epoch 37/200, Loss: 2.6473\n",
      "Epoch 38/200, Loss: 2.5925\n",
      "Epoch 39/200, Loss: 2.5128\n",
      "Epoch 40/200, Loss: 2.6008\n",
      "Epoch 41/200, Loss: 2.4624\n",
      "Epoch 42/200, Loss: 2.5369\n",
      "Epoch 43/200, Loss: 2.5181\n",
      "Epoch 44/200, Loss: 2.6313\n",
      "Epoch 45/200, Loss: 2.4381\n",
      "Epoch 46/200, Loss: 2.4327\n",
      "Epoch 47/200, Loss: 2.5575\n",
      "Epoch 48/200, Loss: 2.4067\n",
      "Epoch 49/200, Loss: 2.5687\n",
      "Epoch 50/200, Loss: 2.5684\n",
      "Epoch 51/200, Loss: 2.5866\n",
      "Epoch 52/200, Loss: 2.4510\n",
      "Epoch 53/200, Loss: 2.5572\n",
      "Epoch 54/200, Loss: 2.4489\n",
      "Epoch 55/200, Loss: 2.4405\n",
      "Epoch 56/200, Loss: 2.4506\n",
      "Epoch 57/200, Loss: 2.4283\n",
      "Epoch 58/200, Loss: 2.4455\n",
      "Epoch 59/200, Loss: 2.5252\n",
      "Epoch 60/200, Loss: 2.5354\n",
      "Epoch 61/200, Loss: 2.4214\n",
      "Epoch 62/200, Loss: 2.4720\n",
      "Epoch 63/200, Loss: 2.4878\n",
      "Epoch 64/200, Loss: 2.5167\n",
      "Epoch 65/200, Loss: 2.4332\n",
      "Epoch 66/200, Loss: 2.3623\n",
      "Epoch 67/200, Loss: 2.3973\n",
      "Epoch 68/200, Loss: 2.3824\n",
      "Epoch 69/200, Loss: 2.4296\n",
      "Epoch 70/200, Loss: 2.3920\n",
      "Epoch 71/200, Loss: 2.4070\n",
      "Epoch 72/200, Loss: 2.3652\n",
      "Epoch 73/200, Loss: 2.3645\n",
      "Epoch 74/200, Loss: 2.3293\n",
      "Epoch 75/200, Loss: 2.3722\n",
      "Epoch 76/200, Loss: 2.3547\n",
      "Epoch 77/200, Loss: 2.3875\n",
      "Epoch 78/200, Loss: 2.2525\n",
      "Epoch 79/200, Loss: 2.2566\n",
      "Epoch 80/200, Loss: 2.2701\n",
      "Epoch 81/200, Loss: 2.3620\n",
      "Epoch 82/200, Loss: 2.3176\n",
      "Epoch 83/200, Loss: 2.3136\n",
      "Epoch 84/200, Loss: 2.2560\n",
      "Epoch 85/200, Loss: 2.2662\n",
      "Epoch 86/200, Loss: 2.2493\n",
      "Epoch 87/200, Loss: 2.1958\n",
      "Epoch 88/200, Loss: 2.0911\n",
      "Epoch 89/200, Loss: 2.3974\n",
      "Epoch 90/200, Loss: 2.2048\n",
      "Epoch 91/200, Loss: 2.1622\n",
      "Epoch 92/200, Loss: 2.1985\n",
      "Epoch 93/200, Loss: 2.2280\n",
      "Epoch 94/200, Loss: 2.1978\n",
      "Epoch 95/200, Loss: 2.2524\n",
      "Epoch 96/200, Loss: 2.2740\n",
      "Epoch 97/200, Loss: 2.2858\n",
      "Epoch 98/200, Loss: 2.2321\n",
      "Epoch 99/200, Loss: 2.2351\n",
      "Epoch 100/200, Loss: 2.3205\n",
      "Epoch 101/200, Loss: 2.3116\n",
      "Epoch 102/200, Loss: 2.2108\n",
      "Epoch 103/200, Loss: 2.1094\n",
      "Epoch 104/200, Loss: 2.0969\n",
      "Epoch 105/200, Loss: 2.1610\n",
      "Epoch 106/200, Loss: 2.1564\n",
      "Epoch 107/200, Loss: 2.0559\n",
      "Epoch 108/200, Loss: 2.1025\n",
      "Epoch 109/200, Loss: 2.1557\n",
      "Epoch 110/200, Loss: 2.1203\n",
      "Epoch 111/200, Loss: 2.1376\n",
      "Epoch 112/200, Loss: 2.2059\n",
      "Epoch 113/200, Loss: 2.0302\n",
      "Epoch 114/200, Loss: 2.1310\n",
      "Epoch 115/200, Loss: 2.1746\n",
      "Epoch 116/200, Loss: 2.1973\n",
      "Epoch 117/200, Loss: 2.0711\n",
      "Epoch 118/200, Loss: 2.0719\n",
      "Epoch 119/200, Loss: 2.1447\n",
      "Epoch 120/200, Loss: 2.0535\n",
      "Epoch 121/200, Loss: 2.1031\n",
      "Epoch 122/200, Loss: 2.1053\n",
      "Epoch 123/200, Loss: 2.0973\n",
      "Epoch 124/200, Loss: 2.0618\n",
      "Epoch 125/200, Loss: 2.0452\n",
      "Epoch 126/200, Loss: 2.2844\n",
      "Epoch 127/200, Loss: 2.0984\n",
      "Epoch 128/200, Loss: 2.1071\n",
      "Epoch 129/200, Loss: 2.0030\n",
      "Epoch 130/200, Loss: 2.0561\n",
      "Epoch 131/200, Loss: 2.1028\n",
      "Epoch 132/200, Loss: 2.0502\n",
      "Epoch 133/200, Loss: 2.1090\n",
      "Epoch 134/200, Loss: 2.1460\n",
      "Epoch 135/200, Loss: 2.1525\n",
      "Epoch 136/200, Loss: 2.0705\n",
      "Epoch 137/200, Loss: 2.0721\n",
      "Epoch 138/200, Loss: 2.0144\n",
      "Epoch 139/200, Loss: 2.0579\n",
      "Epoch 140/200, Loss: 2.1121\n",
      "Epoch 141/200, Loss: 1.9866\n",
      "Epoch 142/200, Loss: 2.1108\n",
      "Epoch 143/200, Loss: 1.9278\n",
      "Epoch 144/200, Loss: 2.0571\n",
      "Epoch 145/200, Loss: 1.9458\n",
      "Epoch 146/200, Loss: 2.0154\n",
      "Epoch 147/200, Loss: 2.0988\n",
      "Epoch 148/200, Loss: 2.0382\n",
      "Epoch 149/200, Loss: 2.0364\n",
      "Epoch 150/200, Loss: 1.9140\n",
      "Epoch 151/200, Loss: 2.0287\n",
      "Epoch 152/200, Loss: 2.1362\n",
      "Epoch 153/200, Loss: 1.9361\n",
      "Epoch 154/200, Loss: 2.0987\n",
      "Epoch 155/200, Loss: 2.0084\n",
      "Epoch 156/200, Loss: 1.8883\n",
      "Epoch 157/200, Loss: 2.0401\n",
      "Epoch 158/200, Loss: 2.0873\n",
      "Epoch 159/200, Loss: 2.0055\n",
      "Epoch 160/200, Loss: 2.0743\n",
      "Epoch 161/200, Loss: 1.9924\n",
      "Epoch 162/200, Loss: 1.9636\n",
      "Epoch 163/200, Loss: 1.9092\n",
      "Epoch 164/200, Loss: 1.9537\n",
      "Epoch 165/200, Loss: 1.9020\n",
      "Epoch 166/200, Loss: 1.9391\n",
      "Epoch 167/200, Loss: 1.9614\n",
      "Epoch 168/200, Loss: 2.0250\n",
      "Epoch 169/200, Loss: 2.0081\n",
      "Epoch 170/200, Loss: 2.0010\n",
      "Epoch 171/200, Loss: 1.9048\n",
      "Epoch 172/200, Loss: 1.9786\n",
      "Epoch 173/200, Loss: 1.8077\n",
      "Epoch 174/200, Loss: 1.8625\n",
      "Epoch 175/200, Loss: 1.8859\n",
      "Epoch 176/200, Loss: 1.9407\n",
      "Epoch 177/200, Loss: 1.8910\n",
      "Epoch 178/200, Loss: 1.8946\n",
      "Epoch 179/200, Loss: 1.8552\n",
      "Epoch 180/200, Loss: 1.9096\n",
      "Epoch 181/200, Loss: 1.8418\n",
      "Epoch 182/200, Loss: 1.8296\n",
      "Epoch 183/200, Loss: 1.9288\n",
      "Epoch 184/200, Loss: 1.9204\n",
      "Epoch 185/200, Loss: 1.8528\n",
      "Epoch 186/200, Loss: 1.8917\n",
      "Epoch 187/200, Loss: 1.9594\n",
      "Epoch 188/200, Loss: 1.8951\n",
      "Epoch 189/200, Loss: 1.8383\n",
      "Epoch 190/200, Loss: 1.9247\n",
      "Epoch 191/200, Loss: 1.9310\n",
      "Epoch 192/200, Loss: 1.8100\n",
      "Epoch 193/200, Loss: 1.7929\n",
      "Epoch 194/200, Loss: 1.9734\n",
      "Epoch 195/200, Loss: 1.8705\n",
      "Epoch 196/200, Loss: 1.8122\n",
      "Epoch 197/200, Loss: 1.7948\n",
      "Epoch 198/200, Loss: 1.7787\n",
      "Epoch 199/200, Loss: 1.7702\n",
      "Epoch 200/200, Loss: 1.8985\n",
      "{'0': {'precision': 0.8888888888888888, 'recall': 0.8, 'f1-score': 0.8421052631578948, 'support': 10.0}, '1': {'precision': 0.8181818181818182, 'recall': 0.9, 'f1-score': 0.8571428571428572, 'support': 10.0}, 'accuracy': 0.85, 'macro avg': {'precision': 0.8535353535353536, 'recall': 0.8500000000000001, 'f1-score': 0.849624060150376, 'support': 20.0}, 'weighted avg': {'precision': 0.8535353535353536, 'recall': 0.85, 'f1-score': 0.8496240601503761, 'support': 20.0}}\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 3, attention_layers = 4, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
    "custom_model.train(epochs = 200, learning_rate = 1e-4)\n",
    "model_score = custom_model.evaluate()\n",
    "if (model_score > best_model_score):\n",
    "    best_model = custom_model\n",
    "    best_model_score = model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 3.1855\n",
      "Epoch 2/200, Loss: 3.0821\n",
      "Epoch 3/200, Loss: 2.8801\n",
      "Epoch 4/200, Loss: 2.7995\n",
      "Epoch 5/200, Loss: 2.7702\n",
      "Epoch 6/200, Loss: 2.8036\n",
      "Epoch 7/200, Loss: 2.7827\n",
      "Epoch 8/200, Loss: 2.7005\n",
      "Epoch 9/200, Loss: 2.8289\n",
      "Epoch 10/200, Loss: 2.6972\n",
      "Epoch 11/200, Loss: 2.7488\n",
      "Epoch 12/200, Loss: 2.7259\n",
      "Epoch 13/200, Loss: 2.6383\n",
      "Epoch 14/200, Loss: 2.6299\n",
      "Epoch 15/200, Loss: 2.6038\n",
      "Epoch 16/200, Loss: 2.6907\n",
      "Epoch 17/200, Loss: 2.5941\n",
      "Epoch 18/200, Loss: 2.5082\n",
      "Epoch 19/200, Loss: 2.5880\n",
      "Epoch 20/200, Loss: 2.5058\n",
      "Epoch 21/200, Loss: 2.5373\n",
      "Epoch 22/200, Loss: 2.5499\n",
      "Epoch 23/200, Loss: 2.5644\n",
      "Epoch 24/200, Loss: 2.4505\n",
      "Epoch 25/200, Loss: 2.6489\n",
      "Epoch 26/200, Loss: 2.5073\n",
      "Epoch 27/200, Loss: 2.5369\n",
      "Epoch 28/200, Loss: 2.5927\n",
      "Epoch 29/200, Loss: 2.3730\n",
      "Epoch 30/200, Loss: 2.3582\n",
      "Epoch 31/200, Loss: 2.5138\n",
      "Epoch 32/200, Loss: 2.4980\n",
      "Epoch 33/200, Loss: 2.3138\n",
      "Epoch 34/200, Loss: 2.4034\n",
      "Epoch 35/200, Loss: 2.4088\n",
      "Epoch 36/200, Loss: 2.1296\n",
      "Epoch 37/200, Loss: 2.2291\n",
      "Epoch 38/200, Loss: 2.1211\n",
      "Epoch 39/200, Loss: 2.0504\n",
      "Epoch 40/200, Loss: 2.1218\n",
      "Epoch 41/200, Loss: 1.8677\n",
      "Epoch 42/200, Loss: 2.0219\n",
      "Epoch 43/200, Loss: 1.8569\n",
      "Epoch 44/200, Loss: 1.9696\n",
      "Epoch 45/200, Loss: 1.8049\n",
      "Epoch 46/200, Loss: 1.8141\n",
      "Epoch 47/200, Loss: 1.8490\n",
      "Epoch 48/200, Loss: 1.6889\n",
      "Epoch 49/200, Loss: 1.8151\n",
      "Epoch 50/200, Loss: 1.7740\n",
      "Epoch 51/200, Loss: 1.7864\n",
      "Epoch 52/200, Loss: 1.6904\n",
      "Epoch 53/200, Loss: 1.7631\n",
      "Epoch 54/200, Loss: 1.6906\n",
      "Epoch 55/200, Loss: 1.7477\n",
      "Epoch 56/200, Loss: 1.6726\n",
      "Epoch 57/200, Loss: 1.6181\n",
      "Epoch 58/200, Loss: 1.6915\n",
      "Epoch 59/200, Loss: 1.7037\n",
      "Epoch 60/200, Loss: 1.6428\n",
      "Epoch 61/200, Loss: 1.6057\n",
      "Epoch 62/200, Loss: 1.6378\n",
      "Epoch 63/200, Loss: 1.6360\n",
      "Epoch 64/200, Loss: 1.7671\n",
      "Epoch 65/200, Loss: 1.5937\n",
      "Epoch 66/200, Loss: 1.5789\n",
      "Epoch 67/200, Loss: 1.6171\n",
      "Epoch 68/200, Loss: 1.6004\n",
      "Epoch 69/200, Loss: 1.6756\n",
      "Epoch 70/200, Loss: 1.5663\n",
      "Epoch 71/200, Loss: 1.6156\n",
      "Epoch 72/200, Loss: 1.6185\n",
      "Epoch 73/200, Loss: 1.5709\n",
      "Epoch 74/200, Loss: 1.5274\n",
      "Epoch 75/200, Loss: 1.5503\n",
      "Epoch 76/200, Loss: 1.5908\n",
      "Epoch 77/200, Loss: 1.5779\n",
      "Epoch 78/200, Loss: 1.4302\n",
      "Epoch 79/200, Loss: 1.4526\n",
      "Epoch 80/200, Loss: 1.4795\n",
      "Epoch 81/200, Loss: 1.5364\n",
      "Epoch 82/200, Loss: 1.4230\n",
      "Epoch 83/200, Loss: 1.5495\n",
      "Epoch 84/200, Loss: 1.4823\n",
      "Epoch 85/200, Loss: 1.4498\n",
      "Epoch 86/200, Loss: 1.4381\n",
      "Epoch 87/200, Loss: 1.3956\n",
      "Epoch 88/200, Loss: 1.3362\n",
      "Epoch 89/200, Loss: 1.6031\n",
      "Epoch 90/200, Loss: 1.4006\n",
      "Epoch 91/200, Loss: 1.3909\n",
      "Epoch 92/200, Loss: 1.4418\n",
      "Epoch 93/200, Loss: 1.4234\n",
      "Epoch 94/200, Loss: 1.3736\n",
      "Epoch 95/200, Loss: 1.4558\n",
      "Epoch 96/200, Loss: 1.4094\n",
      "Epoch 97/200, Loss: 1.4450\n",
      "Epoch 98/200, Loss: 1.4337\n",
      "Epoch 99/200, Loss: 1.4465\n",
      "Epoch 100/200, Loss: 1.5160\n",
      "Epoch 101/200, Loss: 1.5104\n",
      "Epoch 102/200, Loss: 1.3974\n",
      "Epoch 103/200, Loss: 1.3790\n",
      "Epoch 104/200, Loss: 1.3340\n",
      "Epoch 105/200, Loss: 1.3419\n",
      "Epoch 106/200, Loss: 1.3316\n",
      "Epoch 107/200, Loss: 1.2808\n",
      "Epoch 108/200, Loss: 1.3310\n",
      "Epoch 109/200, Loss: 1.3654\n",
      "Epoch 110/200, Loss: 1.3916\n",
      "Epoch 111/200, Loss: 1.4426\n",
      "Epoch 112/200, Loss: 1.3959\n",
      "Epoch 113/200, Loss: 1.2666\n",
      "Epoch 114/200, Loss: 1.3195\n",
      "Epoch 115/200, Loss: 1.3772\n",
      "Epoch 116/200, Loss: 1.4431\n",
      "Epoch 117/200, Loss: 1.3222\n",
      "Epoch 118/200, Loss: 1.2982\n",
      "Epoch 119/200, Loss: 1.3380\n",
      "Epoch 120/200, Loss: 1.2732\n",
      "Epoch 121/200, Loss: 1.3614\n",
      "Epoch 122/200, Loss: 1.3166\n",
      "Epoch 123/200, Loss: 1.3657\n",
      "Epoch 124/200, Loss: 1.2381\n",
      "Epoch 125/200, Loss: 1.2159\n",
      "Epoch 126/200, Loss: 1.4373\n",
      "Epoch 127/200, Loss: 1.2239\n",
      "Epoch 128/200, Loss: 1.3076\n",
      "Epoch 129/200, Loss: 1.2393\n",
      "Epoch 130/200, Loss: 1.2858\n",
      "Epoch 131/200, Loss: 1.2648\n",
      "Epoch 132/200, Loss: 1.2413\n",
      "Epoch 133/200, Loss: 1.3321\n",
      "Epoch 134/200, Loss: 1.3159\n",
      "Epoch 135/200, Loss: 1.3157\n",
      "Epoch 136/200, Loss: 1.2303\n",
      "Epoch 137/200, Loss: 1.2476\n",
      "Epoch 138/200, Loss: 1.1560\n",
      "Epoch 139/200, Loss: 1.1705\n",
      "Epoch 140/200, Loss: 1.2576\n",
      "Epoch 141/200, Loss: 1.1791\n",
      "Epoch 142/200, Loss: 1.3837\n",
      "Epoch 143/200, Loss: 1.0839\n",
      "Epoch 144/200, Loss: 1.2227\n",
      "Epoch 145/200, Loss: 1.1386\n",
      "Epoch 146/200, Loss: 1.1550\n",
      "Epoch 147/200, Loss: 1.2170\n",
      "Epoch 148/200, Loss: 1.2281\n",
      "Epoch 149/200, Loss: 1.2443\n",
      "Epoch 150/200, Loss: 1.0952\n",
      "Epoch 151/200, Loss: 1.1639\n",
      "Epoch 152/200, Loss: 1.1693\n",
      "Epoch 153/200, Loss: 1.1259\n",
      "Epoch 154/200, Loss: 1.1812\n",
      "Epoch 155/200, Loss: 1.1543\n",
      "Epoch 156/200, Loss: 1.0781\n",
      "Epoch 157/200, Loss: 1.2160\n",
      "Epoch 158/200, Loss: 1.1987\n",
      "Epoch 159/200, Loss: 1.1764\n",
      "Epoch 160/200, Loss: 1.1111\n",
      "Epoch 161/200, Loss: 1.0740\n",
      "Epoch 162/200, Loss: 1.0666\n",
      "Epoch 163/200, Loss: 1.0204\n",
      "Epoch 164/200, Loss: 1.0688\n",
      "Epoch 165/200, Loss: 1.0187\n",
      "Epoch 166/200, Loss: 1.0725\n",
      "Epoch 167/200, Loss: 1.1465\n",
      "Epoch 168/200, Loss: 1.1347\n",
      "Epoch 169/200, Loss: 1.0920\n",
      "Epoch 170/200, Loss: 1.1041\n",
      "Epoch 171/200, Loss: 0.9881\n",
      "Epoch 172/200, Loss: 1.1098\n",
      "Epoch 173/200, Loss: 0.9622\n",
      "Epoch 174/200, Loss: 1.0412\n",
      "Epoch 175/200, Loss: 0.9748\n",
      "Epoch 176/200, Loss: 1.0769\n",
      "Epoch 177/200, Loss: 0.9959\n",
      "Epoch 178/200, Loss: 1.0179\n",
      "Epoch 179/200, Loss: 1.0197\n",
      "Epoch 180/200, Loss: 1.0490\n",
      "Epoch 181/200, Loss: 0.9912\n",
      "Epoch 182/200, Loss: 0.9840\n",
      "Epoch 183/200, Loss: 1.0584\n",
      "Epoch 184/200, Loss: 1.0564\n",
      "Epoch 185/200, Loss: 0.9994\n",
      "Epoch 186/200, Loss: 1.0191\n",
      "Epoch 187/200, Loss: 1.0383\n",
      "Epoch 188/200, Loss: 1.1137\n",
      "Epoch 189/200, Loss: 0.9509\n",
      "Epoch 190/200, Loss: 1.0290\n",
      "Epoch 191/200, Loss: 1.0585\n",
      "Epoch 192/200, Loss: 0.9404\n",
      "Epoch 193/200, Loss: 0.9215\n",
      "Epoch 194/200, Loss: 1.0407\n",
      "Epoch 195/200, Loss: 0.9987\n",
      "Epoch 196/200, Loss: 0.9447\n",
      "Epoch 197/200, Loss: 0.9177\n",
      "Epoch 198/200, Loss: 0.9466\n",
      "Epoch 199/200, Loss: 0.9482\n",
      "Epoch 200/200, Loss: 0.9505\n",
      "{'0': {'precision': 1.0, 'recall': 0.9, 'f1-score': 0.9473684210526316, 'support': 10.0}, '1': {'precision': 0.9090909090909091, 'recall': 1.0, 'f1-score': 0.9523809523809523, 'support': 10.0}, 'accuracy': 0.95, 'macro avg': {'precision': 0.9545454545454546, 'recall': 0.95, 'f1-score': 0.949874686716792, 'support': 20.0}, 'weighted avg': {'precision': 0.9545454545454545, 'recall': 0.95, 'f1-score': 0.949874686716792, 'support': 20.0}}\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 6, attention_layers = 4, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
    "custom_model.train(epochs = 200, learning_rate = 1e-4)\n",
    "model_score = custom_model.evaluate()\n",
    "if (model_score > best_model_score):\n",
    "    best_model = custom_model\n",
    "    best_model_score = model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.4148\n",
      "Epoch 2/100, Loss: 3.4060\n",
      "Epoch 3/100, Loss: 2.7263\n",
      "Epoch 4/100, Loss: 3.0022\n",
      "Epoch 5/100, Loss: 2.7940\n",
      "Epoch 6/100, Loss: 2.8148\n",
      "Epoch 7/100, Loss: 2.7509\n",
      "Epoch 8/100, Loss: 2.7201\n",
      "Epoch 9/100, Loss: 2.8014\n",
      "Epoch 10/100, Loss: 2.7367\n",
      "Epoch 11/100, Loss: 2.8092\n",
      "Epoch 12/100, Loss: 2.7298\n",
      "Epoch 13/100, Loss: 2.7013\n",
      "Epoch 14/100, Loss: 2.7266\n",
      "Epoch 15/100, Loss: 2.6533\n",
      "Epoch 16/100, Loss: 2.6065\n",
      "Epoch 17/100, Loss: 2.6124\n",
      "Epoch 18/100, Loss: 2.6445\n",
      "Epoch 19/100, Loss: 2.5875\n",
      "Epoch 20/100, Loss: 2.6470\n",
      "Epoch 21/100, Loss: 2.6301\n",
      "Epoch 22/100, Loss: 2.4814\n",
      "Epoch 23/100, Loss: 2.6890\n",
      "Epoch 24/100, Loss: 2.5190\n",
      "Epoch 25/100, Loss: 2.5917\n",
      "Epoch 26/100, Loss: 2.4187\n",
      "Epoch 27/100, Loss: 2.5217\n",
      "Epoch 28/100, Loss: 2.4281\n",
      "Epoch 29/100, Loss: 2.4685\n",
      "Epoch 30/100, Loss: 2.5657\n",
      "Epoch 31/100, Loss: 2.3830\n",
      "Epoch 32/100, Loss: 2.3090\n",
      "Epoch 33/100, Loss: 2.3849\n",
      "Epoch 34/100, Loss: 2.3645\n",
      "Epoch 35/100, Loss: 2.3103\n",
      "Epoch 36/100, Loss: 2.2014\n",
      "Epoch 37/100, Loss: 2.2237\n",
      "Epoch 38/100, Loss: 2.1271\n",
      "Epoch 39/100, Loss: 2.0891\n",
      "Epoch 40/100, Loss: 2.0463\n",
      "Epoch 41/100, Loss: 2.0202\n",
      "Epoch 42/100, Loss: 1.9141\n",
      "Epoch 43/100, Loss: 1.8401\n",
      "Epoch 44/100, Loss: 1.8207\n",
      "Epoch 45/100, Loss: 1.9549\n",
      "Epoch 46/100, Loss: 1.7776\n",
      "Epoch 47/100, Loss: 2.0533\n",
      "Epoch 48/100, Loss: 1.9761\n",
      "Epoch 49/100, Loss: 1.7349\n",
      "Epoch 50/100, Loss: 1.7382\n",
      "Epoch 51/100, Loss: 1.8551\n",
      "Epoch 52/100, Loss: 1.8278\n",
      "Epoch 53/100, Loss: 1.7047\n",
      "Epoch 54/100, Loss: 1.7868\n",
      "Epoch 55/100, Loss: 1.6700\n",
      "Epoch 56/100, Loss: 1.7380\n",
      "Epoch 57/100, Loss: 1.7836\n",
      "Epoch 58/100, Loss: 1.6640\n",
      "Epoch 59/100, Loss: 1.8027\n",
      "Epoch 60/100, Loss: 1.6926\n",
      "Epoch 61/100, Loss: 1.8515\n",
      "Epoch 62/100, Loss: 1.7181\n",
      "Epoch 63/100, Loss: 1.6887\n",
      "Epoch 64/100, Loss: 1.6753\n",
      "Epoch 65/100, Loss: 1.6785\n",
      "Epoch 66/100, Loss: 1.6978\n",
      "Epoch 67/100, Loss: 1.6743\n",
      "Epoch 68/100, Loss: 1.7755\n",
      "Epoch 69/100, Loss: 1.7342\n",
      "Epoch 70/100, Loss: 1.6972\n",
      "Epoch 71/100, Loss: 1.6835\n",
      "Epoch 72/100, Loss: 1.6675\n",
      "Epoch 73/100, Loss: 1.5967\n",
      "Epoch 74/100, Loss: 1.6361\n",
      "Epoch 75/100, Loss: 1.6960\n",
      "Epoch 76/100, Loss: 1.5361\n",
      "Epoch 77/100, Loss: 1.6498\n",
      "Epoch 78/100, Loss: 1.5829\n",
      "Epoch 79/100, Loss: 1.6200\n",
      "Epoch 80/100, Loss: 1.6515\n",
      "Epoch 81/100, Loss: 1.7338\n",
      "Epoch 82/100, Loss: 1.5939\n",
      "Epoch 83/100, Loss: 1.6167\n",
      "Epoch 84/100, Loss: 1.6167\n",
      "Epoch 85/100, Loss: 1.5629\n",
      "Epoch 86/100, Loss: 1.7223\n",
      "Epoch 87/100, Loss: 1.5421\n",
      "Epoch 88/100, Loss: 1.6296\n",
      "Epoch 89/100, Loss: 1.6309\n",
      "Epoch 90/100, Loss: 1.5297\n",
      "Epoch 91/100, Loss: 1.5161\n",
      "Epoch 92/100, Loss: 1.5628\n",
      "Epoch 93/100, Loss: 1.5837\n",
      "Epoch 94/100, Loss: 1.4992\n",
      "Epoch 95/100, Loss: 1.4584\n",
      "Epoch 96/100, Loss: 1.4591\n",
      "Epoch 97/100, Loss: 1.4472\n",
      "Epoch 98/100, Loss: 1.4888\n",
      "Epoch 99/100, Loss: 1.4785\n",
      "Epoch 100/100, Loss: 1.5101\n",
      "{'0': {'precision': 0.9, 'recall': 0.9, 'f1-score': 0.9, 'support': 10.0}, '1': {'precision': 0.9, 'recall': 0.9, 'f1-score': 0.9, 'support': 10.0}, 'accuracy': 0.9, 'macro avg': {'precision': 0.9, 'recall': 0.9, 'f1-score': 0.9, 'support': 20.0}, 'weighted avg': {'precision': 0.9, 'recall': 0.9, 'f1-score': 0.9, 'support': 20.0}}\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 6, attention_layers = 6, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
    "custom_model.train(epochs = 100, learning_rate = 1e-4)\n",
    "model_score = custom_model.evaluate()\n",
    "if (model_score > best_model_score):\n",
    "    best_model = custom_model\n",
    "    best_model_score = model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score in Validation:  0.949874686716792\n",
      "Best model score in Test: \n",
      "\n",
      "Labels:      [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "Predictions: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        10\n",
      "           1       0.67      1.00      0.80        10\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.83      0.75      0.73        20\n",
      "weighted avg       0.83      0.75      0.73        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model score in Validation: \", best_model_score)\n",
    "print(\"Best model score in Test: \\n\")\n",
    "best_model.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
