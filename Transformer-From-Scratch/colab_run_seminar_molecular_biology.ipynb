{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiyuubP-Zx6J"
      },
      "source": [
        "NOTE: This was run using the following split:\n",
        "- Test: 7/7\n",
        "- Val: 7/7\n",
        "- Train: 26/96  <- more training than in \"tryout_models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IQ233IaKZx6K"
      },
      "outputs": [],
      "source": [
        "global_seed = 44\n",
        "best_model = None\n",
        "best_model_score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F71tpVhATZf9",
        "outputId": "908ffc86-923e-4bd7-f925-84488e462a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory seminar-dlmb-2024-winter-public does not exist or is empty.\n",
            "Directory Antibiotic-Resistance-Prediction does not exist or is empty.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "directory = \"seminar-dlmb-2024-winter-public\"\n",
        "\n",
        "if os.path.exists(directory) and os.listdir(directory):\n",
        "    print(f\"Directory {directory} exists and is non-empty.\")\n",
        "else:\n",
        "    print(f\"Directory {directory} does not exist or is empty.\")\n",
        "    !!git clone https://github.com/hzi-bifo/seminar-dlmb-2024-winter-public.git\n",
        "\n",
        "directory = \"Antibiotic-Resistance-Prediction\"\n",
        "\n",
        "if os.path.exists(directory) and os.listdir(directory):\n",
        "    print(f\"Directory {directory} exists and is non-empty.\")\n",
        "else:\n",
        "    print(f\"Directory {directory} does not exist or is empty.\")\n",
        "    !!git clone https://github.com/Derrnux/Antibiotic-Resistance-Prediction.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVSdzzgUXBMB",
        "outputId": "cebefb25-269f-430d-b22b-2ad609ddfa20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files moved from 'Antibiotic-Resistance-Prediction' to '.'\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = \"Antibiotic-Resistance-Prediction\"\n",
        "\n",
        "dest_dir = \".\"\n",
        "\n",
        "for filename in os.listdir(source_dir):\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    dest_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "    shutil.move(source_path, dest_path)\n",
        "\n",
        "print(f\"Files moved from '{source_dir}' to '{dest_dir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki_Gkj9LXgzu",
        "outputId": "9123549e-6208-4917-f7de-cb8fda52373f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.84\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython seaborn\n",
        "%pip install biopython\n",
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BNCMaEVctnv",
        "outputId": "0884285d-72be-4a9f-9f14-e1a45dff9408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB is available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JuZFemYUT3Th"
      },
      "outputs": [],
      "source": [
        "from transformer_manager import Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7v45fJoWzoh",
        "outputId": "dcf52825-0fc8-4fc4-d4a6-04a84364e0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.9143\n",
            "Epoch 2/50, Loss: 2.7709\n",
            "Epoch 3/50, Loss: 2.7647\n",
            "Epoch 4/50, Loss: 2.7695\n",
            "Epoch 5/50, Loss: 2.6904\n",
            "Epoch 6/50, Loss: 2.6783\n",
            "Epoch 7/50, Loss: 2.6307\n",
            "Epoch 8/50, Loss: 2.6172\n",
            "Epoch 9/50, Loss: 2.6049\n",
            "Epoch 10/50, Loss: 2.5834\n",
            "Epoch 11/50, Loss: 2.6090\n",
            "Epoch 12/50, Loss: 2.5725\n",
            "Epoch 13/50, Loss: 2.5690\n",
            "Epoch 14/50, Loss: 2.5743\n",
            "Epoch 15/50, Loss: 2.5811\n",
            "Epoch 16/50, Loss: 2.5748\n",
            "Epoch 17/50, Loss: 2.5669\n",
            "Epoch 18/50, Loss: 2.5749\n",
            "Epoch 19/50, Loss: 2.5685\n",
            "Epoch 20/50, Loss: 2.5278\n",
            "Epoch 21/50, Loss: 2.5541\n",
            "Epoch 22/50, Loss: 2.5475\n",
            "Epoch 23/50, Loss: 2.5314\n",
            "Epoch 24/50, Loss: 2.5303\n",
            "Epoch 25/50, Loss: 2.5655\n",
            "Epoch 26/50, Loss: 2.5537\n",
            "Epoch 27/50, Loss: 2.5803\n",
            "Epoch 28/50, Loss: 2.5775\n",
            "Epoch 29/50, Loss: 2.5938\n",
            "Epoch 30/50, Loss: 2.5648\n",
            "Epoch 31/50, Loss: 2.5108\n",
            "Epoch 32/50, Loss: 2.5362\n",
            "Epoch 33/50, Loss: 2.5558\n",
            "Epoch 34/50, Loss: 2.5568\n",
            "Epoch 35/50, Loss: 2.5566\n",
            "Epoch 36/50, Loss: 2.6126\n",
            "Epoch 37/50, Loss: 2.5483\n",
            "Epoch 38/50, Loss: 2.5065\n",
            "Epoch 39/50, Loss: 2.5839\n",
            "Epoch 40/50, Loss: 2.5733\n",
            "Epoch 41/50, Loss: 2.5867\n",
            "Epoch 42/50, Loss: 2.6419\n",
            "Epoch 43/50, Loss: 2.5427\n",
            "Epoch 44/50, Loss: 2.6003\n",
            "Epoch 45/50, Loss: 2.5661\n",
            "Epoch 46/50, Loss: 2.5527\n",
            "Epoch 47/50, Loss: 2.5582\n",
            "Epoch 48/50, Loss: 2.5712\n",
            "Epoch 49/50, Loss: 2.5371\n",
            "Epoch 50/50, Loss: 2.5487\n",
            "{'0': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, '1': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}, 'weighted avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 1, attention_layers = 3, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 128, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 50, learning_rate = 1e-4)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEjRuMZi3NLo",
        "outputId": "7859e5f4-a2a8-4078-a995-6dbe27c4c3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 2.8555\n",
            "Epoch 2/500, Loss: 2.8517\n",
            "Epoch 3/500, Loss: 2.6880\n",
            "Epoch 4/500, Loss: 2.6976\n",
            "Epoch 5/500, Loss: 2.6478\n",
            "Epoch 6/500, Loss: 2.6172\n",
            "Epoch 7/500, Loss: 2.6092\n",
            "Epoch 8/500, Loss: 2.6019\n",
            "Epoch 9/500, Loss: 2.5899\n",
            "Epoch 10/500, Loss: 2.5701\n",
            "Epoch 11/500, Loss: 2.5935\n",
            "Epoch 12/500, Loss: 2.5617\n",
            "Epoch 13/500, Loss: 2.5555\n",
            "Epoch 14/500, Loss: 2.5519\n",
            "Epoch 15/500, Loss: 2.5897\n",
            "Epoch 16/500, Loss: 2.5625\n",
            "Epoch 17/500, Loss: 2.5586\n",
            "Epoch 18/500, Loss: 2.5723\n",
            "Epoch 19/500, Loss: 2.5494\n",
            "Epoch 20/500, Loss: 2.5289\n",
            "Epoch 21/500, Loss: 2.5596\n",
            "Epoch 22/500, Loss: 2.5450\n",
            "Epoch 23/500, Loss: 2.5082\n",
            "Epoch 24/500, Loss: 2.5447\n",
            "Epoch 25/500, Loss: 2.5620\n",
            "Epoch 26/500, Loss: 2.5605\n",
            "Epoch 27/500, Loss: 2.5807\n",
            "Epoch 28/500, Loss: 2.5731\n",
            "Epoch 29/500, Loss: 2.5802\n",
            "Epoch 30/500, Loss: 2.5606\n",
            "Epoch 31/500, Loss: 2.5048\n",
            "Epoch 32/500, Loss: 2.5295\n",
            "Epoch 33/500, Loss: 2.5314\n",
            "Epoch 34/500, Loss: 2.5491\n",
            "Epoch 35/500, Loss: 2.5411\n",
            "Epoch 36/500, Loss: 2.6152\n",
            "Epoch 37/500, Loss: 2.5220\n",
            "Epoch 38/500, Loss: 2.4983\n",
            "Epoch 39/500, Loss: 2.5501\n",
            "Epoch 40/500, Loss: 2.5327\n",
            "Epoch 41/500, Loss: 2.5747\n",
            "Epoch 42/500, Loss: 2.6220\n",
            "Epoch 43/500, Loss: 2.5110\n",
            "Epoch 44/500, Loss: 2.6197\n",
            "Epoch 45/500, Loss: 2.5541\n",
            "Epoch 46/500, Loss: 2.5275\n",
            "Epoch 47/500, Loss: 2.5371\n",
            "Epoch 48/500, Loss: 2.5423\n",
            "Epoch 49/500, Loss: 2.5187\n",
            "Epoch 50/500, Loss: 2.5259\n",
            "Epoch 51/500, Loss: 2.5176\n",
            "Epoch 52/500, Loss: 2.4952\n",
            "Epoch 53/500, Loss: 2.5103\n",
            "Epoch 54/500, Loss: 2.5124\n",
            "Epoch 55/500, Loss: 2.5380\n",
            "Epoch 56/500, Loss: 2.5651\n",
            "Epoch 57/500, Loss: 2.5375\n",
            "Epoch 58/500, Loss: 2.4897\n",
            "Epoch 59/500, Loss: 2.5105\n",
            "Epoch 60/500, Loss: 2.5222\n",
            "Epoch 61/500, Loss: 2.5131\n",
            "Epoch 62/500, Loss: 2.5251\n",
            "Epoch 63/500, Loss: 2.5371\n",
            "Epoch 64/500, Loss: 2.5144\n",
            "Epoch 65/500, Loss: 2.5349\n",
            "Epoch 66/500, Loss: 2.4945\n",
            "Epoch 67/500, Loss: 2.4971\n",
            "Epoch 68/500, Loss: 2.5161\n",
            "Epoch 69/500, Loss: 2.4979\n",
            "Epoch 70/500, Loss: 2.5343\n",
            "Epoch 71/500, Loss: 2.6093\n",
            "Epoch 72/500, Loss: 2.4908\n",
            "Epoch 73/500, Loss: 2.5187\n",
            "Epoch 74/500, Loss: 2.4974\n",
            "Epoch 75/500, Loss: 2.5252\n",
            "Epoch 76/500, Loss: 2.5124\n",
            "Epoch 77/500, Loss: 2.5191\n",
            "Epoch 78/500, Loss: 2.5114\n",
            "Epoch 79/500, Loss: 2.4850\n",
            "Epoch 80/500, Loss: 2.5055\n",
            "Epoch 81/500, Loss: 2.4817\n",
            "Epoch 82/500, Loss: 2.4871\n",
            "Epoch 83/500, Loss: 2.5113\n",
            "Epoch 84/500, Loss: 2.5300\n",
            "Epoch 85/500, Loss: 2.5045\n",
            "Epoch 86/500, Loss: 2.4836\n",
            "Epoch 87/500, Loss: 2.5108\n",
            "Epoch 88/500, Loss: 2.4829\n",
            "Epoch 89/500, Loss: 2.5123\n",
            "Epoch 90/500, Loss: 2.4999\n",
            "Epoch 91/500, Loss: 2.4925\n",
            "Epoch 92/500, Loss: 2.4838\n",
            "Epoch 93/500, Loss: 2.4633\n",
            "Epoch 94/500, Loss: 2.4809\n",
            "Epoch 95/500, Loss: 2.4933\n",
            "Epoch 96/500, Loss: 2.4706\n",
            "Epoch 97/500, Loss: 2.5078\n",
            "Epoch 98/500, Loss: 2.4876\n",
            "Epoch 99/500, Loss: 2.5054\n",
            "Epoch 100/500, Loss: 2.4847\n",
            "Epoch 101/500, Loss: 2.5260\n",
            "Epoch 102/500, Loss: 2.5334\n",
            "Epoch 103/500, Loss: 2.4612\n",
            "Epoch 104/500, Loss: 2.5166\n",
            "Epoch 105/500, Loss: 2.4914\n",
            "Epoch 106/500, Loss: 2.4867\n",
            "Epoch 107/500, Loss: 2.4998\n",
            "Epoch 108/500, Loss: 2.4601\n",
            "Epoch 109/500, Loss: 2.4590\n",
            "Epoch 110/500, Loss: 2.4616\n",
            "Epoch 111/500, Loss: 2.4794\n",
            "Epoch 112/500, Loss: 2.4462\n",
            "Epoch 113/500, Loss: 2.4390\n",
            "Epoch 114/500, Loss: 2.4602\n",
            "Epoch 115/500, Loss: 2.4953\n",
            "Epoch 116/500, Loss: 2.4581\n",
            "Epoch 117/500, Loss: 2.4752\n",
            "Epoch 118/500, Loss: 2.4981\n",
            "Epoch 119/500, Loss: 2.4719\n",
            "Epoch 120/500, Loss: 2.4689\n",
            "Epoch 121/500, Loss: 2.4876\n",
            "Epoch 122/500, Loss: 2.4998\n",
            "Epoch 123/500, Loss: 2.4434\n",
            "Epoch 124/500, Loss: 2.4793\n",
            "Epoch 125/500, Loss: 2.4744\n",
            "Epoch 126/500, Loss: 2.4718\n",
            "Epoch 127/500, Loss: 2.4486\n",
            "Epoch 128/500, Loss: 2.4480\n",
            "Epoch 129/500, Loss: 2.4201\n",
            "Epoch 130/500, Loss: 2.3704\n",
            "Epoch 131/500, Loss: 2.4243\n",
            "Epoch 132/500, Loss: 2.4906\n",
            "Epoch 133/500, Loss: 2.3860\n",
            "Epoch 134/500, Loss: 2.4633\n",
            "Epoch 135/500, Loss: 2.4957\n",
            "Epoch 136/500, Loss: 2.4800\n",
            "Epoch 137/500, Loss: 2.5178\n",
            "Epoch 138/500, Loss: 2.4592\n",
            "Epoch 139/500, Loss: 2.4956\n",
            "Epoch 140/500, Loss: 2.4462\n",
            "Epoch 141/500, Loss: 2.4323\n",
            "Epoch 142/500, Loss: 2.4135\n",
            "Epoch 143/500, Loss: 2.4594\n",
            "Epoch 144/500, Loss: 2.4439\n",
            "Epoch 145/500, Loss: 2.4201\n",
            "Epoch 146/500, Loss: 2.4349\n",
            "Epoch 147/500, Loss: 2.4470\n",
            "Epoch 148/500, Loss: 2.4184\n",
            "Epoch 149/500, Loss: 2.4139\n",
            "Epoch 150/500, Loss: 2.4519\n",
            "Epoch 151/500, Loss: 2.4488\n",
            "Epoch 152/500, Loss: 2.4163\n",
            "Epoch 153/500, Loss: 2.4770\n",
            "Epoch 154/500, Loss: 2.4361\n",
            "Epoch 155/500, Loss: 2.4266\n",
            "Epoch 156/500, Loss: 2.4246\n",
            "Epoch 157/500, Loss: 2.4177\n",
            "Epoch 158/500, Loss: 2.3965\n",
            "Epoch 159/500, Loss: 2.3857\n",
            "Epoch 160/500, Loss: 2.3359\n",
            "Epoch 161/500, Loss: 2.4011\n",
            "Epoch 162/500, Loss: 2.4288\n",
            "Epoch 163/500, Loss: 2.3990\n",
            "Epoch 164/500, Loss: 2.4404\n",
            "Epoch 165/500, Loss: 2.4349\n",
            "Epoch 166/500, Loss: 2.3870\n",
            "Epoch 167/500, Loss: 2.4071\n",
            "Epoch 168/500, Loss: 2.3420\n",
            "Epoch 169/500, Loss: 2.4108\n",
            "Epoch 170/500, Loss: 2.3986\n",
            "Epoch 171/500, Loss: 2.3981\n",
            "Epoch 172/500, Loss: 2.3905\n",
            "Epoch 173/500, Loss: 2.4258\n",
            "Epoch 174/500, Loss: 2.4092\n",
            "Epoch 175/500, Loss: 2.3898\n",
            "Epoch 176/500, Loss: 2.4129\n",
            "Epoch 177/500, Loss: 2.3910\n",
            "Epoch 178/500, Loss: 2.4214\n",
            "Epoch 179/500, Loss: 2.3551\n",
            "Epoch 180/500, Loss: 2.3874\n",
            "Epoch 181/500, Loss: 2.3837\n",
            "Epoch 182/500, Loss: 2.3934\n",
            "Epoch 183/500, Loss: 2.3553\n",
            "Epoch 184/500, Loss: 2.4193\n",
            "Epoch 185/500, Loss: 2.4141\n",
            "Epoch 186/500, Loss: 2.3841\n",
            "Epoch 187/500, Loss: 2.3997\n",
            "Epoch 188/500, Loss: 2.3901\n",
            "Epoch 189/500, Loss: 2.4541\n",
            "Epoch 190/500, Loss: 2.4092\n",
            "Epoch 191/500, Loss: 2.4281\n",
            "Epoch 192/500, Loss: 2.4137\n",
            "Epoch 193/500, Loss: 2.4196\n",
            "Epoch 194/500, Loss: 2.4213\n",
            "Epoch 195/500, Loss: 2.4220\n",
            "Epoch 196/500, Loss: 2.3871\n",
            "Epoch 197/500, Loss: 2.3924\n",
            "Epoch 198/500, Loss: 2.4368\n",
            "Epoch 199/500, Loss: 2.4273\n",
            "Epoch 200/500, Loss: 2.3795\n",
            "Epoch 201/500, Loss: 2.4056\n",
            "Epoch 202/500, Loss: 2.3834\n",
            "Epoch 203/500, Loss: 2.3971\n",
            "Epoch 204/500, Loss: 2.3916\n",
            "Epoch 205/500, Loss: 2.3948\n",
            "Epoch 206/500, Loss: 2.3728\n",
            "Epoch 207/500, Loss: 2.3728\n",
            "Epoch 208/500, Loss: 2.3754\n",
            "Epoch 209/500, Loss: 2.4068\n",
            "Epoch 210/500, Loss: 2.3808\n",
            "Epoch 211/500, Loss: 2.4107\n",
            "Epoch 212/500, Loss: 2.3353\n",
            "Epoch 213/500, Loss: 2.3829\n",
            "Epoch 214/500, Loss: 2.3952\n",
            "Epoch 215/500, Loss: 2.4499\n",
            "Epoch 216/500, Loss: 2.3884\n",
            "Epoch 217/500, Loss: 2.4028\n",
            "Epoch 218/500, Loss: 2.4004\n",
            "Epoch 219/500, Loss: 2.3268\n",
            "Epoch 220/500, Loss: 2.3756\n",
            "Epoch 221/500, Loss: 2.3649\n",
            "Epoch 222/500, Loss: 2.3659\n",
            "Epoch 223/500, Loss: 2.4218\n",
            "Epoch 224/500, Loss: 2.3692\n",
            "Epoch 225/500, Loss: 2.3833\n",
            "Epoch 226/500, Loss: 2.3824\n",
            "Epoch 227/500, Loss: 2.3811\n",
            "Epoch 228/500, Loss: 2.3672\n",
            "Epoch 229/500, Loss: 2.3591\n",
            "Epoch 230/500, Loss: 2.3608\n",
            "Epoch 231/500, Loss: 2.3643\n",
            "Epoch 232/500, Loss: 2.4044\n",
            "Epoch 233/500, Loss: 2.3612\n",
            "Epoch 234/500, Loss: 2.4109\n",
            "Epoch 235/500, Loss: 2.3677\n",
            "Epoch 236/500, Loss: 2.3881\n",
            "Epoch 237/500, Loss: 2.3603\n",
            "Epoch 238/500, Loss: 2.3025\n",
            "Epoch 239/500, Loss: 2.3630\n",
            "Epoch 240/500, Loss: 2.3997\n",
            "Epoch 241/500, Loss: 2.3352\n",
            "Epoch 242/500, Loss: 2.3395\n",
            "Epoch 243/500, Loss: 2.3580\n",
            "Epoch 244/500, Loss: 2.3826\n",
            "Epoch 245/500, Loss: 2.4012\n",
            "Epoch 246/500, Loss: 2.3929\n",
            "Epoch 247/500, Loss: 2.4247\n",
            "Epoch 248/500, Loss: 2.3532\n",
            "Epoch 249/500, Loss: 2.3514\n",
            "Epoch 250/500, Loss: 2.3335\n",
            "Epoch 251/500, Loss: 2.4115\n",
            "Epoch 252/500, Loss: 2.3327\n",
            "Epoch 253/500, Loss: 2.3527\n",
            "Epoch 254/500, Loss: 2.3774\n",
            "Epoch 255/500, Loss: 2.3912\n",
            "Epoch 256/500, Loss: 2.3458\n",
            "Epoch 257/500, Loss: 2.3536\n",
            "Epoch 258/500, Loss: 2.3692\n",
            "Epoch 259/500, Loss: 2.3489\n",
            "Epoch 260/500, Loss: 2.3841\n",
            "Epoch 261/500, Loss: 2.3907\n",
            "Epoch 262/500, Loss: 2.3308\n",
            "Epoch 263/500, Loss: 2.3349\n",
            "Epoch 264/500, Loss: 2.3803\n",
            "Epoch 265/500, Loss: 2.3346\n",
            "Epoch 266/500, Loss: 2.3598\n",
            "Epoch 267/500, Loss: 2.3171\n",
            "Epoch 268/500, Loss: 2.3479\n",
            "Epoch 269/500, Loss: 2.3302\n",
            "Epoch 270/500, Loss: 2.3661\n",
            "Epoch 271/500, Loss: 2.3256\n",
            "Epoch 272/500, Loss: 2.3798\n",
            "Epoch 273/500, Loss: 2.3671\n",
            "Epoch 274/500, Loss: 2.3105\n",
            "Epoch 275/500, Loss: 2.3379\n",
            "Epoch 276/500, Loss: 2.3276\n",
            "Epoch 277/500, Loss: 2.3521\n",
            "Epoch 278/500, Loss: 2.3221\n",
            "Epoch 279/500, Loss: 2.3578\n",
            "Epoch 280/500, Loss: 2.2992\n",
            "Epoch 281/500, Loss: 2.3563\n",
            "Epoch 282/500, Loss: 2.3917\n",
            "Epoch 283/500, Loss: 2.3559\n",
            "Epoch 284/500, Loss: 2.3261\n",
            "Epoch 285/500, Loss: 2.3476\n",
            "Epoch 286/500, Loss: 2.3289\n",
            "Epoch 287/500, Loss: 2.2888\n",
            "Epoch 288/500, Loss: 2.3464\n",
            "Epoch 289/500, Loss: 2.3372\n",
            "Epoch 290/500, Loss: 2.3172\n",
            "Epoch 291/500, Loss: 2.3488\n",
            "Epoch 292/500, Loss: 2.3335\n",
            "Epoch 293/500, Loss: 2.3096\n",
            "Epoch 294/500, Loss: 2.3154\n",
            "Epoch 295/500, Loss: 2.3889\n",
            "Epoch 296/500, Loss: 2.3516\n",
            "Epoch 297/500, Loss: 2.3339\n",
            "Epoch 298/500, Loss: 2.3313\n",
            "Epoch 299/500, Loss: 2.3056\n",
            "Epoch 300/500, Loss: 2.3224\n",
            "Epoch 301/500, Loss: 2.3061\n",
            "Epoch 302/500, Loss: 2.3044\n",
            "Epoch 303/500, Loss: 2.3242\n",
            "Epoch 304/500, Loss: 2.3443\n",
            "Epoch 305/500, Loss: 2.3123\n",
            "Epoch 306/500, Loss: 2.3259\n",
            "Epoch 307/500, Loss: 2.3345\n",
            "Epoch 308/500, Loss: 2.3152\n",
            "Epoch 309/500, Loss: 2.3106\n",
            "Epoch 310/500, Loss: 2.3044\n",
            "Epoch 311/500, Loss: 2.3903\n",
            "Epoch 312/500, Loss: 2.2968\n",
            "Epoch 313/500, Loss: 2.3254\n",
            "Epoch 314/500, Loss: 2.3208\n",
            "Epoch 315/500, Loss: 2.2844\n",
            "Epoch 316/500, Loss: 2.3240\n",
            "Epoch 317/500, Loss: 2.3180\n",
            "Epoch 318/500, Loss: 2.3095\n",
            "Epoch 319/500, Loss: 2.2621\n",
            "Epoch 320/500, Loss: 2.3511\n",
            "Epoch 321/500, Loss: 2.2971\n",
            "Epoch 322/500, Loss: 2.2785\n",
            "Epoch 323/500, Loss: 2.2852\n",
            "Epoch 324/500, Loss: 2.3472\n",
            "Epoch 325/500, Loss: 2.2957\n",
            "Epoch 326/500, Loss: 2.2793\n",
            "Epoch 327/500, Loss: 2.3188\n",
            "Epoch 328/500, Loss: 2.2974\n",
            "Epoch 329/500, Loss: 2.3611\n",
            "Epoch 330/500, Loss: 2.2905\n",
            "Epoch 331/500, Loss: 2.2894\n",
            "Epoch 332/500, Loss: 2.3098\n",
            "Epoch 333/500, Loss: 2.2911\n",
            "Epoch 334/500, Loss: 2.3170\n",
            "Epoch 335/500, Loss: 2.2728\n",
            "Epoch 336/500, Loss: 2.3174\n",
            "Epoch 337/500, Loss: 2.3205\n",
            "Epoch 338/500, Loss: 2.3529\n",
            "Epoch 339/500, Loss: 2.2938\n",
            "Epoch 340/500, Loss: 2.2906\n",
            "Epoch 341/500, Loss: 2.3498\n",
            "Epoch 342/500, Loss: 2.2939\n",
            "Epoch 343/500, Loss: 2.2583\n",
            "Epoch 344/500, Loss: 2.2666\n",
            "Epoch 345/500, Loss: 2.3275\n",
            "Epoch 346/500, Loss: 2.2846\n",
            "Epoch 347/500, Loss: 2.2869\n",
            "Epoch 348/500, Loss: 2.3251\n",
            "Epoch 349/500, Loss: 2.2859\n",
            "Epoch 350/500, Loss: 2.3216\n",
            "Epoch 351/500, Loss: 2.2845\n",
            "Epoch 352/500, Loss: 2.2582\n",
            "Epoch 353/500, Loss: 2.2645\n",
            "Epoch 354/500, Loss: 2.2598\n",
            "Epoch 355/500, Loss: 2.2618\n",
            "Epoch 356/500, Loss: 2.3051\n",
            "Epoch 357/500, Loss: 2.2482\n",
            "Epoch 358/500, Loss: 2.2954\n",
            "Epoch 359/500, Loss: 2.2433\n",
            "Epoch 360/500, Loss: 2.2525\n",
            "Epoch 361/500, Loss: 2.2611\n",
            "Epoch 362/500, Loss: 2.2404\n",
            "Epoch 363/500, Loss: 2.2667\n",
            "Epoch 364/500, Loss: 2.2708\n",
            "Epoch 365/500, Loss: 2.2920\n",
            "Epoch 366/500, Loss: 2.2729\n",
            "Epoch 367/500, Loss: 2.2812\n",
            "Epoch 368/500, Loss: 2.2695\n",
            "Epoch 369/500, Loss: 2.2646\n",
            "Epoch 370/500, Loss: 2.3069\n",
            "Epoch 371/500, Loss: 2.2526\n",
            "Epoch 372/500, Loss: 2.2653\n",
            "Epoch 373/500, Loss: 2.2806\n",
            "Epoch 374/500, Loss: 2.2820\n",
            "Epoch 375/500, Loss: 2.2920\n",
            "Epoch 376/500, Loss: 2.2832\n",
            "Epoch 377/500, Loss: 2.2848\n",
            "Epoch 378/500, Loss: 2.2695\n",
            "Epoch 379/500, Loss: 2.2901\n",
            "Epoch 380/500, Loss: 2.2666\n",
            "Epoch 381/500, Loss: 2.2618\n",
            "Epoch 382/500, Loss: 2.3017\n",
            "Epoch 383/500, Loss: 2.2583\n",
            "Epoch 384/500, Loss: 2.2806\n",
            "Epoch 385/500, Loss: 2.2639\n",
            "Epoch 386/500, Loss: 2.2386\n",
            "Epoch 387/500, Loss: 2.2400\n",
            "Epoch 388/500, Loss: 2.2663\n",
            "Epoch 389/500, Loss: 2.2612\n",
            "Epoch 390/500, Loss: 2.2429\n",
            "Epoch 391/500, Loss: 2.2344\n",
            "Epoch 392/500, Loss: 2.2463\n",
            "Epoch 393/500, Loss: 2.2374\n",
            "Epoch 394/500, Loss: 2.2110\n",
            "Epoch 395/500, Loss: 2.3315\n",
            "Epoch 396/500, Loss: 2.2104\n",
            "Epoch 397/500, Loss: 2.2360\n",
            "Epoch 398/500, Loss: 2.2645\n",
            "Epoch 399/500, Loss: 2.2750\n",
            "Epoch 400/500, Loss: 2.2562\n",
            "Epoch 401/500, Loss: 2.2094\n",
            "Epoch 402/500, Loss: 2.1948\n",
            "Epoch 403/500, Loss: 2.2743\n",
            "Epoch 404/500, Loss: 2.2895\n",
            "Epoch 405/500, Loss: 2.3101\n",
            "Epoch 406/500, Loss: 2.2634\n",
            "Epoch 407/500, Loss: 2.2479\n",
            "Epoch 408/500, Loss: 2.2475\n",
            "Epoch 409/500, Loss: 2.2590\n",
            "Epoch 410/500, Loss: 2.2402\n",
            "Epoch 411/500, Loss: 2.2152\n",
            "Epoch 412/500, Loss: 2.2320\n",
            "Epoch 413/500, Loss: 2.2542\n",
            "Epoch 414/500, Loss: 2.2553\n",
            "Epoch 415/500, Loss: 2.1888\n",
            "Epoch 416/500, Loss: 2.2656\n",
            "Epoch 417/500, Loss: 2.2299\n",
            "Epoch 418/500, Loss: 2.2225\n",
            "Epoch 419/500, Loss: 2.2329\n",
            "Epoch 420/500, Loss: 2.2145\n",
            "Epoch 421/500, Loss: 2.1867\n",
            "Epoch 422/500, Loss: 2.2042\n",
            "Epoch 423/500, Loss: 2.2358\n",
            "Epoch 424/500, Loss: 2.2399\n",
            "Epoch 425/500, Loss: 2.2185\n",
            "Epoch 426/500, Loss: 2.1883\n",
            "Epoch 427/500, Loss: 2.2075\n",
            "Epoch 428/500, Loss: 2.2388\n",
            "Epoch 429/500, Loss: 2.2579\n",
            "Epoch 430/500, Loss: 2.2249\n",
            "Epoch 431/500, Loss: 2.2310\n",
            "Epoch 432/500, Loss: 2.1878\n",
            "Epoch 433/500, Loss: 2.2122\n",
            "Epoch 434/500, Loss: 2.2677\n",
            "Epoch 435/500, Loss: 2.2060\n",
            "Epoch 436/500, Loss: 2.2362\n",
            "Epoch 437/500, Loss: 2.2595\n",
            "Epoch 438/500, Loss: 2.2382\n",
            "Epoch 439/500, Loss: 2.2113\n",
            "Epoch 440/500, Loss: 2.2239\n",
            "Epoch 441/500, Loss: 2.1970\n",
            "Epoch 442/500, Loss: 2.1964\n",
            "Epoch 443/500, Loss: 2.2045\n",
            "Epoch 444/500, Loss: 2.1886\n",
            "Epoch 445/500, Loss: 2.1986\n",
            "Epoch 446/500, Loss: 2.1983\n",
            "Epoch 447/500, Loss: 2.1645\n",
            "Epoch 448/500, Loss: 2.1940\n",
            "Epoch 449/500, Loss: 2.2062\n",
            "Epoch 450/500, Loss: 2.2254\n",
            "Epoch 451/500, Loss: 2.1602\n",
            "Epoch 452/500, Loss: 2.1681\n",
            "Epoch 453/500, Loss: 2.1777\n",
            "Epoch 454/500, Loss: 2.1595\n",
            "Epoch 455/500, Loss: 2.2343\n",
            "Epoch 456/500, Loss: 2.1834\n",
            "Epoch 457/500, Loss: 2.2126\n",
            "Epoch 458/500, Loss: 2.1978\n",
            "Epoch 459/500, Loss: 2.1901\n",
            "Epoch 460/500, Loss: 2.1774\n",
            "Epoch 461/500, Loss: 2.0896\n",
            "Epoch 462/500, Loss: 2.1694\n",
            "Epoch 463/500, Loss: 2.1770\n",
            "Epoch 464/500, Loss: 2.2154\n",
            "Epoch 465/500, Loss: 2.1794\n",
            "Epoch 466/500, Loss: 2.1673\n",
            "Epoch 467/500, Loss: 2.2256\n",
            "Epoch 468/500, Loss: 2.1835\n",
            "Epoch 469/500, Loss: 2.1806\n",
            "Epoch 470/500, Loss: 2.1231\n",
            "Epoch 471/500, Loss: 2.1917\n",
            "Epoch 472/500, Loss: 2.1937\n",
            "Epoch 473/500, Loss: 2.1389\n",
            "Epoch 474/500, Loss: 2.1694\n",
            "Epoch 475/500, Loss: 2.1740\n",
            "Epoch 476/500, Loss: 2.1635\n",
            "Epoch 477/500, Loss: 2.1416\n",
            "Epoch 478/500, Loss: 2.1623\n",
            "Epoch 479/500, Loss: 2.1476\n",
            "Epoch 480/500, Loss: 2.1572\n",
            "Epoch 481/500, Loss: 2.1753\n",
            "Epoch 482/500, Loss: 2.1664\n",
            "Epoch 483/500, Loss: 2.1448\n",
            "Epoch 484/500, Loss: 2.1236\n",
            "Epoch 485/500, Loss: 2.1628\n",
            "Epoch 486/500, Loss: 2.1465\n",
            "Epoch 487/500, Loss: 2.1318\n",
            "Epoch 488/500, Loss: 2.1463\n",
            "Epoch 489/500, Loss: 2.1762\n",
            "Epoch 490/500, Loss: 2.1427\n",
            "Epoch 491/500, Loss: 2.1391\n",
            "Epoch 492/500, Loss: 2.1398\n",
            "Epoch 493/500, Loss: 2.1356\n",
            "Epoch 494/500, Loss: 2.0805\n",
            "Epoch 495/500, Loss: 2.1345\n",
            "Epoch 496/500, Loss: 2.1276\n",
            "Epoch 497/500, Loss: 2.1165\n",
            "Epoch 498/500, Loss: 2.1932\n",
            "Epoch 499/500, Loss: 2.0938\n",
            "Epoch 500/500, Loss: 2.1717\n",
            "{'0': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, '1': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}, 'weighted avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 2, attention_layers = 4, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 128, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 500, learning_rate = 1e-4)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeriB2MwiKx1",
        "outputId": "9f4669ed-cb82-4322-bbbd-c43f5c81d23e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 2.8919\n",
            "Epoch 2/200, Loss: 2.7662\n",
            "Epoch 3/200, Loss: 2.7968\n",
            "Epoch 4/200, Loss: 2.8220\n",
            "Epoch 5/200, Loss: 2.7400\n",
            "Epoch 6/200, Loss: 2.7244\n",
            "Epoch 7/200, Loss: 2.6967\n",
            "Epoch 8/200, Loss: 2.6689\n",
            "Epoch 9/200, Loss: 2.6635\n",
            "Epoch 10/200, Loss: 2.6332\n",
            "Epoch 11/200, Loss: 2.6440\n",
            "Epoch 12/200, Loss: 2.6027\n",
            "Epoch 13/200, Loss: 2.5926\n",
            "Epoch 14/200, Loss: 2.5853\n",
            "Epoch 15/200, Loss: 2.5820\n",
            "Epoch 16/200, Loss: 2.5746\n",
            "Epoch 17/200, Loss: 2.5662\n",
            "Epoch 18/200, Loss: 2.5774\n",
            "Epoch 19/200, Loss: 2.5443\n",
            "Epoch 20/200, Loss: 2.5231\n",
            "Epoch 21/200, Loss: 2.5590\n",
            "Epoch 22/200, Loss: 2.5383\n",
            "Epoch 23/200, Loss: 2.5069\n",
            "Epoch 24/200, Loss: 2.5361\n",
            "Epoch 25/200, Loss: 2.5473\n",
            "Epoch 26/200, Loss: 2.5316\n",
            "Epoch 27/200, Loss: 2.5638\n",
            "Epoch 28/200, Loss: 2.5502\n",
            "Epoch 29/200, Loss: 2.5590\n",
            "Epoch 30/200, Loss: 2.5467\n",
            "Epoch 31/200, Loss: 2.4854\n",
            "Epoch 32/200, Loss: 2.5150\n",
            "Epoch 33/200, Loss: 2.5168\n",
            "Epoch 34/200, Loss: 2.5273\n",
            "Epoch 35/200, Loss: 2.5336\n",
            "Epoch 36/200, Loss: 2.6182\n",
            "Epoch 37/200, Loss: 2.5097\n",
            "Epoch 38/200, Loss: 2.4689\n",
            "Epoch 39/200, Loss: 2.5616\n",
            "Epoch 40/200, Loss: 2.5105\n",
            "Epoch 41/200, Loss: 2.5088\n",
            "Epoch 42/200, Loss: 2.6108\n",
            "Epoch 43/200, Loss: 2.4815\n",
            "Epoch 44/200, Loss: 2.5806\n",
            "Epoch 45/200, Loss: 2.5262\n",
            "Epoch 46/200, Loss: 2.4969\n",
            "Epoch 47/200, Loss: 2.5047\n",
            "Epoch 48/200, Loss: 2.5030\n",
            "Epoch 49/200, Loss: 2.4816\n",
            "Epoch 50/200, Loss: 2.4891\n",
            "Epoch 51/200, Loss: 2.4820\n",
            "Epoch 52/200, Loss: 2.4538\n",
            "Epoch 53/200, Loss: 2.4766\n",
            "Epoch 54/200, Loss: 2.4704\n",
            "Epoch 55/200, Loss: 2.4902\n",
            "Epoch 56/200, Loss: 2.5188\n",
            "Epoch 57/200, Loss: 2.4820\n",
            "Epoch 58/200, Loss: 2.4305\n",
            "Epoch 59/200, Loss: 2.4529\n",
            "Epoch 60/200, Loss: 2.4587\n",
            "Epoch 61/200, Loss: 2.4489\n",
            "Epoch 62/200, Loss: 2.4596\n",
            "Epoch 63/200, Loss: 2.4749\n",
            "Epoch 64/200, Loss: 2.4460\n",
            "Epoch 65/200, Loss: 2.4671\n",
            "Epoch 66/200, Loss: 2.4251\n",
            "Epoch 67/200, Loss: 2.4242\n",
            "Epoch 68/200, Loss: 2.4347\n",
            "Epoch 69/200, Loss: 2.4218\n",
            "Epoch 70/200, Loss: 2.4417\n",
            "Epoch 71/200, Loss: 2.5049\n",
            "Epoch 72/200, Loss: 2.3997\n",
            "Epoch 73/200, Loss: 2.4396\n",
            "Epoch 74/200, Loss: 2.3984\n",
            "Epoch 75/200, Loss: 2.4183\n",
            "Epoch 76/200, Loss: 2.4058\n",
            "Epoch 77/200, Loss: 2.4152\n",
            "Epoch 78/200, Loss: 2.4151\n",
            "Epoch 79/200, Loss: 2.3873\n",
            "Epoch 80/200, Loss: 2.3959\n",
            "Epoch 81/200, Loss: 2.3698\n",
            "Epoch 82/200, Loss: 2.3799\n",
            "Epoch 83/200, Loss: 2.3984\n",
            "Epoch 84/200, Loss: 2.4141\n",
            "Epoch 85/200, Loss: 2.3778\n",
            "Epoch 86/200, Loss: 2.3696\n",
            "Epoch 87/200, Loss: 2.3871\n",
            "Epoch 88/200, Loss: 2.3583\n",
            "Epoch 89/200, Loss: 2.3821\n",
            "Epoch 90/200, Loss: 2.3745\n",
            "Epoch 91/200, Loss: 2.3649\n",
            "Epoch 92/200, Loss: 2.3683\n",
            "Epoch 93/200, Loss: 2.3388\n",
            "Epoch 94/200, Loss: 2.3382\n",
            "Epoch 95/200, Loss: 2.3493\n",
            "Epoch 96/200, Loss: 2.3393\n",
            "Epoch 97/200, Loss: 2.3671\n",
            "Epoch 98/200, Loss: 2.3401\n",
            "Epoch 99/200, Loss: 2.3557\n",
            "Epoch 100/200, Loss: 2.3396\n",
            "Epoch 101/200, Loss: 2.3653\n",
            "Epoch 102/200, Loss: 2.3731\n",
            "Epoch 103/200, Loss: 2.3177\n",
            "Epoch 104/200, Loss: 2.3630\n",
            "Epoch 105/200, Loss: 2.3273\n",
            "Epoch 106/200, Loss: 2.3234\n",
            "Epoch 107/200, Loss: 2.3321\n",
            "Epoch 108/200, Loss: 2.3064\n",
            "Epoch 109/200, Loss: 2.3011\n",
            "Epoch 110/200, Loss: 2.2856\n",
            "Epoch 111/200, Loss: 2.3021\n",
            "Epoch 112/200, Loss: 2.2885\n",
            "Epoch 113/200, Loss: 2.2751\n",
            "Epoch 114/200, Loss: 2.2875\n",
            "Epoch 115/200, Loss: 2.3334\n",
            "Epoch 116/200, Loss: 2.2947\n",
            "Epoch 117/200, Loss: 2.3055\n",
            "Epoch 118/200, Loss: 2.3150\n",
            "Epoch 119/200, Loss: 2.3046\n",
            "Epoch 120/200, Loss: 2.2938\n",
            "Epoch 121/200, Loss: 2.2973\n",
            "Epoch 122/200, Loss: 2.3048\n",
            "Epoch 123/200, Loss: 2.2851\n",
            "Epoch 124/200, Loss: 2.3101\n",
            "Epoch 125/200, Loss: 2.3021\n",
            "Epoch 126/200, Loss: 2.2819\n",
            "Epoch 127/200, Loss: 2.2765\n",
            "Epoch 128/200, Loss: 2.2895\n",
            "Epoch 129/200, Loss: 2.2410\n",
            "Epoch 130/200, Loss: 2.2114\n",
            "Epoch 131/200, Loss: 2.2287\n",
            "Epoch 132/200, Loss: 2.2785\n",
            "Epoch 133/200, Loss: 2.2188\n",
            "Epoch 134/200, Loss: 2.2998\n",
            "Epoch 135/200, Loss: 2.2873\n",
            "Epoch 136/200, Loss: 2.2947\n",
            "Epoch 137/200, Loss: 2.3167\n",
            "Epoch 138/200, Loss: 2.2757\n",
            "Epoch 139/200, Loss: 2.2821\n",
            "Epoch 140/200, Loss: 2.2537\n",
            "Epoch 141/200, Loss: 2.2391\n",
            "Epoch 142/200, Loss: 2.2247\n",
            "Epoch 143/200, Loss: 2.2695\n",
            "Epoch 144/200, Loss: 2.2410\n",
            "Epoch 145/200, Loss: 2.2218\n",
            "Epoch 146/200, Loss: 2.2315\n",
            "Epoch 147/200, Loss: 2.2710\n",
            "Epoch 148/200, Loss: 2.2361\n",
            "Epoch 149/200, Loss: 2.2192\n",
            "Epoch 150/200, Loss: 2.2649\n",
            "Epoch 151/200, Loss: 2.2698\n",
            "Epoch 152/200, Loss: 2.2224\n",
            "Epoch 153/200, Loss: 2.2721\n",
            "Epoch 154/200, Loss: 2.2474\n",
            "Epoch 155/200, Loss: 2.2139\n",
            "Epoch 156/200, Loss: 2.2482\n",
            "Epoch 157/200, Loss: 2.2153\n",
            "Epoch 158/200, Loss: 2.2043\n",
            "Epoch 159/200, Loss: 2.2021\n",
            "Epoch 160/200, Loss: 2.1460\n",
            "Epoch 161/200, Loss: 2.2101\n",
            "Epoch 162/200, Loss: 2.2385\n",
            "Epoch 163/200, Loss: 2.2066\n",
            "Epoch 164/200, Loss: 2.2318\n",
            "Epoch 165/200, Loss: 2.2330\n",
            "Epoch 166/200, Loss: 2.1864\n",
            "Epoch 167/200, Loss: 2.2099\n",
            "Epoch 168/200, Loss: 2.1774\n",
            "Epoch 169/200, Loss: 2.1972\n",
            "Epoch 170/200, Loss: 2.1877\n",
            "Epoch 171/200, Loss: 2.1971\n",
            "Epoch 172/200, Loss: 2.1870\n",
            "Epoch 173/200, Loss: 2.2428\n",
            "Epoch 174/200, Loss: 2.1968\n",
            "Epoch 175/200, Loss: 2.1809\n",
            "Epoch 176/200, Loss: 2.1921\n",
            "Epoch 177/200, Loss: 2.1945\n",
            "Epoch 178/200, Loss: 2.1695\n",
            "Epoch 179/200, Loss: 2.1372\n",
            "Epoch 180/200, Loss: 2.1540\n",
            "Epoch 181/200, Loss: 2.1622\n",
            "Epoch 182/200, Loss: 2.1881\n",
            "Epoch 183/200, Loss: 2.1527\n",
            "Epoch 184/200, Loss: 2.1725\n",
            "Epoch 185/200, Loss: 2.1800\n",
            "Epoch 186/200, Loss: 2.1550\n",
            "Epoch 187/200, Loss: 2.1639\n",
            "Epoch 188/200, Loss: 2.1385\n",
            "Epoch 189/200, Loss: 2.1946\n",
            "Epoch 190/200, Loss: 2.1688\n",
            "Epoch 191/200, Loss: 2.1675\n",
            "Epoch 192/200, Loss: 2.1555\n",
            "Epoch 193/200, Loss: 2.1804\n",
            "Epoch 194/200, Loss: 2.2020\n",
            "Epoch 195/200, Loss: 2.1471\n",
            "Epoch 196/200, Loss: 2.1528\n",
            "Epoch 197/200, Loss: 2.1667\n",
            "Epoch 198/200, Loss: 2.1814\n",
            "Epoch 199/200, Loss: 2.1340\n",
            "Epoch 200/200, Loss: 2.1226\n",
            "{'0': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, '1': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}, 'weighted avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 3, attention_layers = 4, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 128, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 200, learning_rate = 1e-4)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VZmXuaflJ7J",
        "outputId": "d0fad451-6879-475e-e45e-3bcac8d6313b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400, Loss: 2.7935\n",
            "Epoch 2/400, Loss: 2.7734\n",
            "Epoch 3/400, Loss: 2.7660\n",
            "Epoch 4/400, Loss: 2.7646\n",
            "Epoch 5/400, Loss: 2.7607\n",
            "Epoch 6/400, Loss: 2.7585\n",
            "Epoch 7/400, Loss: 2.7545\n",
            "Epoch 8/400, Loss: 2.7527\n",
            "Epoch 9/400, Loss: 2.7505\n",
            "Epoch 10/400, Loss: 2.7447\n",
            "Epoch 11/400, Loss: 2.7452\n",
            "Epoch 12/400, Loss: 2.7440\n",
            "Epoch 13/400, Loss: 2.7375\n",
            "Epoch 14/400, Loss: 2.7334\n",
            "Epoch 15/400, Loss: 2.7348\n",
            "Epoch 16/400, Loss: 2.7300\n",
            "Epoch 17/400, Loss: 2.7255\n",
            "Epoch 18/400, Loss: 2.7304\n",
            "Epoch 19/400, Loss: 2.7219\n",
            "Epoch 20/400, Loss: 2.7155\n",
            "Epoch 21/400, Loss: 2.7143\n",
            "Epoch 22/400, Loss: 2.7089\n",
            "Epoch 23/400, Loss: 2.7024\n",
            "Epoch 24/400, Loss: 2.7037\n",
            "Epoch 25/400, Loss: 2.7066\n",
            "Epoch 26/400, Loss: 2.6919\n",
            "Epoch 27/400, Loss: 2.6971\n",
            "Epoch 28/400, Loss: 2.6920\n",
            "Epoch 29/400, Loss: 2.6940\n",
            "Epoch 30/400, Loss: 2.6875\n",
            "Epoch 31/400, Loss: 2.6656\n",
            "Epoch 32/400, Loss: 2.6748\n",
            "Epoch 33/400, Loss: 2.6732\n",
            "Epoch 34/400, Loss: 2.6747\n",
            "Epoch 35/400, Loss: 2.6712\n",
            "Epoch 36/400, Loss: 2.7018\n",
            "Epoch 37/400, Loss: 2.6568\n",
            "Epoch 38/400, Loss: 2.6406\n",
            "Epoch 39/400, Loss: 2.6686\n",
            "Epoch 40/400, Loss: 2.6668\n",
            "Epoch 41/400, Loss: 2.6497\n",
            "Epoch 42/400, Loss: 2.6886\n",
            "Epoch 43/400, Loss: 2.6349\n",
            "Epoch 44/400, Loss: 2.6473\n",
            "Epoch 45/400, Loss: 2.6311\n",
            "Epoch 46/400, Loss: 2.6336\n",
            "Epoch 47/400, Loss: 2.6201\n",
            "Epoch 48/400, Loss: 2.6162\n",
            "Epoch 49/400, Loss: 2.6079\n",
            "Epoch 50/400, Loss: 2.6081\n",
            "Epoch 51/400, Loss: 2.6111\n",
            "Epoch 52/400, Loss: 2.5938\n",
            "Epoch 53/400, Loss: 2.5967\n",
            "Epoch 54/400, Loss: 2.5907\n",
            "Epoch 55/400, Loss: 2.5989\n",
            "Epoch 56/400, Loss: 2.6143\n",
            "Epoch 57/400, Loss: 2.5879\n",
            "Epoch 58/400, Loss: 2.5625\n",
            "Epoch 59/400, Loss: 2.5679\n",
            "Epoch 60/400, Loss: 2.5708\n",
            "Epoch 61/400, Loss: 2.5588\n",
            "Epoch 62/400, Loss: 2.5758\n",
            "Epoch 63/400, Loss: 2.5904\n",
            "Epoch 64/400, Loss: 2.5730\n",
            "Epoch 65/400, Loss: 2.5810\n",
            "Epoch 66/400, Loss: 2.5443\n",
            "Epoch 67/400, Loss: 2.5445\n",
            "Epoch 68/400, Loss: 2.5559\n",
            "Epoch 69/400, Loss: 2.5268\n",
            "Epoch 70/400, Loss: 2.5630\n",
            "Epoch 71/400, Loss: 2.6057\n",
            "Epoch 72/400, Loss: 2.5280\n",
            "Epoch 73/400, Loss: 2.5667\n",
            "Epoch 74/400, Loss: 2.5129\n",
            "Epoch 75/400, Loss: 2.5500\n",
            "Epoch 76/400, Loss: 2.5381\n",
            "Epoch 77/400, Loss: 2.5307\n",
            "Epoch 78/400, Loss: 2.5413\n",
            "Epoch 79/400, Loss: 2.5134\n",
            "Epoch 80/400, Loss: 2.5386\n",
            "Epoch 81/400, Loss: 2.5133\n",
            "Epoch 82/400, Loss: 2.5061\n",
            "Epoch 83/400, Loss: 2.5355\n",
            "Epoch 84/400, Loss: 2.5551\n",
            "Epoch 85/400, Loss: 2.5265\n",
            "Epoch 86/400, Loss: 2.5090\n",
            "Epoch 87/400, Loss: 2.5398\n",
            "Epoch 88/400, Loss: 2.5078\n",
            "Epoch 89/400, Loss: 2.5365\n",
            "Epoch 90/400, Loss: 2.5236\n",
            "Epoch 91/400, Loss: 2.5238\n",
            "Epoch 92/400, Loss: 2.5169\n",
            "Epoch 93/400, Loss: 2.5124\n",
            "Epoch 94/400, Loss: 2.4936\n",
            "Epoch 95/400, Loss: 2.5167\n",
            "Epoch 96/400, Loss: 2.4945\n",
            "Epoch 97/400, Loss: 2.5292\n",
            "Epoch 98/400, Loss: 2.5121\n",
            "Epoch 99/400, Loss: 2.5280\n",
            "Epoch 100/400, Loss: 2.5294\n",
            "Epoch 101/400, Loss: 2.5474\n",
            "Epoch 102/400, Loss: 2.5521\n",
            "Epoch 103/400, Loss: 2.4886\n",
            "Epoch 104/400, Loss: 2.5517\n",
            "Epoch 105/400, Loss: 2.5235\n",
            "Epoch 106/400, Loss: 2.5211\n",
            "Epoch 107/400, Loss: 2.5336\n",
            "Epoch 108/400, Loss: 2.4958\n",
            "Epoch 109/400, Loss: 2.4920\n",
            "Epoch 110/400, Loss: 2.4891\n",
            "Epoch 111/400, Loss: 2.5112\n",
            "Epoch 112/400, Loss: 2.4884\n",
            "Epoch 113/400, Loss: 2.4819\n",
            "Epoch 114/400, Loss: 2.5036\n",
            "Epoch 115/400, Loss: 2.5336\n",
            "Epoch 116/400, Loss: 2.4875\n",
            "Epoch 117/400, Loss: 2.5176\n",
            "Epoch 118/400, Loss: 2.5026\n",
            "Epoch 119/400, Loss: 2.5028\n",
            "Epoch 120/400, Loss: 2.5065\n",
            "Epoch 121/400, Loss: 2.5286\n",
            "Epoch 122/400, Loss: 2.5463\n",
            "Epoch 123/400, Loss: 2.4913\n",
            "Epoch 124/400, Loss: 2.5069\n",
            "Epoch 125/400, Loss: 2.5173\n",
            "Epoch 126/400, Loss: 2.5032\n",
            "Epoch 127/400, Loss: 2.4769\n",
            "Epoch 128/400, Loss: 2.4893\n",
            "Epoch 129/400, Loss: 2.4769\n",
            "Epoch 130/400, Loss: 2.4530\n",
            "Epoch 131/400, Loss: 2.4539\n",
            "Epoch 132/400, Loss: 2.5030\n",
            "Epoch 133/400, Loss: 2.4514\n",
            "Epoch 134/400, Loss: 2.4904\n",
            "Epoch 135/400, Loss: 2.5333\n",
            "Epoch 136/400, Loss: 2.5246\n",
            "Epoch 137/400, Loss: 2.5400\n",
            "Epoch 138/400, Loss: 2.5173\n",
            "Epoch 139/400, Loss: 2.5359\n",
            "Epoch 140/400, Loss: 2.4926\n",
            "Epoch 141/400, Loss: 2.4775\n",
            "Epoch 142/400, Loss: 2.4541\n",
            "Epoch 143/400, Loss: 2.4981\n",
            "Epoch 144/400, Loss: 2.4970\n",
            "Epoch 145/400, Loss: 2.4750\n",
            "Epoch 146/400, Loss: 2.4898\n",
            "Epoch 147/400, Loss: 2.4923\n",
            "Epoch 148/400, Loss: 2.4623\n",
            "Epoch 149/400, Loss: 2.4578\n",
            "Epoch 150/400, Loss: 2.4848\n",
            "Epoch 151/400, Loss: 2.4978\n",
            "Epoch 152/400, Loss: 2.4916\n",
            "Epoch 153/400, Loss: 2.5134\n",
            "Epoch 154/400, Loss: 2.4857\n",
            "Epoch 155/400, Loss: 2.4955\n",
            "Epoch 156/400, Loss: 2.4910\n",
            "Epoch 157/400, Loss: 2.4886\n",
            "Epoch 158/400, Loss: 2.4518\n",
            "Epoch 159/400, Loss: 2.4370\n",
            "Epoch 160/400, Loss: 2.4316\n",
            "Epoch 161/400, Loss: 2.4680\n",
            "Epoch 162/400, Loss: 2.4857\n",
            "Epoch 163/400, Loss: 2.4582\n",
            "Epoch 164/400, Loss: 2.4586\n",
            "Epoch 165/400, Loss: 2.4870\n",
            "Epoch 166/400, Loss: 2.4656\n",
            "Epoch 167/400, Loss: 2.4765\n",
            "Epoch 168/400, Loss: 2.4299\n",
            "Epoch 169/400, Loss: 2.4854\n",
            "Epoch 170/400, Loss: 2.4498\n",
            "Epoch 171/400, Loss: 2.4676\n",
            "Epoch 172/400, Loss: 2.4477\n",
            "Epoch 173/400, Loss: 2.4936\n",
            "Epoch 174/400, Loss: 2.4546\n",
            "Epoch 175/400, Loss: 2.4302\n",
            "Epoch 176/400, Loss: 2.4602\n",
            "Epoch 177/400, Loss: 2.4533\n",
            "Epoch 178/400, Loss: 2.4981\n",
            "Epoch 179/400, Loss: 2.4188\n",
            "Epoch 180/400, Loss: 2.4301\n",
            "Epoch 181/400, Loss: 2.4244\n",
            "Epoch 182/400, Loss: 2.4340\n",
            "Epoch 183/400, Loss: 2.4203\n",
            "Epoch 184/400, Loss: 2.4597\n",
            "Epoch 185/400, Loss: 2.4481\n",
            "Epoch 186/400, Loss: 2.4519\n",
            "Epoch 187/400, Loss: 2.4307\n",
            "Epoch 188/400, Loss: 2.4007\n",
            "Epoch 189/400, Loss: 2.4797\n",
            "Epoch 190/400, Loss: 2.4407\n",
            "Epoch 191/400, Loss: 2.4412\n",
            "Epoch 192/400, Loss: 2.4537\n",
            "Epoch 193/400, Loss: 2.4494\n",
            "Epoch 194/400, Loss: 2.4548\n",
            "Epoch 195/400, Loss: 2.4680\n",
            "Epoch 196/400, Loss: 2.4309\n",
            "Epoch 197/400, Loss: 2.4310\n",
            "Epoch 198/400, Loss: 2.4835\n",
            "Epoch 199/400, Loss: 2.4332\n",
            "Epoch 200/400, Loss: 2.4330\n",
            "Epoch 201/400, Loss: 2.4597\n",
            "Epoch 202/400, Loss: 2.4132\n",
            "Epoch 203/400, Loss: 2.4371\n",
            "Epoch 204/400, Loss: 2.4248\n",
            "Epoch 205/400, Loss: 2.4151\n",
            "Epoch 206/400, Loss: 2.4156\n",
            "Epoch 207/400, Loss: 2.4184\n",
            "Epoch 208/400, Loss: 2.4230\n",
            "Epoch 209/400, Loss: 2.4081\n",
            "Epoch 210/400, Loss: 2.4155\n",
            "Epoch 211/400, Loss: 2.4507\n",
            "Epoch 212/400, Loss: 2.3987\n",
            "Epoch 213/400, Loss: 2.3982\n",
            "Epoch 214/400, Loss: 2.4289\n",
            "Epoch 215/400, Loss: 2.4909\n",
            "Epoch 216/400, Loss: 2.4056\n",
            "Epoch 217/400, Loss: 2.4179\n",
            "Epoch 218/400, Loss: 2.4434\n",
            "Epoch 219/400, Loss: 2.3505\n",
            "Epoch 220/400, Loss: 2.3850\n",
            "Epoch 221/400, Loss: 2.3999\n",
            "Epoch 222/400, Loss: 2.3831\n",
            "Epoch 223/400, Loss: 2.4350\n",
            "Epoch 224/400, Loss: 2.3958\n",
            "Epoch 225/400, Loss: 2.3791\n",
            "Epoch 226/400, Loss: 2.3781\n",
            "Epoch 227/400, Loss: 2.4098\n",
            "Epoch 228/400, Loss: 2.3790\n",
            "Epoch 229/400, Loss: 2.3668\n",
            "Epoch 230/400, Loss: 2.3961\n",
            "Epoch 231/400, Loss: 2.3859\n",
            "Epoch 232/400, Loss: 2.3967\n",
            "Epoch 233/400, Loss: 2.3966\n",
            "Epoch 234/400, Loss: 2.3705\n",
            "Epoch 235/400, Loss: 2.3518\n",
            "Epoch 236/400, Loss: 2.3701\n",
            "Epoch 237/400, Loss: 2.3675\n",
            "Epoch 238/400, Loss: 2.3045\n",
            "Epoch 239/400, Loss: 2.3622\n",
            "Epoch 240/400, Loss: 2.3696\n",
            "Epoch 241/400, Loss: 2.3299\n",
            "Epoch 242/400, Loss: 2.3498\n",
            "Epoch 243/400, Loss: 2.3542\n",
            "Epoch 244/400, Loss: 2.3509\n",
            "Epoch 245/400, Loss: 2.4131\n",
            "Epoch 246/400, Loss: 2.3551\n",
            "Epoch 247/400, Loss: 2.4000\n",
            "Epoch 248/400, Loss: 2.3365\n",
            "Epoch 249/400, Loss: 2.3396\n",
            "Epoch 250/400, Loss: 2.3220\n",
            "Epoch 251/400, Loss: 2.3712\n",
            "Epoch 252/400, Loss: 2.3303\n",
            "Epoch 253/400, Loss: 2.3158\n",
            "Epoch 254/400, Loss: 2.3226\n",
            "Epoch 255/400, Loss: 2.3554\n",
            "Epoch 256/400, Loss: 2.3167\n",
            "Epoch 257/400, Loss: 2.3224\n",
            "Epoch 258/400, Loss: 2.3185\n",
            "Epoch 259/400, Loss: 2.3061\n",
            "Epoch 260/400, Loss: 2.3403\n",
            "Epoch 261/400, Loss: 2.3347\n",
            "Epoch 262/400, Loss: 2.2670\n",
            "Epoch 263/400, Loss: 2.3074\n",
            "Epoch 264/400, Loss: 2.3342\n",
            "Epoch 265/400, Loss: 2.2834\n",
            "Epoch 266/400, Loss: 2.2874\n",
            "Epoch 267/400, Loss: 2.2916\n",
            "Epoch 268/400, Loss: 2.2604\n",
            "Epoch 269/400, Loss: 2.2581\n",
            "Epoch 270/400, Loss: 2.2880\n",
            "Epoch 271/400, Loss: 2.2611\n",
            "Epoch 272/400, Loss: 2.2986\n",
            "Epoch 273/400, Loss: 2.3077\n",
            "Epoch 274/400, Loss: 2.2381\n",
            "Epoch 275/400, Loss: 2.2713\n",
            "Epoch 276/400, Loss: 2.2458\n",
            "Epoch 277/400, Loss: 2.2476\n",
            "Epoch 278/400, Loss: 2.2455\n",
            "Epoch 279/400, Loss: 2.2629\n",
            "Epoch 280/400, Loss: 2.2182\n",
            "Epoch 281/400, Loss: 2.2587\n",
            "Epoch 282/400, Loss: 2.2759\n",
            "Epoch 283/400, Loss: 2.2453\n",
            "Epoch 284/400, Loss: 2.2080\n",
            "Epoch 285/400, Loss: 2.2378\n",
            "Epoch 286/400, Loss: 2.2295\n",
            "Epoch 287/400, Loss: 2.1967\n",
            "Epoch 288/400, Loss: 2.2326\n",
            "Epoch 289/400, Loss: 2.2055\n",
            "Epoch 290/400, Loss: 2.2167\n",
            "Epoch 291/400, Loss: 2.2080\n",
            "Epoch 292/400, Loss: 2.2219\n",
            "Epoch 293/400, Loss: 2.2018\n",
            "Epoch 294/400, Loss: 2.1927\n",
            "Epoch 295/400, Loss: 2.2558\n",
            "Epoch 296/400, Loss: 2.2123\n",
            "Epoch 297/400, Loss: 2.2091\n",
            "Epoch 298/400, Loss: 2.1846\n",
            "Epoch 299/400, Loss: 2.2012\n",
            "Epoch 300/400, Loss: 2.1920\n",
            "Epoch 301/400, Loss: 2.1613\n",
            "Epoch 302/400, Loss: 2.1694\n",
            "Epoch 303/400, Loss: 2.1871\n",
            "Epoch 304/400, Loss: 2.2101\n",
            "Epoch 305/400, Loss: 2.1692\n",
            "Epoch 306/400, Loss: 2.1714\n",
            "Epoch 307/400, Loss: 2.1835\n",
            "Epoch 308/400, Loss: 2.1774\n",
            "Epoch 309/400, Loss: 2.1705\n",
            "Epoch 310/400, Loss: 2.1758\n",
            "Epoch 311/400, Loss: 2.2143\n",
            "Epoch 312/400, Loss: 2.1377\n",
            "Epoch 313/400, Loss: 2.1773\n",
            "Epoch 314/400, Loss: 2.1613\n",
            "Epoch 315/400, Loss: 2.1145\n",
            "Epoch 316/400, Loss: 2.1611\n",
            "Epoch 317/400, Loss: 2.1582\n",
            "Epoch 318/400, Loss: 2.1512\n",
            "Epoch 319/400, Loss: 2.1196\n",
            "Epoch 320/400, Loss: 2.1642\n",
            "Epoch 321/400, Loss: 2.1335\n",
            "Epoch 322/400, Loss: 2.1193\n",
            "Epoch 323/400, Loss: 2.1232\n",
            "Epoch 324/400, Loss: 2.1678\n",
            "Epoch 325/400, Loss: 2.1161\n",
            "Epoch 326/400, Loss: 2.1015\n",
            "Epoch 327/400, Loss: 2.1287\n",
            "Epoch 328/400, Loss: 2.1350\n",
            "Epoch 329/400, Loss: 2.1819\n",
            "Epoch 330/400, Loss: 2.1091\n",
            "Epoch 331/400, Loss: 2.1212\n",
            "Epoch 332/400, Loss: 2.1103\n",
            "Epoch 333/400, Loss: 2.1046\n",
            "Epoch 334/400, Loss: 2.1432\n",
            "Epoch 335/400, Loss: 2.0709\n",
            "Epoch 336/400, Loss: 2.1207\n",
            "Epoch 337/400, Loss: 2.1530\n",
            "Epoch 338/400, Loss: 2.1483\n",
            "Epoch 339/400, Loss: 2.0999\n",
            "Epoch 340/400, Loss: 2.0970\n",
            "Epoch 341/400, Loss: 2.1410\n",
            "Epoch 342/400, Loss: 2.1443\n",
            "Epoch 343/400, Loss: 2.0803\n",
            "Epoch 344/400, Loss: 2.0920\n",
            "Epoch 345/400, Loss: 2.1099\n",
            "Epoch 346/400, Loss: 2.0661\n",
            "Epoch 347/400, Loss: 2.0761\n",
            "Epoch 348/400, Loss: 2.1273\n",
            "Epoch 349/400, Loss: 2.1034\n",
            "Epoch 350/400, Loss: 2.0978\n",
            "Epoch 351/400, Loss: 2.0949\n",
            "Epoch 352/400, Loss: 2.0788\n",
            "Epoch 353/400, Loss: 2.0556\n",
            "Epoch 354/400, Loss: 2.0470\n",
            "Epoch 355/400, Loss: 2.0663\n",
            "Epoch 356/400, Loss: 2.0931\n",
            "Epoch 357/400, Loss: 2.0671\n",
            "Epoch 358/400, Loss: 2.0685\n",
            "Epoch 359/400, Loss: 2.0600\n",
            "Epoch 360/400, Loss: 2.0423\n",
            "Epoch 361/400, Loss: 2.0435\n",
            "Epoch 362/400, Loss: 2.0417\n",
            "Epoch 363/400, Loss: 2.0393\n",
            "Epoch 364/400, Loss: 2.0883\n",
            "Epoch 365/400, Loss: 2.0814\n",
            "Epoch 366/400, Loss: 2.0962\n",
            "Epoch 367/400, Loss: 2.0798\n",
            "Epoch 368/400, Loss: 2.0813\n",
            "Epoch 369/400, Loss: 2.0970\n",
            "Epoch 370/400, Loss: 2.0252\n",
            "Epoch 371/400, Loss: 2.0719\n",
            "Epoch 372/400, Loss: 2.0776\n",
            "Epoch 373/400, Loss: 2.0510\n",
            "Epoch 374/400, Loss: 2.0607\n",
            "Epoch 375/400, Loss: 2.0435\n",
            "Epoch 376/400, Loss: 2.0600\n",
            "Epoch 377/400, Loss: 2.0592\n",
            "Epoch 378/400, Loss: 2.0441\n",
            "Epoch 379/400, Loss: 2.0800\n",
            "Epoch 380/400, Loss: 2.0741\n",
            "Epoch 381/400, Loss: 2.0294\n",
            "Epoch 382/400, Loss: 2.0577\n",
            "Epoch 383/400, Loss: 2.0470\n",
            "Epoch 384/400, Loss: 2.0593\n",
            "Epoch 385/400, Loss: 2.0415\n",
            "Epoch 386/400, Loss: 2.0459\n",
            "Epoch 387/400, Loss: 2.0305\n",
            "Epoch 388/400, Loss: 2.0256\n",
            "Epoch 389/400, Loss: 2.0229\n",
            "Epoch 390/400, Loss: 2.0307\n",
            "Epoch 391/400, Loss: 2.0399\n",
            "Epoch 392/400, Loss: 2.0249\n",
            "Epoch 393/400, Loss: 2.0183\n",
            "Epoch 394/400, Loss: 1.9960\n",
            "Epoch 395/400, Loss: 2.0658\n",
            "Epoch 396/400, Loss: 1.9917\n",
            "Epoch 397/400, Loss: 2.0199\n",
            "Epoch 398/400, Loss: 2.0472\n",
            "Epoch 399/400, Loss: 2.0531\n",
            "Epoch 400/400, Loss: 2.0657\n",
            "{'0': {'precision': 0.8333333333333334, 'recall': 0.7142857142857143, 'f1-score': 0.7692307692307693, 'support': 7.0}, '1': {'precision': 0.75, 'recall': 0.8571428571428571, 'f1-score': 0.8, 'support': 7.0}, 'accuracy': 0.7857142857142857, 'macro avg': {'precision': 0.7916666666666667, 'recall': 0.7857142857142857, 'f1-score': 0.7846153846153847, 'support': 14.0}, 'weighted avg': {'precision': 0.7916666666666667, 'recall': 0.7857142857142857, 'f1-score': 0.7846153846153846, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 4, attention_layers = 6, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 400, learning_rate = 1e-5)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KRqN_9XaZx6M",
        "outputId": "be957871-a1d6-4f52-c0af-5dc8b7885061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400, Loss: 2.8018\n",
            "Epoch 2/400, Loss: 2.7858\n",
            "Epoch 3/400, Loss: 2.7779\n",
            "Epoch 4/400, Loss: 2.7860\n",
            "Epoch 5/400, Loss: 2.7754\n",
            "Epoch 6/400, Loss: 2.7722\n",
            "Epoch 7/400, Loss: 2.7704\n",
            "Epoch 8/400, Loss: 2.7659\n",
            "Epoch 9/400, Loss: 2.7673\n",
            "Epoch 10/400, Loss: 2.7619\n",
            "Epoch 11/400, Loss: 2.7605\n",
            "Epoch 12/400, Loss: 2.7630\n",
            "Epoch 13/400, Loss: 2.7547\n",
            "Epoch 14/400, Loss: 2.7492\n",
            "Epoch 15/400, Loss: 2.7514\n",
            "Epoch 16/400, Loss: 2.7468\n",
            "Epoch 17/400, Loss: 2.7434\n",
            "Epoch 18/400, Loss: 2.7432\n",
            "Epoch 19/400, Loss: 2.7412\n",
            "Epoch 20/400, Loss: 2.7321\n",
            "Epoch 21/400, Loss: 2.7297\n",
            "Epoch 22/400, Loss: 2.7259\n",
            "Epoch 23/400, Loss: 2.7241\n",
            "Epoch 24/400, Loss: 2.7232\n",
            "Epoch 25/400, Loss: 2.7254\n",
            "Epoch 26/400, Loss: 2.7149\n",
            "Epoch 27/400, Loss: 2.7182\n",
            "Epoch 28/400, Loss: 2.7137\n",
            "Epoch 29/400, Loss: 2.7161\n",
            "Epoch 30/400, Loss: 2.7101\n",
            "Epoch 31/400, Loss: 2.6960\n",
            "Epoch 32/400, Loss: 2.6961\n",
            "Epoch 33/400, Loss: 2.6973\n",
            "Epoch 34/400, Loss: 2.6939\n",
            "Epoch 35/400, Loss: 2.6925\n",
            "Epoch 36/400, Loss: 2.7185\n",
            "Epoch 37/400, Loss: 2.6814\n",
            "Epoch 38/400, Loss: 2.6651\n",
            "Epoch 39/400, Loss: 2.6903\n",
            "Epoch 40/400, Loss: 2.6897\n",
            "Epoch 41/400, Loss: 2.6758\n",
            "Epoch 42/400, Loss: 2.7008\n",
            "Epoch 43/400, Loss: 2.6596\n",
            "Epoch 44/400, Loss: 2.6709\n",
            "Epoch 45/400, Loss: 2.6506\n",
            "Epoch 46/400, Loss: 2.6564\n",
            "Epoch 47/400, Loss: 2.6460\n",
            "Epoch 48/400, Loss: 2.6398\n",
            "Epoch 49/400, Loss: 2.6317\n",
            "Epoch 50/400, Loss: 2.6327\n",
            "Epoch 51/400, Loss: 2.6348\n",
            "Epoch 52/400, Loss: 2.6218\n",
            "Epoch 53/400, Loss: 2.6197\n",
            "Epoch 54/400, Loss: 2.6159\n",
            "Epoch 55/400, Loss: 2.6180\n",
            "Epoch 56/400, Loss: 2.6292\n",
            "Epoch 57/400, Loss: 2.6069\n",
            "Epoch 58/400, Loss: 2.5795\n",
            "Epoch 59/400, Loss: 2.5875\n",
            "Epoch 60/400, Loss: 2.5939\n",
            "Epoch 61/400, Loss: 2.5792\n",
            "Epoch 62/400, Loss: 2.5879\n",
            "Epoch 63/400, Loss: 2.6053\n",
            "Epoch 64/400, Loss: 2.5835\n",
            "Epoch 65/400, Loss: 2.5857\n",
            "Epoch 66/400, Loss: 2.5538\n",
            "Epoch 67/400, Loss: 2.5513\n",
            "Epoch 68/400, Loss: 2.5602\n",
            "Epoch 69/400, Loss: 2.5362\n",
            "Epoch 70/400, Loss: 2.5647\n",
            "Epoch 71/400, Loss: 2.5999\n",
            "Epoch 72/400, Loss: 2.5218\n",
            "Epoch 73/400, Loss: 2.5592\n",
            "Epoch 74/400, Loss: 2.5102\n",
            "Epoch 75/400, Loss: 2.5420\n",
            "Epoch 76/400, Loss: 2.5310\n",
            "Epoch 77/400, Loss: 2.5253\n",
            "Epoch 78/400, Loss: 2.5299\n",
            "Epoch 79/400, Loss: 2.5005\n",
            "Epoch 80/400, Loss: 2.5179\n",
            "Epoch 81/400, Loss: 2.4971\n",
            "Epoch 82/400, Loss: 2.4926\n",
            "Epoch 83/400, Loss: 2.5178\n",
            "Epoch 84/400, Loss: 2.5331\n",
            "Epoch 85/400, Loss: 2.5051\n",
            "Epoch 86/400, Loss: 2.4903\n",
            "Epoch 87/400, Loss: 2.5118\n",
            "Epoch 88/400, Loss: 2.4809\n",
            "Epoch 89/400, Loss: 2.5129\n",
            "Epoch 90/400, Loss: 2.4952\n",
            "Epoch 91/400, Loss: 2.4958\n",
            "Epoch 92/400, Loss: 2.4869\n",
            "Epoch 93/400, Loss: 2.4769\n",
            "Epoch 94/400, Loss: 2.4593\n",
            "Epoch 95/400, Loss: 2.4876\n",
            "Epoch 96/400, Loss: 2.4606\n",
            "Epoch 97/400, Loss: 2.4946\n",
            "Epoch 98/400, Loss: 2.4715\n",
            "Epoch 99/400, Loss: 2.4896\n",
            "Epoch 100/400, Loss: 2.4856\n",
            "Epoch 101/400, Loss: 2.5021\n",
            "Epoch 102/400, Loss: 2.5142\n",
            "Epoch 103/400, Loss: 2.4470\n",
            "Epoch 104/400, Loss: 2.5073\n",
            "Epoch 105/400, Loss: 2.4863\n",
            "Epoch 106/400, Loss: 2.4747\n",
            "Epoch 107/400, Loss: 2.4882\n",
            "Epoch 108/400, Loss: 2.4482\n",
            "Epoch 109/400, Loss: 2.4464\n",
            "Epoch 110/400, Loss: 2.4415\n",
            "Epoch 111/400, Loss: 2.4608\n",
            "Epoch 112/400, Loss: 2.4353\n",
            "Epoch 113/400, Loss: 2.4281\n",
            "Epoch 114/400, Loss: 2.4456\n",
            "Epoch 115/400, Loss: 2.4732\n",
            "Epoch 116/400, Loss: 2.4298\n",
            "Epoch 117/400, Loss: 2.4567\n",
            "Epoch 118/400, Loss: 2.4387\n",
            "Epoch 119/400, Loss: 2.4439\n",
            "Epoch 120/400, Loss: 2.4473\n",
            "Epoch 121/400, Loss: 2.4635\n",
            "Epoch 122/400, Loss: 2.4839\n",
            "Epoch 123/400, Loss: 2.4208\n",
            "Epoch 124/400, Loss: 2.4406\n",
            "Epoch 125/400, Loss: 2.4486\n",
            "Epoch 126/400, Loss: 2.4334\n",
            "Epoch 127/400, Loss: 2.4036\n",
            "Epoch 128/400, Loss: 2.4138\n",
            "Epoch 129/400, Loss: 2.4039\n",
            "Epoch 130/400, Loss: 2.3660\n",
            "Epoch 131/400, Loss: 2.3740\n",
            "Epoch 132/400, Loss: 2.4250\n",
            "Epoch 133/400, Loss: 2.3697\n",
            "Epoch 134/400, Loss: 2.4097\n",
            "Epoch 135/400, Loss: 2.4441\n",
            "Epoch 136/400, Loss: 2.4427\n",
            "Epoch 137/400, Loss: 2.4558\n",
            "Epoch 138/400, Loss: 2.4281\n",
            "Epoch 139/400, Loss: 2.4544\n",
            "Epoch 140/400, Loss: 2.4004\n",
            "Epoch 141/400, Loss: 2.3852\n",
            "Epoch 142/400, Loss: 2.3547\n",
            "Epoch 143/400, Loss: 2.3987\n",
            "Epoch 144/400, Loss: 2.4040\n",
            "Epoch 145/400, Loss: 2.3600\n",
            "Epoch 146/400, Loss: 2.3867\n",
            "Epoch 147/400, Loss: 2.3857\n",
            "Epoch 148/400, Loss: 2.3511\n",
            "Epoch 149/400, Loss: 2.3383\n",
            "Epoch 150/400, Loss: 2.3654\n",
            "Epoch 151/400, Loss: 2.3816\n",
            "Epoch 152/400, Loss: 2.3712\n",
            "Epoch 153/400, Loss: 2.3989\n",
            "Epoch 154/400, Loss: 2.3715\n",
            "Epoch 155/400, Loss: 2.3785\n",
            "Epoch 156/400, Loss: 2.3529\n",
            "Epoch 157/400, Loss: 2.3439\n",
            "Epoch 158/400, Loss: 2.3028\n",
            "Epoch 159/400, Loss: 2.2974\n",
            "Epoch 160/400, Loss: 2.2800\n",
            "Epoch 161/400, Loss: 2.3236\n",
            "Epoch 162/400, Loss: 2.3344\n",
            "Epoch 163/400, Loss: 2.3028\n",
            "Epoch 164/400, Loss: 2.3034\n",
            "Epoch 165/400, Loss: 2.3338\n",
            "Epoch 166/400, Loss: 2.2867\n",
            "Epoch 167/400, Loss: 2.3063\n",
            "Epoch 168/400, Loss: 2.2478\n",
            "Epoch 169/400, Loss: 2.3023\n",
            "Epoch 170/400, Loss: 2.2806\n",
            "Epoch 171/400, Loss: 2.2767\n",
            "Epoch 172/400, Loss: 2.2615\n",
            "Epoch 173/400, Loss: 2.3164\n",
            "Epoch 174/400, Loss: 2.2513\n",
            "Epoch 175/400, Loss: 2.2187\n",
            "Epoch 176/400, Loss: 2.2600\n",
            "Epoch 177/400, Loss: 2.2388\n",
            "Epoch 178/400, Loss: 2.3018\n",
            "Epoch 179/400, Loss: 2.1913\n",
            "Epoch 180/400, Loss: 2.2210\n",
            "Epoch 181/400, Loss: 2.1882\n",
            "Epoch 182/400, Loss: 2.1795\n",
            "Epoch 183/400, Loss: 2.1728\n",
            "Epoch 184/400, Loss: 2.2242\n",
            "Epoch 185/400, Loss: 2.2065\n",
            "Epoch 186/400, Loss: 2.1954\n",
            "Epoch 187/400, Loss: 2.1701\n",
            "Epoch 188/400, Loss: 2.1312\n",
            "Epoch 189/400, Loss: 2.2042\n",
            "Epoch 190/400, Loss: 2.1624\n",
            "Epoch 191/400, Loss: 2.1730\n",
            "Epoch 192/400, Loss: 2.1621\n",
            "Epoch 193/400, Loss: 2.1647\n",
            "Epoch 194/400, Loss: 2.1703\n",
            "Epoch 195/400, Loss: 2.1919\n",
            "Epoch 196/400, Loss: 2.1320\n",
            "Epoch 197/400, Loss: 2.1125\n",
            "Epoch 198/400, Loss: 2.1652\n",
            "Epoch 199/400, Loss: 2.1247\n",
            "Epoch 200/400, Loss: 2.1144\n",
            "Epoch 201/400, Loss: 2.1201\n",
            "Epoch 202/400, Loss: 2.0832\n",
            "Epoch 203/400, Loss: 2.0821\n",
            "Epoch 204/400, Loss: 2.0750\n",
            "Epoch 205/400, Loss: 2.0623\n",
            "Epoch 206/400, Loss: 2.0548\n",
            "Epoch 207/400, Loss: 2.0539\n",
            "Epoch 208/400, Loss: 2.0350\n",
            "Epoch 209/400, Loss: 2.0792\n",
            "Epoch 210/400, Loss: 2.0259\n",
            "Epoch 211/400, Loss: 2.0749\n",
            "Epoch 212/400, Loss: 2.0002\n",
            "Epoch 213/400, Loss: 2.0272\n",
            "Epoch 214/400, Loss: 2.0464\n",
            "Epoch 215/400, Loss: 2.1044\n",
            "Epoch 216/400, Loss: 2.0019\n",
            "Epoch 217/400, Loss: 1.9925\n",
            "Epoch 218/400, Loss: 2.0184\n",
            "Epoch 219/400, Loss: 1.9467\n",
            "Epoch 220/400, Loss: 1.9560\n",
            "Epoch 221/400, Loss: 1.9662\n",
            "Epoch 222/400, Loss: 1.9733\n",
            "Epoch 223/400, Loss: 1.9969\n",
            "Epoch 224/400, Loss: 1.9642\n",
            "Epoch 225/400, Loss: 1.9737\n",
            "Epoch 226/400, Loss: 1.9613\n",
            "Epoch 227/400, Loss: 1.9714\n",
            "Epoch 228/400, Loss: 1.9481\n",
            "Epoch 229/400, Loss: 1.9263\n",
            "Epoch 230/400, Loss: 1.9541\n",
            "Epoch 231/400, Loss: 1.9412\n",
            "Epoch 232/400, Loss: 1.9484\n",
            "Epoch 233/400, Loss: 1.9571\n",
            "Epoch 234/400, Loss: 1.8992\n",
            "Epoch 235/400, Loss: 1.9075\n",
            "Epoch 236/400, Loss: 1.9006\n",
            "Epoch 237/400, Loss: 1.9000\n",
            "Epoch 238/400, Loss: 1.8289\n",
            "Epoch 239/400, Loss: 1.9115\n",
            "Epoch 240/400, Loss: 1.9037\n",
            "Epoch 241/400, Loss: 1.8556\n",
            "Epoch 242/400, Loss: 1.8846\n",
            "Epoch 243/400, Loss: 1.9012\n",
            "Epoch 244/400, Loss: 1.8740\n",
            "Epoch 245/400, Loss: 1.9051\n",
            "Epoch 246/400, Loss: 1.8944\n",
            "Epoch 247/400, Loss: 1.9147\n",
            "Epoch 248/400, Loss: 1.8842\n",
            "Epoch 249/400, Loss: 1.8549\n",
            "Epoch 250/400, Loss: 1.8539\n",
            "Epoch 251/400, Loss: 1.9180\n",
            "Epoch 252/400, Loss: 1.8429\n",
            "Epoch 253/400, Loss: 1.8268\n",
            "Epoch 254/400, Loss: 1.8406\n",
            "Epoch 255/400, Loss: 1.8608\n",
            "Epoch 256/400, Loss: 1.8373\n",
            "Epoch 257/400, Loss: 1.8443\n",
            "Epoch 258/400, Loss: 1.8465\n",
            "Epoch 259/400, Loss: 1.8276\n",
            "Epoch 260/400, Loss: 1.8706\n",
            "Epoch 261/400, Loss: 1.8655\n",
            "Epoch 262/400, Loss: 1.8054\n",
            "Epoch 263/400, Loss: 1.8181\n",
            "Epoch 264/400, Loss: 1.8678\n",
            "Epoch 265/400, Loss: 1.8007\n",
            "Epoch 266/400, Loss: 1.8256\n",
            "Epoch 267/400, Loss: 1.8129\n",
            "Epoch 268/400, Loss: 1.7925\n",
            "Epoch 269/400, Loss: 1.7756\n",
            "Epoch 270/400, Loss: 1.8189\n",
            "Epoch 271/400, Loss: 1.7837\n",
            "Epoch 272/400, Loss: 1.8316\n",
            "Epoch 273/400, Loss: 1.8255\n",
            "Epoch 274/400, Loss: 1.7824\n",
            "Epoch 275/400, Loss: 1.7807\n",
            "Epoch 276/400, Loss: 1.7869\n",
            "Epoch 277/400, Loss: 1.8032\n",
            "Epoch 278/400, Loss: 1.7763\n",
            "Epoch 279/400, Loss: 1.8003\n",
            "Epoch 280/400, Loss: 1.7763\n",
            "Epoch 281/400, Loss: 1.7989\n",
            "Epoch 282/400, Loss: 1.8193\n",
            "Epoch 283/400, Loss: 1.7862\n",
            "Epoch 284/400, Loss: 1.7583\n",
            "Epoch 285/400, Loss: 1.7938\n",
            "Epoch 286/400, Loss: 1.7803\n",
            "Epoch 287/400, Loss: 1.7325\n",
            "Epoch 288/400, Loss: 1.7561\n",
            "Epoch 289/400, Loss: 1.7374\n",
            "Epoch 290/400, Loss: 1.7605\n",
            "Epoch 291/400, Loss: 1.7764\n",
            "Epoch 292/400, Loss: 1.7601\n",
            "Epoch 293/400, Loss: 1.7364\n",
            "Epoch 294/400, Loss: 1.7504\n",
            "Epoch 295/400, Loss: 1.7927\n",
            "Epoch 296/400, Loss: 1.7700\n",
            "Epoch 297/400, Loss: 1.7545\n",
            "Epoch 298/400, Loss: 1.7654\n",
            "Epoch 299/400, Loss: 1.7468\n",
            "Epoch 300/400, Loss: 1.7491\n",
            "Epoch 301/400, Loss: 1.7346\n",
            "Epoch 302/400, Loss: 1.7407\n",
            "Epoch 303/400, Loss: 1.7618\n",
            "Epoch 304/400, Loss: 1.7354\n",
            "Epoch 305/400, Loss: 1.7369\n",
            "Epoch 306/400, Loss: 1.7365\n",
            "Epoch 307/400, Loss: 1.7449\n",
            "Epoch 308/400, Loss: 1.7337\n",
            "Epoch 309/400, Loss: 1.7187\n",
            "Epoch 310/400, Loss: 1.7181\n",
            "Epoch 311/400, Loss: 1.7637\n",
            "Epoch 312/400, Loss: 1.7077\n",
            "Epoch 313/400, Loss: 1.7289\n",
            "Epoch 314/400, Loss: 1.7181\n",
            "Epoch 315/400, Loss: 1.6965\n",
            "Epoch 316/400, Loss: 1.7191\n",
            "Epoch 317/400, Loss: 1.7213\n",
            "Epoch 318/400, Loss: 1.7113\n",
            "Epoch 319/400, Loss: 1.7165\n",
            "Epoch 320/400, Loss: 1.7487\n",
            "Epoch 321/400, Loss: 1.7090\n",
            "Epoch 322/400, Loss: 1.6967\n",
            "Epoch 323/400, Loss: 1.7074\n",
            "Epoch 324/400, Loss: 1.7300\n",
            "Epoch 325/400, Loss: 1.6897\n",
            "Epoch 326/400, Loss: 1.7152\n",
            "Epoch 327/400, Loss: 1.7027\n",
            "Epoch 328/400, Loss: 1.7102\n",
            "Epoch 329/400, Loss: 1.7381\n",
            "Epoch 330/400, Loss: 1.6783\n",
            "Epoch 331/400, Loss: 1.6915\n",
            "Epoch 332/400, Loss: 1.6963\n",
            "Epoch 333/400, Loss: 1.6760\n",
            "Epoch 334/400, Loss: 1.7443\n",
            "Epoch 335/400, Loss: 1.6743\n",
            "Epoch 336/400, Loss: 1.6790\n",
            "Epoch 337/400, Loss: 1.7118\n",
            "Epoch 338/400, Loss: 1.7087\n",
            "Epoch 339/400, Loss: 1.7138\n",
            "Epoch 340/400, Loss: 1.6960\n",
            "Epoch 341/400, Loss: 1.7134\n",
            "Epoch 342/400, Loss: 1.6936\n",
            "Epoch 343/400, Loss: 1.6726\n",
            "Epoch 344/400, Loss: 1.6766\n",
            "Epoch 345/400, Loss: 1.6730\n",
            "Epoch 346/400, Loss: 1.6749\n",
            "Epoch 347/400, Loss: 1.6658\n",
            "Epoch 348/400, Loss: 1.6966\n",
            "Epoch 349/400, Loss: 1.6762\n",
            "Epoch 350/400, Loss: 1.6765\n",
            "Epoch 351/400, Loss: 1.6911\n",
            "Epoch 352/400, Loss: 1.6613\n",
            "Epoch 353/400, Loss: 1.6454\n",
            "Epoch 354/400, Loss: 1.6472\n",
            "Epoch 355/400, Loss: 1.6664\n",
            "Epoch 356/400, Loss: 1.6615\n",
            "Epoch 357/400, Loss: 1.6853\n",
            "Epoch 358/400, Loss: 1.6681\n",
            "Epoch 359/400, Loss: 1.6734\n",
            "Epoch 360/400, Loss: 1.6568\n",
            "Epoch 361/400, Loss: 1.6724\n",
            "Epoch 362/400, Loss: 1.6434\n",
            "Epoch 363/400, Loss: 1.6253\n",
            "Epoch 364/400, Loss: 1.6455\n",
            "Epoch 365/400, Loss: 1.7009\n",
            "Epoch 366/400, Loss: 1.6693\n",
            "Epoch 367/400, Loss: 1.6855\n",
            "Epoch 368/400, Loss: 1.6864\n",
            "Epoch 369/400, Loss: 1.6819\n",
            "Epoch 370/400, Loss: 1.6536\n",
            "Epoch 371/400, Loss: 1.6802\n",
            "Epoch 372/400, Loss: 1.6857\n",
            "Epoch 373/400, Loss: 1.6154\n",
            "Epoch 374/400, Loss: 1.6564\n",
            "Epoch 375/400, Loss: 1.6532\n",
            "Epoch 376/400, Loss: 1.6645\n",
            "Epoch 377/400, Loss: 1.6454\n",
            "Epoch 378/400, Loss: 1.6482\n",
            "Epoch 379/400, Loss: 1.6850\n",
            "Epoch 380/400, Loss: 1.6553\n",
            "Epoch 381/400, Loss: 1.6200\n",
            "Epoch 382/400, Loss: 1.6561\n",
            "Epoch 383/400, Loss: 1.6692\n",
            "Epoch 384/400, Loss: 1.6515\n",
            "Epoch 385/400, Loss: 1.6419\n",
            "Epoch 386/400, Loss: 1.6357\n",
            "Epoch 387/400, Loss: 1.6325\n",
            "Epoch 388/400, Loss: 1.6502\n",
            "Epoch 389/400, Loss: 1.6380\n",
            "Epoch 390/400, Loss: 1.6322\n",
            "Epoch 391/400, Loss: 1.6380\n",
            "Epoch 392/400, Loss: 1.6520\n",
            "Epoch 393/400, Loss: 1.6335\n",
            "Epoch 394/400, Loss: 1.6044\n",
            "Epoch 395/400, Loss: 1.6885\n",
            "Epoch 396/400, Loss: 1.5899\n",
            "Epoch 397/400, Loss: 1.6447\n",
            "Epoch 398/400, Loss: 1.6760\n",
            "Epoch 399/400, Loss: 1.6895\n",
            "Epoch 400/400, Loss: 1.6662\n",
            "{'0': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 7.0}, '1': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 7.0}, 'accuracy': 0.8571428571428571, 'macro avg': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 14.0}, 'weighted avg': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 6, attention_layers = 6, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 400, learning_rate = 1e-5)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JyoFegSIZx6M",
        "outputId": "03288d7e-aa2c-4418-83f6-e60eb3cad422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400, Loss: 2.8041\n",
            "Epoch 2/400, Loss: 2.7769\n",
            "Epoch 3/400, Loss: 2.7765\n",
            "Epoch 4/400, Loss: 2.7907\n",
            "Epoch 5/400, Loss: 2.7744\n",
            "Epoch 6/400, Loss: 2.7715\n",
            "Epoch 7/400, Loss: 2.7689\n",
            "Epoch 8/400, Loss: 2.7646\n",
            "Epoch 9/400, Loss: 2.7677\n",
            "Epoch 10/400, Loss: 2.7644\n",
            "Epoch 11/400, Loss: 2.7628\n",
            "Epoch 12/400, Loss: 2.7661\n",
            "Epoch 13/400, Loss: 2.7615\n",
            "Epoch 14/400, Loss: 2.7559\n",
            "Epoch 15/400, Loss: 2.7576\n",
            "Epoch 16/400, Loss: 2.7503\n",
            "Epoch 17/400, Loss: 2.7472\n",
            "Epoch 18/400, Loss: 2.7539\n",
            "Epoch 19/400, Loss: 2.7522\n",
            "Epoch 20/400, Loss: 2.7392\n",
            "Epoch 21/400, Loss: 2.7407\n",
            "Epoch 22/400, Loss: 2.7374\n",
            "Epoch 23/400, Loss: 2.7335\n",
            "Epoch 24/400, Loss: 2.7393\n",
            "Epoch 25/400, Loss: 2.7375\n",
            "Epoch 26/400, Loss: 2.7303\n",
            "Epoch 27/400, Loss: 2.7309\n",
            "Epoch 28/400, Loss: 2.7266\n",
            "Epoch 29/400, Loss: 2.7349\n",
            "Epoch 30/400, Loss: 2.7247\n",
            "Epoch 31/400, Loss: 2.7116\n",
            "Epoch 32/400, Loss: 2.7157\n",
            "Epoch 33/400, Loss: 2.7118\n",
            "Epoch 34/400, Loss: 2.7104\n",
            "Epoch 35/400, Loss: 2.7118\n",
            "Epoch 36/400, Loss: 2.7365\n",
            "Epoch 37/400, Loss: 2.6971\n",
            "Epoch 38/400, Loss: 2.6836\n",
            "Epoch 39/400, Loss: 2.7117\n",
            "Epoch 40/400, Loss: 2.7096\n",
            "Epoch 41/400, Loss: 2.6913\n",
            "Epoch 42/400, Loss: 2.7398\n",
            "Epoch 43/400, Loss: 2.6746\n",
            "Epoch 44/400, Loss: 2.6955\n",
            "Epoch 45/400, Loss: 2.6756\n",
            "Epoch 46/400, Loss: 2.6765\n",
            "Epoch 47/400, Loss: 2.6622\n",
            "Epoch 48/400, Loss: 2.6569\n",
            "Epoch 49/400, Loss: 2.6546\n",
            "Epoch 50/400, Loss: 2.6493\n",
            "Epoch 51/400, Loss: 2.6506\n",
            "Epoch 52/400, Loss: 2.6342\n",
            "Epoch 53/400, Loss: 2.6371\n",
            "Epoch 54/400, Loss: 2.6261\n",
            "Epoch 55/400, Loss: 2.6293\n",
            "Epoch 56/400, Loss: 2.6450\n",
            "Epoch 57/400, Loss: 2.6175\n",
            "Epoch 58/400, Loss: 2.5882\n",
            "Epoch 59/400, Loss: 2.5912\n",
            "Epoch 60/400, Loss: 2.5943\n",
            "Epoch 61/400, Loss: 2.5805\n",
            "Epoch 62/400, Loss: 2.5978\n",
            "Epoch 63/400, Loss: 2.6080\n",
            "Epoch 64/400, Loss: 2.5895\n",
            "Epoch 65/400, Loss: 2.5821\n",
            "Epoch 66/400, Loss: 2.5465\n",
            "Epoch 67/400, Loss: 2.5444\n",
            "Epoch 68/400, Loss: 2.5537\n",
            "Epoch 69/400, Loss: 2.5232\n",
            "Epoch 70/400, Loss: 2.5624\n",
            "Epoch 71/400, Loss: 2.6098\n",
            "Epoch 72/400, Loss: 2.5203\n",
            "Epoch 73/400, Loss: 2.5514\n",
            "Epoch 74/400, Loss: 2.4963\n",
            "Epoch 75/400, Loss: 2.5279\n",
            "Epoch 76/400, Loss: 2.5253\n",
            "Epoch 77/400, Loss: 2.5101\n",
            "Epoch 78/400, Loss: 2.5182\n",
            "Epoch 79/400, Loss: 2.4860\n",
            "Epoch 80/400, Loss: 2.5094\n",
            "Epoch 81/400, Loss: 2.4749\n",
            "Epoch 82/400, Loss: 2.4728\n",
            "Epoch 83/400, Loss: 2.5106\n",
            "Epoch 84/400, Loss: 2.5296\n",
            "Epoch 85/400, Loss: 2.5039\n",
            "Epoch 86/400, Loss: 2.4697\n",
            "Epoch 87/400, Loss: 2.5067\n",
            "Epoch 88/400, Loss: 2.4764\n",
            "Epoch 89/400, Loss: 2.5051\n",
            "Epoch 90/400, Loss: 2.4837\n",
            "Epoch 91/400, Loss: 2.4812\n",
            "Epoch 92/400, Loss: 2.4695\n",
            "Epoch 93/400, Loss: 2.4746\n",
            "Epoch 94/400, Loss: 2.4476\n",
            "Epoch 95/400, Loss: 2.4747\n",
            "Epoch 96/400, Loss: 2.4513\n",
            "Epoch 97/400, Loss: 2.4814\n",
            "Epoch 98/400, Loss: 2.4631\n",
            "Epoch 99/400, Loss: 2.4768\n",
            "Epoch 100/400, Loss: 2.4696\n",
            "Epoch 101/400, Loss: 2.4948\n",
            "Epoch 102/400, Loss: 2.4998\n",
            "Epoch 103/400, Loss: 2.4267\n",
            "Epoch 104/400, Loss: 2.4890\n",
            "Epoch 105/400, Loss: 2.4734\n",
            "Epoch 106/400, Loss: 2.4581\n",
            "Epoch 107/400, Loss: 2.4684\n",
            "Epoch 108/400, Loss: 2.4317\n",
            "Epoch 109/400, Loss: 2.4302\n",
            "Epoch 110/400, Loss: 2.4197\n",
            "Epoch 111/400, Loss: 2.4384\n",
            "Epoch 112/400, Loss: 2.4138\n",
            "Epoch 113/400, Loss: 2.4062\n",
            "Epoch 114/400, Loss: 2.4251\n",
            "Epoch 115/400, Loss: 2.4466\n",
            "Epoch 116/400, Loss: 2.4059\n",
            "Epoch 117/400, Loss: 2.4295\n",
            "Epoch 118/400, Loss: 2.4143\n",
            "Epoch 119/400, Loss: 2.4167\n",
            "Epoch 120/400, Loss: 2.4250\n",
            "Epoch 121/400, Loss: 2.4383\n",
            "Epoch 122/400, Loss: 2.4612\n",
            "Epoch 123/400, Loss: 2.3975\n",
            "Epoch 124/400, Loss: 2.4004\n",
            "Epoch 125/400, Loss: 2.4176\n",
            "Epoch 126/400, Loss: 2.3955\n",
            "Epoch 127/400, Loss: 2.3620\n",
            "Epoch 128/400, Loss: 2.3835\n",
            "Epoch 129/400, Loss: 2.3692\n",
            "Epoch 130/400, Loss: 2.3233\n",
            "Epoch 131/400, Loss: 2.3316\n",
            "Epoch 132/400, Loss: 2.3824\n",
            "Epoch 133/400, Loss: 2.3271\n",
            "Epoch 134/400, Loss: 2.3876\n",
            "Epoch 135/400, Loss: 2.4151\n",
            "Epoch 136/400, Loss: 2.3940\n",
            "Epoch 137/400, Loss: 2.3998\n",
            "Epoch 138/400, Loss: 2.3911\n",
            "Epoch 139/400, Loss: 2.4097\n",
            "Epoch 140/400, Loss: 2.3482\n",
            "Epoch 141/400, Loss: 2.3289\n",
            "Epoch 142/400, Loss: 2.2982\n",
            "Epoch 143/400, Loss: 2.3275\n",
            "Epoch 144/400, Loss: 2.3371\n",
            "Epoch 145/400, Loss: 2.2968\n",
            "Epoch 146/400, Loss: 2.3210\n",
            "Epoch 147/400, Loss: 2.3220\n",
            "Epoch 148/400, Loss: 2.2843\n",
            "Epoch 149/400, Loss: 2.2665\n",
            "Epoch 150/400, Loss: 2.2850\n",
            "Epoch 151/400, Loss: 2.3017\n",
            "Epoch 152/400, Loss: 2.2862\n",
            "Epoch 153/400, Loss: 2.3263\n",
            "Epoch 154/400, Loss: 2.2867\n",
            "Epoch 155/400, Loss: 2.3014\n",
            "Epoch 156/400, Loss: 2.2538\n",
            "Epoch 157/400, Loss: 2.2407\n",
            "Epoch 158/400, Loss: 2.2008\n",
            "Epoch 159/400, Loss: 2.1922\n",
            "Epoch 160/400, Loss: 2.1625\n",
            "Epoch 161/400, Loss: 2.2019\n",
            "Epoch 162/400, Loss: 2.2361\n",
            "Epoch 163/400, Loss: 2.1977\n",
            "Epoch 164/400, Loss: 2.2021\n",
            "Epoch 165/400, Loss: 2.2058\n",
            "Epoch 166/400, Loss: 2.1604\n",
            "Epoch 167/400, Loss: 2.1824\n",
            "Epoch 168/400, Loss: 2.1080\n",
            "Epoch 169/400, Loss: 2.1671\n",
            "Epoch 170/400, Loss: 2.1573\n",
            "Epoch 171/400, Loss: 2.1306\n",
            "Epoch 172/400, Loss: 2.1241\n",
            "Epoch 173/400, Loss: 2.2175\n",
            "Epoch 174/400, Loss: 2.1076\n",
            "Epoch 175/400, Loss: 2.0809\n",
            "Epoch 176/400, Loss: 2.0992\n",
            "Epoch 177/400, Loss: 2.0838\n",
            "Epoch 178/400, Loss: 2.1626\n",
            "Epoch 179/400, Loss: 2.0367\n",
            "Epoch 180/400, Loss: 2.0625\n",
            "Epoch 181/400, Loss: 2.0174\n",
            "Epoch 182/400, Loss: 2.0289\n",
            "Epoch 183/400, Loss: 1.9979\n",
            "Epoch 184/400, Loss: 2.0664\n",
            "Epoch 185/400, Loss: 2.0362\n",
            "Epoch 186/400, Loss: 2.0229\n",
            "Epoch 187/400, Loss: 2.0008\n",
            "Epoch 188/400, Loss: 1.9769\n",
            "Epoch 189/400, Loss: 2.0395\n",
            "Epoch 190/400, Loss: 1.9957\n",
            "Epoch 191/400, Loss: 2.0245\n",
            "Epoch 192/400, Loss: 1.9947\n",
            "Epoch 193/400, Loss: 2.0135\n",
            "Epoch 194/400, Loss: 2.0484\n",
            "Epoch 195/400, Loss: 2.0401\n",
            "Epoch 196/400, Loss: 1.9693\n",
            "Epoch 197/400, Loss: 1.9514\n",
            "Epoch 198/400, Loss: 2.0107\n",
            "Epoch 199/400, Loss: 1.9900\n",
            "Epoch 200/400, Loss: 1.9477\n",
            "Epoch 201/400, Loss: 1.9501\n",
            "Epoch 202/400, Loss: 1.9395\n",
            "Epoch 203/400, Loss: 1.9215\n",
            "Epoch 204/400, Loss: 1.9308\n",
            "Epoch 205/400, Loss: 1.9179\n",
            "Epoch 206/400, Loss: 1.9137\n",
            "Epoch 207/400, Loss: 1.9161\n",
            "Epoch 208/400, Loss: 1.9046\n",
            "Epoch 209/400, Loss: 1.9490\n",
            "Epoch 210/400, Loss: 1.9014\n",
            "Epoch 211/400, Loss: 1.9672\n",
            "Epoch 212/400, Loss: 1.8732\n",
            "Epoch 213/400, Loss: 1.9207\n",
            "Epoch 214/400, Loss: 1.9319\n",
            "Epoch 215/400, Loss: 1.9850\n",
            "Epoch 216/400, Loss: 1.8754\n",
            "Epoch 217/400, Loss: 1.8712\n",
            "Epoch 218/400, Loss: 1.9046\n",
            "Epoch 219/400, Loss: 1.8193\n",
            "Epoch 220/400, Loss: 1.8455\n",
            "Epoch 221/400, Loss: 1.8986\n",
            "Epoch 222/400, Loss: 1.8688\n",
            "Epoch 223/400, Loss: 1.9111\n",
            "Epoch 224/400, Loss: 1.8654\n",
            "Epoch 225/400, Loss: 1.8636\n",
            "Epoch 226/400, Loss: 1.8686\n",
            "Epoch 227/400, Loss: 1.8815\n",
            "Epoch 228/400, Loss: 1.8416\n",
            "Epoch 229/400, Loss: 1.8413\n",
            "Epoch 230/400, Loss: 1.8407\n",
            "Epoch 231/400, Loss: 1.8706\n",
            "Epoch 232/400, Loss: 1.8576\n",
            "Epoch 233/400, Loss: 1.8727\n",
            "Epoch 234/400, Loss: 1.8474\n",
            "Epoch 235/400, Loss: 1.8300\n",
            "Epoch 236/400, Loss: 1.8574\n",
            "Epoch 237/400, Loss: 1.8087\n",
            "Epoch 238/400, Loss: 1.7562\n",
            "Epoch 239/400, Loss: 1.8280\n",
            "Epoch 240/400, Loss: 1.8549\n",
            "Epoch 241/400, Loss: 1.8031\n",
            "Epoch 242/400, Loss: 1.8070\n",
            "Epoch 243/400, Loss: 1.8209\n",
            "Epoch 244/400, Loss: 1.8136\n",
            "Epoch 245/400, Loss: 1.8380\n",
            "Epoch 246/400, Loss: 1.8115\n",
            "Epoch 247/400, Loss: 1.8487\n",
            "Epoch 248/400, Loss: 1.7955\n",
            "Epoch 249/400, Loss: 1.8017\n",
            "Epoch 250/400, Loss: 1.7885\n",
            "Epoch 251/400, Loss: 1.8527\n",
            "Epoch 252/400, Loss: 1.7756\n",
            "Epoch 253/400, Loss: 1.7563\n",
            "Epoch 254/400, Loss: 1.7972\n",
            "Epoch 255/400, Loss: 1.8220\n",
            "Epoch 256/400, Loss: 1.7913\n",
            "Epoch 257/400, Loss: 1.8081\n",
            "Epoch 258/400, Loss: 1.7919\n",
            "Epoch 259/400, Loss: 1.7669\n",
            "Epoch 260/400, Loss: 1.8150\n",
            "Epoch 261/400, Loss: 1.8577\n",
            "Epoch 262/400, Loss: 1.7641\n",
            "Epoch 263/400, Loss: 1.7641\n",
            "Epoch 264/400, Loss: 1.8326\n",
            "Epoch 265/400, Loss: 1.7706\n",
            "Epoch 266/400, Loss: 1.7788\n",
            "Epoch 267/400, Loss: 1.7743\n",
            "Epoch 268/400, Loss: 1.7654\n",
            "Epoch 269/400, Loss: 1.7601\n",
            "Epoch 270/400, Loss: 1.7766\n",
            "Epoch 271/400, Loss: 1.7397\n",
            "Epoch 272/400, Loss: 1.7818\n",
            "Epoch 273/400, Loss: 1.7684\n",
            "Epoch 274/400, Loss: 1.7385\n",
            "Epoch 275/400, Loss: 1.7633\n",
            "Epoch 276/400, Loss: 1.7648\n",
            "Epoch 277/400, Loss: 1.7742\n",
            "Epoch 278/400, Loss: 1.7703\n",
            "Epoch 279/400, Loss: 1.7600\n",
            "Epoch 280/400, Loss: 1.7220\n",
            "Epoch 281/400, Loss: 1.7613\n",
            "Epoch 282/400, Loss: 1.7853\n",
            "Epoch 283/400, Loss: 1.7475\n",
            "Epoch 284/400, Loss: 1.7506\n",
            "Epoch 285/400, Loss: 1.7603\n",
            "Epoch 286/400, Loss: 1.7272\n",
            "Epoch 287/400, Loss: 1.7133\n",
            "Epoch 288/400, Loss: 1.7448\n",
            "Epoch 289/400, Loss: 1.7069\n",
            "Epoch 290/400, Loss: 1.7651\n",
            "Epoch 291/400, Loss: 1.7821\n",
            "Epoch 292/400, Loss: 1.7621\n",
            "Epoch 293/400, Loss: 1.7054\n",
            "Epoch 294/400, Loss: 1.7315\n",
            "Epoch 295/400, Loss: 1.7778\n",
            "Epoch 296/400, Loss: 1.7579\n",
            "Epoch 297/400, Loss: 1.7694\n",
            "Epoch 298/400, Loss: 1.6977\n",
            "Epoch 299/400, Loss: 1.7135\n",
            "Epoch 300/400, Loss: 1.6930\n",
            "Epoch 301/400, Loss: 1.7021\n",
            "Epoch 302/400, Loss: 1.7240\n",
            "Epoch 303/400, Loss: 1.7228\n",
            "Epoch 304/400, Loss: 1.7508\n",
            "Epoch 305/400, Loss: 1.7305\n",
            "Epoch 306/400, Loss: 1.7476\n",
            "Epoch 307/400, Loss: 1.7243\n",
            "Epoch 308/400, Loss: 1.7069\n",
            "Epoch 309/400, Loss: 1.7150\n",
            "Epoch 310/400, Loss: 1.7092\n",
            "Epoch 311/400, Loss: 1.7800\n",
            "Epoch 312/400, Loss: 1.7000\n",
            "Epoch 313/400, Loss: 1.7285\n",
            "Epoch 314/400, Loss: 1.7446\n",
            "Epoch 315/400, Loss: 1.6922\n",
            "Epoch 316/400, Loss: 1.7128\n",
            "Epoch 317/400, Loss: 1.7475\n",
            "Epoch 318/400, Loss: 1.7035\n",
            "Epoch 319/400, Loss: 1.7070\n",
            "Epoch 320/400, Loss: 1.7713\n",
            "Epoch 321/400, Loss: 1.7283\n",
            "Epoch 322/400, Loss: 1.6747\n",
            "Epoch 323/400, Loss: 1.7001\n",
            "Epoch 324/400, Loss: 1.6876\n",
            "Epoch 325/400, Loss: 1.7199\n",
            "Epoch 326/400, Loss: 1.6890\n",
            "Epoch 327/400, Loss: 1.6951\n",
            "Epoch 328/400, Loss: 1.7144\n",
            "Epoch 329/400, Loss: 1.7293\n",
            "Epoch 330/400, Loss: 1.7121\n",
            "Epoch 331/400, Loss: 1.6853\n",
            "Epoch 332/400, Loss: 1.6964\n",
            "Epoch 333/400, Loss: 1.6786\n",
            "Epoch 334/400, Loss: 1.7289\n",
            "Epoch 335/400, Loss: 1.7068\n",
            "Epoch 336/400, Loss: 1.6719\n",
            "Epoch 337/400, Loss: 1.7482\n",
            "Epoch 338/400, Loss: 1.7289\n",
            "Epoch 339/400, Loss: 1.6856\n",
            "Epoch 340/400, Loss: 1.7039\n",
            "Epoch 341/400, Loss: 1.7398\n",
            "Epoch 342/400, Loss: 1.6813\n",
            "Epoch 343/400, Loss: 1.6700\n",
            "Epoch 344/400, Loss: 1.6816\n",
            "Epoch 345/400, Loss: 1.7076\n",
            "Epoch 346/400, Loss: 1.6903\n",
            "Epoch 347/400, Loss: 1.6830\n",
            "Epoch 348/400, Loss: 1.6953\n",
            "Epoch 349/400, Loss: 1.6660\n",
            "Epoch 350/400, Loss: 1.6844\n",
            "Epoch 351/400, Loss: 1.6785\n",
            "Epoch 352/400, Loss: 1.6479\n",
            "Epoch 353/400, Loss: 1.6598\n",
            "Epoch 354/400, Loss: 1.6633\n",
            "Epoch 355/400, Loss: 1.6866\n",
            "Epoch 356/400, Loss: 1.6773\n",
            "Epoch 357/400, Loss: 1.6534\n",
            "Epoch 358/400, Loss: 1.6774\n",
            "Epoch 359/400, Loss: 1.6775\n",
            "Epoch 360/400, Loss: 1.6653\n",
            "Epoch 361/400, Loss: 1.6420\n",
            "Epoch 362/400, Loss: 1.6812\n",
            "Epoch 363/400, Loss: 1.6339\n",
            "Epoch 364/400, Loss: 1.6541\n",
            "Epoch 365/400, Loss: 1.6975\n",
            "Epoch 366/400, Loss: 1.6929\n",
            "Epoch 367/400, Loss: 1.6857\n",
            "Epoch 368/400, Loss: 1.6826\n",
            "Epoch 369/400, Loss: 1.7036\n",
            "Epoch 370/400, Loss: 1.6589\n",
            "Epoch 371/400, Loss: 1.6819\n",
            "Epoch 372/400, Loss: 1.6918\n",
            "Epoch 373/400, Loss: 1.6761\n",
            "Epoch 374/400, Loss: 1.6834\n",
            "Epoch 375/400, Loss: 1.6573\n",
            "Epoch 376/400, Loss: 1.6256\n",
            "Epoch 377/400, Loss: 1.6601\n",
            "Epoch 378/400, Loss: 1.6611\n",
            "Epoch 379/400, Loss: 1.6748\n",
            "Epoch 380/400, Loss: 1.6512\n",
            "Epoch 381/400, Loss: 1.6484\n",
            "Epoch 382/400, Loss: 1.6969\n",
            "Epoch 383/400, Loss: 1.6596\n",
            "Epoch 384/400, Loss: 1.6678\n",
            "Epoch 385/400, Loss: 1.6727\n",
            "Epoch 386/400, Loss: 1.6304\n",
            "Epoch 387/400, Loss: 1.6580\n",
            "Epoch 388/400, Loss: 1.6229\n",
            "Epoch 389/400, Loss: 1.6512\n",
            "Epoch 390/400, Loss: 1.6361\n",
            "Epoch 391/400, Loss: 1.6311\n",
            "Epoch 392/400, Loss: 1.6457\n",
            "Epoch 393/400, Loss: 1.6398\n",
            "Epoch 394/400, Loss: 1.6223\n",
            "Epoch 395/400, Loss: 1.6972\n",
            "Epoch 396/400, Loss: 1.5941\n",
            "Epoch 397/400, Loss: 1.6747\n",
            "Epoch 398/400, Loss: 1.6752\n",
            "Epoch 399/400, Loss: 1.7234\n",
            "Epoch 400/400, Loss: 1.6865\n",
            "{'0': {'precision': 0.8333333333333334, 'recall': 0.7142857142857143, 'f1-score': 0.7692307692307693, 'support': 7.0}, '1': {'precision': 0.75, 'recall': 0.8571428571428571, 'f1-score': 0.8, 'support': 7.0}, 'accuracy': 0.7857142857142857, 'macro avg': {'precision': 0.7916666666666667, 'recall': 0.7857142857142857, 'f1-score': 0.7846153846153847, 'support': 14.0}, 'weighted avg': {'precision': 0.7916666666666667, 'recall': 0.7857142857142857, 'f1-score': 0.7846153846153846, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 6, attention_layers = 8, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 400, learning_rate = 1e-5)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2xgRZy1DZx6M",
        "outputId": "fecbd655-3f5a-4f9e-a9bb-918c03c58bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400, Loss: 2.8400\n",
            "Epoch 2/400, Loss: 2.8050\n",
            "Epoch 3/400, Loss: 2.7824\n",
            "Epoch 4/400, Loss: 2.7744\n",
            "Epoch 5/400, Loss: 2.7826\n",
            "Epoch 6/400, Loss: 2.7796\n",
            "Epoch 7/400, Loss: 2.7759\n",
            "Epoch 8/400, Loss: 2.7666\n",
            "Epoch 9/400, Loss: 2.7775\n",
            "Epoch 10/400, Loss: 2.7713\n",
            "Epoch 11/400, Loss: 2.7650\n",
            "Epoch 12/400, Loss: 2.7751\n",
            "Epoch 13/400, Loss: 2.7678\n",
            "Epoch 14/400, Loss: 2.7622\n",
            "Epoch 15/400, Loss: 2.7648\n",
            "Epoch 16/400, Loss: 2.7602\n",
            "Epoch 17/400, Loss: 2.7577\n",
            "Epoch 18/400, Loss: 2.7651\n",
            "Epoch 19/400, Loss: 2.7623\n",
            "Epoch 20/400, Loss: 2.7533\n",
            "Epoch 21/400, Loss: 2.7548\n",
            "Epoch 22/400, Loss: 2.7514\n",
            "Epoch 23/400, Loss: 2.7549\n",
            "Epoch 24/400, Loss: 2.7540\n",
            "Epoch 25/400, Loss: 2.7571\n",
            "Epoch 26/400, Loss: 2.7486\n",
            "Epoch 27/400, Loss: 2.7507\n",
            "Epoch 28/400, Loss: 2.7456\n",
            "Epoch 29/400, Loss: 2.7569\n",
            "Epoch 30/400, Loss: 2.7492\n",
            "Epoch 31/400, Loss: 2.7410\n",
            "Epoch 32/400, Loss: 2.7405\n",
            "Epoch 33/400, Loss: 2.7379\n",
            "Epoch 34/400, Loss: 2.7336\n",
            "Epoch 35/400, Loss: 2.7340\n",
            "Epoch 36/400, Loss: 2.7561\n",
            "Epoch 37/400, Loss: 2.7297\n",
            "Epoch 38/400, Loss: 2.7199\n",
            "Epoch 39/400, Loss: 2.7375\n",
            "Epoch 40/400, Loss: 2.7428\n",
            "Epoch 41/400, Loss: 2.7256\n",
            "Epoch 42/400, Loss: 2.7821\n",
            "Epoch 43/400, Loss: 2.7096\n",
            "Epoch 44/400, Loss: 2.7325\n",
            "Epoch 45/400, Loss: 2.7221\n",
            "Epoch 46/400, Loss: 2.7108\n",
            "Epoch 47/400, Loss: 2.7059\n",
            "Epoch 48/400, Loss: 2.7064\n",
            "Epoch 49/400, Loss: 2.7013\n",
            "Epoch 50/400, Loss: 2.6936\n",
            "Epoch 51/400, Loss: 2.7038\n",
            "Epoch 52/400, Loss: 2.6924\n",
            "Epoch 53/400, Loss: 2.6887\n",
            "Epoch 54/400, Loss: 2.6817\n",
            "Epoch 55/400, Loss: 2.6873\n",
            "Epoch 56/400, Loss: 2.6961\n",
            "Epoch 57/400, Loss: 2.6756\n",
            "Epoch 58/400, Loss: 2.6554\n",
            "Epoch 59/400, Loss: 2.6536\n",
            "Epoch 60/400, Loss: 2.6629\n",
            "Epoch 61/400, Loss: 2.6520\n",
            "Epoch 62/400, Loss: 2.6563\n",
            "Epoch 63/400, Loss: 2.6684\n",
            "Epoch 64/400, Loss: 2.6341\n",
            "Epoch 65/400, Loss: 2.6374\n",
            "Epoch 66/400, Loss: 2.6075\n",
            "Epoch 67/400, Loss: 2.6022\n",
            "Epoch 68/400, Loss: 2.6059\n",
            "Epoch 69/400, Loss: 2.5857\n",
            "Epoch 70/400, Loss: 2.6157\n",
            "Epoch 71/400, Loss: 2.6400\n",
            "Epoch 72/400, Loss: 2.5761\n",
            "Epoch 73/400, Loss: 2.5805\n",
            "Epoch 74/400, Loss: 2.5348\n",
            "Epoch 75/400, Loss: 2.5584\n",
            "Epoch 76/400, Loss: 2.5465\n",
            "Epoch 77/400, Loss: 2.5344\n",
            "Epoch 78/400, Loss: 2.5296\n",
            "Epoch 79/400, Loss: 2.4938\n",
            "Epoch 80/400, Loss: 2.5184\n",
            "Epoch 81/400, Loss: 2.4811\n",
            "Epoch 82/400, Loss: 2.4815\n",
            "Epoch 83/400, Loss: 2.5108\n",
            "Epoch 84/400, Loss: 2.5241\n",
            "Epoch 85/400, Loss: 2.4976\n",
            "Epoch 86/400, Loss: 2.4629\n",
            "Epoch 87/400, Loss: 2.5037\n",
            "Epoch 88/400, Loss: 2.4656\n",
            "Epoch 89/400, Loss: 2.4922\n",
            "Epoch 90/400, Loss: 2.4656\n",
            "Epoch 91/400, Loss: 2.4746\n",
            "Epoch 92/400, Loss: 2.4448\n",
            "Epoch 93/400, Loss: 2.4497\n",
            "Epoch 94/400, Loss: 2.4242\n",
            "Epoch 95/400, Loss: 2.4572\n",
            "Epoch 96/400, Loss: 2.4248\n",
            "Epoch 97/400, Loss: 2.4576\n",
            "Epoch 98/400, Loss: 2.4388\n",
            "Epoch 99/400, Loss: 2.4433\n",
            "Epoch 100/400, Loss: 2.4307\n",
            "Epoch 101/400, Loss: 2.4674\n",
            "Epoch 102/400, Loss: 2.4669\n",
            "Epoch 103/400, Loss: 2.3864\n",
            "Epoch 104/400, Loss: 2.4549\n",
            "Epoch 105/400, Loss: 2.4331\n",
            "Epoch 106/400, Loss: 2.4160\n",
            "Epoch 107/400, Loss: 2.4239\n",
            "Epoch 108/400, Loss: 2.3770\n",
            "Epoch 109/400, Loss: 2.3739\n",
            "Epoch 110/400, Loss: 2.3678\n",
            "Epoch 111/400, Loss: 2.3897\n",
            "Epoch 112/400, Loss: 2.3557\n",
            "Epoch 113/400, Loss: 2.3457\n",
            "Epoch 114/400, Loss: 2.3592\n",
            "Epoch 115/400, Loss: 2.3845\n",
            "Epoch 116/400, Loss: 2.3451\n",
            "Epoch 117/400, Loss: 2.3726\n",
            "Epoch 118/400, Loss: 2.3469\n",
            "Epoch 119/400, Loss: 2.3522\n",
            "Epoch 120/400, Loss: 2.3486\n",
            "Epoch 121/400, Loss: 2.3571\n",
            "Epoch 122/400, Loss: 2.3765\n",
            "Epoch 123/400, Loss: 2.3054\n",
            "Epoch 124/400, Loss: 2.3227\n",
            "Epoch 125/400, Loss: 2.3143\n",
            "Epoch 126/400, Loss: 2.3095\n",
            "Epoch 127/400, Loss: 2.2688\n",
            "Epoch 128/400, Loss: 2.2816\n",
            "Epoch 129/400, Loss: 2.2590\n",
            "Epoch 130/400, Loss: 2.2034\n",
            "Epoch 131/400, Loss: 2.2236\n",
            "Epoch 132/400, Loss: 2.2606\n",
            "Epoch 133/400, Loss: 2.1953\n",
            "Epoch 134/400, Loss: 2.2516\n",
            "Epoch 135/400, Loss: 2.2774\n",
            "Epoch 136/400, Loss: 2.2514\n",
            "Epoch 137/400, Loss: 2.2413\n",
            "Epoch 138/400, Loss: 2.2400\n",
            "Epoch 139/400, Loss: 2.2352\n",
            "Epoch 140/400, Loss: 2.1718\n",
            "Epoch 141/400, Loss: 2.1585\n",
            "Epoch 142/400, Loss: 2.1237\n",
            "Epoch 143/400, Loss: 2.1494\n",
            "Epoch 144/400, Loss: 2.1419\n",
            "Epoch 145/400, Loss: 2.0926\n",
            "Epoch 146/400, Loss: 2.1082\n",
            "Epoch 147/400, Loss: 2.1037\n",
            "Epoch 148/400, Loss: 2.0787\n",
            "Epoch 149/400, Loss: 2.0408\n",
            "Epoch 150/400, Loss: 2.0638\n",
            "Epoch 151/400, Loss: 2.0673\n",
            "Epoch 152/400, Loss: 2.0277\n",
            "Epoch 153/400, Loss: 2.0675\n",
            "Epoch 154/400, Loss: 2.0113\n",
            "Epoch 155/400, Loss: 2.0414\n",
            "Epoch 156/400, Loss: 1.9721\n",
            "Epoch 157/400, Loss: 1.9662\n",
            "Epoch 158/400, Loss: 1.9334\n",
            "Epoch 159/400, Loss: 1.9219\n",
            "Epoch 160/400, Loss: 1.8832\n",
            "Epoch 161/400, Loss: 1.9357\n",
            "Epoch 162/400, Loss: 1.9268\n",
            "Epoch 163/400, Loss: 1.9279\n",
            "Epoch 164/400, Loss: 1.9349\n",
            "Epoch 165/400, Loss: 1.9044\n",
            "Epoch 166/400, Loss: 1.8856\n",
            "Epoch 167/400, Loss: 1.8869\n",
            "Epoch 168/400, Loss: 1.8291\n",
            "Epoch 169/400, Loss: 1.8849\n",
            "Epoch 170/400, Loss: 1.8869\n",
            "Epoch 171/400, Loss: 1.8609\n",
            "Epoch 172/400, Loss: 1.8288\n",
            "Epoch 173/400, Loss: 1.9090\n",
            "Epoch 174/400, Loss: 1.8016\n",
            "Epoch 175/400, Loss: 1.7994\n",
            "Epoch 176/400, Loss: 1.8028\n",
            "Epoch 177/400, Loss: 1.8024\n",
            "Epoch 178/400, Loss: 1.8428\n",
            "Epoch 179/400, Loss: 1.7749\n",
            "Epoch 180/400, Loss: 1.7856\n",
            "Epoch 181/400, Loss: 1.7756\n",
            "Epoch 182/400, Loss: 1.7435\n",
            "Epoch 183/400, Loss: 1.7420\n",
            "Epoch 184/400, Loss: 1.8153\n",
            "Epoch 185/400, Loss: 1.7841\n",
            "Epoch 186/400, Loss: 1.7652\n",
            "Epoch 187/400, Loss: 1.7645\n",
            "Epoch 188/400, Loss: 1.7296\n",
            "Epoch 189/400, Loss: 1.7769\n",
            "Epoch 190/400, Loss: 1.7828\n",
            "Epoch 191/400, Loss: 1.7838\n",
            "Epoch 192/400, Loss: 1.7409\n",
            "Epoch 193/400, Loss: 1.7756\n",
            "Epoch 194/400, Loss: 1.7874\n",
            "Epoch 195/400, Loss: 1.7927\n",
            "Epoch 196/400, Loss: 1.7381\n",
            "Epoch 197/400, Loss: 1.7315\n",
            "Epoch 198/400, Loss: 1.7828\n",
            "Epoch 199/400, Loss: 1.7674\n",
            "Epoch 200/400, Loss: 1.6897\n",
            "Epoch 201/400, Loss: 1.7343\n",
            "Epoch 202/400, Loss: 1.7310\n",
            "Epoch 203/400, Loss: 1.7002\n",
            "Epoch 204/400, Loss: 1.7282\n",
            "Epoch 205/400, Loss: 1.7322\n",
            "Epoch 206/400, Loss: 1.6934\n",
            "Epoch 207/400, Loss: 1.7067\n",
            "Epoch 208/400, Loss: 1.6755\n",
            "Epoch 209/400, Loss: 1.7398\n",
            "Epoch 210/400, Loss: 1.6970\n",
            "Epoch 211/400, Loss: 1.7345\n",
            "Epoch 212/400, Loss: 1.6483\n",
            "Epoch 213/400, Loss: 1.7442\n",
            "Epoch 214/400, Loss: 1.7359\n",
            "Epoch 215/400, Loss: 1.7831\n",
            "Epoch 216/400, Loss: 1.7031\n",
            "Epoch 217/400, Loss: 1.6865\n",
            "Epoch 218/400, Loss: 1.7594\n",
            "Epoch 219/400, Loss: 1.6443\n",
            "Epoch 220/400, Loss: 1.6714\n",
            "Epoch 221/400, Loss: 1.7394\n",
            "Epoch 222/400, Loss: 1.6791\n",
            "Epoch 223/400, Loss: 1.6713\n",
            "Epoch 224/400, Loss: 1.6789\n",
            "Epoch 225/400, Loss: 1.6570\n",
            "Epoch 226/400, Loss: 1.7040\n",
            "Epoch 227/400, Loss: 1.6928\n",
            "Epoch 228/400, Loss: 1.6732\n",
            "Epoch 229/400, Loss: 1.6549\n",
            "Epoch 230/400, Loss: 1.6573\n",
            "Epoch 231/400, Loss: 1.6776\n",
            "Epoch 232/400, Loss: 1.6804\n",
            "Epoch 233/400, Loss: 1.6634\n",
            "Epoch 234/400, Loss: 1.6392\n",
            "Epoch 235/400, Loss: 1.6484\n",
            "Epoch 236/400, Loss: 1.7301\n",
            "Epoch 237/400, Loss: 1.6203\n",
            "Epoch 238/400, Loss: 1.5820\n",
            "Epoch 239/400, Loss: 1.6109\n",
            "Epoch 240/400, Loss: 1.6536\n",
            "Epoch 241/400, Loss: 1.6375\n",
            "Epoch 242/400, Loss: 1.6321\n",
            "Epoch 243/400, Loss: 1.6544\n",
            "Epoch 244/400, Loss: 1.6294\n",
            "Epoch 245/400, Loss: 1.6572\n",
            "Epoch 246/400, Loss: 1.6660\n",
            "Epoch 247/400, Loss: 1.7055\n",
            "Epoch 248/400, Loss: 1.6207\n",
            "Epoch 249/400, Loss: 1.6313\n",
            "Epoch 250/400, Loss: 1.6293\n",
            "Epoch 251/400, Loss: 1.7196\n",
            "Epoch 252/400, Loss: 1.6057\n",
            "Epoch 253/400, Loss: 1.6099\n",
            "Epoch 254/400, Loss: 1.6480\n",
            "Epoch 255/400, Loss: 1.6448\n",
            "Epoch 256/400, Loss: 1.6009\n",
            "Epoch 257/400, Loss: 1.6648\n",
            "Epoch 258/400, Loss: 1.5866\n",
            "Epoch 259/400, Loss: 1.6170\n",
            "Epoch 260/400, Loss: 1.6606\n",
            "Epoch 261/400, Loss: 1.7038\n",
            "Epoch 262/400, Loss: 1.6001\n",
            "Epoch 263/400, Loss: 1.5661\n",
            "Epoch 264/400, Loss: 1.6423\n",
            "Epoch 265/400, Loss: 1.6077\n",
            "Epoch 266/400, Loss: 1.6364\n",
            "Epoch 267/400, Loss: 1.6010\n",
            "Epoch 268/400, Loss: 1.6312\n",
            "Epoch 269/400, Loss: 1.6093\n",
            "Epoch 270/400, Loss: 1.6212\n",
            "Epoch 271/400, Loss: 1.6086\n",
            "Epoch 272/400, Loss: 1.6546\n",
            "Epoch 273/400, Loss: 1.6428\n",
            "Epoch 274/400, Loss: 1.5960\n",
            "Epoch 275/400, Loss: 1.6096\n",
            "Epoch 276/400, Loss: 1.5765\n",
            "Epoch 277/400, Loss: 1.6222\n",
            "Epoch 278/400, Loss: 1.6282\n",
            "Epoch 279/400, Loss: 1.6542\n",
            "Epoch 280/400, Loss: 1.5680\n",
            "Epoch 281/400, Loss: 1.6172\n",
            "Epoch 282/400, Loss: 1.5967\n",
            "Epoch 283/400, Loss: 1.6158\n",
            "Epoch 284/400, Loss: 1.6131\n",
            "Epoch 285/400, Loss: 1.5984\n",
            "Epoch 286/400, Loss: 1.6165\n",
            "Epoch 287/400, Loss: 1.5976\n",
            "Epoch 288/400, Loss: 1.6038\n",
            "Epoch 289/400, Loss: 1.5507\n",
            "Epoch 290/400, Loss: 1.6023\n",
            "Epoch 291/400, Loss: 1.6191\n",
            "Epoch 292/400, Loss: 1.6110\n",
            "Epoch 293/400, Loss: 1.5594\n",
            "Epoch 294/400, Loss: 1.5990\n",
            "Epoch 295/400, Loss: 1.5979\n",
            "Epoch 296/400, Loss: 1.5705\n",
            "Epoch 297/400, Loss: 1.5749\n",
            "Epoch 298/400, Loss: 1.5851\n",
            "Epoch 299/400, Loss: 1.5622\n",
            "Epoch 300/400, Loss: 1.5498\n",
            "Epoch 301/400, Loss: 1.5365\n",
            "Epoch 302/400, Loss: 1.5703\n",
            "Epoch 303/400, Loss: 1.5769\n",
            "Epoch 304/400, Loss: 1.5774\n",
            "Epoch 305/400, Loss: 1.5460\n",
            "Epoch 306/400, Loss: 1.5577\n",
            "Epoch 307/400, Loss: 1.5681\n",
            "Epoch 308/400, Loss: 1.5732\n",
            "Epoch 309/400, Loss: 1.5903\n",
            "Epoch 310/400, Loss: 1.5585\n",
            "Epoch 311/400, Loss: 1.6143\n",
            "Epoch 312/400, Loss: 1.5529\n",
            "Epoch 313/400, Loss: 1.5508\n",
            "Epoch 314/400, Loss: 1.5337\n",
            "Epoch 315/400, Loss: 1.5300\n",
            "Epoch 316/400, Loss: 1.5703\n",
            "Epoch 317/400, Loss: 1.5528\n",
            "Epoch 318/400, Loss: 1.5443\n",
            "Epoch 319/400, Loss: 1.5257\n",
            "Epoch 320/400, Loss: 1.5812\n",
            "Epoch 321/400, Loss: 1.5194\n",
            "Epoch 322/400, Loss: 1.5578\n",
            "Epoch 323/400, Loss: 1.5378\n",
            "Epoch 324/400, Loss: 1.5306\n",
            "Epoch 325/400, Loss: 1.5329\n",
            "Epoch 326/400, Loss: 1.5271\n",
            "Epoch 327/400, Loss: 1.5552\n",
            "Epoch 328/400, Loss: 1.5572\n",
            "Epoch 329/400, Loss: 1.6073\n",
            "Epoch 330/400, Loss: 1.5146\n",
            "Epoch 331/400, Loss: 1.4967\n",
            "Epoch 332/400, Loss: 1.5453\n",
            "Epoch 333/400, Loss: 1.5013\n",
            "Epoch 334/400, Loss: 1.5512\n",
            "Epoch 335/400, Loss: 1.5067\n",
            "Epoch 336/400, Loss: 1.5314\n",
            "Epoch 337/400, Loss: 1.5331\n",
            "Epoch 338/400, Loss: 1.5468\n",
            "Epoch 339/400, Loss: 1.5748\n",
            "Epoch 340/400, Loss: 1.5546\n",
            "Epoch 341/400, Loss: 1.5742\n",
            "Epoch 342/400, Loss: 1.5375\n",
            "Epoch 343/400, Loss: 1.5288\n",
            "Epoch 344/400, Loss: 1.5560\n",
            "Epoch 345/400, Loss: 1.5168\n",
            "Epoch 346/400, Loss: 1.5345\n",
            "Epoch 347/400, Loss: 1.5163\n",
            "Epoch 348/400, Loss: 1.5711\n",
            "Epoch 349/400, Loss: 1.5391\n",
            "Epoch 350/400, Loss: 1.5038\n",
            "Epoch 351/400, Loss: 1.5733\n",
            "Epoch 352/400, Loss: 1.5367\n",
            "Epoch 353/400, Loss: 1.5275\n",
            "Epoch 354/400, Loss: 1.5064\n",
            "Epoch 355/400, Loss: 1.5534\n",
            "Epoch 356/400, Loss: 1.5028\n",
            "Epoch 357/400, Loss: 1.5131\n",
            "Epoch 358/400, Loss: 1.5376\n",
            "Epoch 359/400, Loss: 1.5334\n",
            "Epoch 360/400, Loss: 1.4930\n",
            "Epoch 361/400, Loss: 1.5380\n",
            "Epoch 362/400, Loss: 1.5097\n",
            "Epoch 363/400, Loss: 1.4828\n",
            "Epoch 364/400, Loss: 1.5012\n",
            "Epoch 365/400, Loss: 1.5320\n",
            "Epoch 366/400, Loss: 1.5195\n",
            "Epoch 367/400, Loss: 1.5354\n",
            "Epoch 368/400, Loss: 1.5547\n",
            "Epoch 369/400, Loss: 1.5461\n",
            "Epoch 370/400, Loss: 1.5068\n",
            "Epoch 371/400, Loss: 1.5138\n",
            "Epoch 372/400, Loss: 1.5268\n",
            "Epoch 373/400, Loss: 1.4918\n",
            "Epoch 374/400, Loss: 1.5298\n",
            "Epoch 375/400, Loss: 1.5137\n",
            "Epoch 376/400, Loss: 1.5093\n",
            "Epoch 377/400, Loss: 1.5005\n",
            "Epoch 378/400, Loss: 1.5281\n",
            "Epoch 379/400, Loss: 1.5197\n",
            "Epoch 380/400, Loss: 1.5646\n",
            "Epoch 381/400, Loss: 1.5298\n",
            "Epoch 382/400, Loss: 1.4872\n",
            "Epoch 383/400, Loss: 1.4871\n",
            "Epoch 384/400, Loss: 1.5181\n",
            "Epoch 385/400, Loss: 1.5196\n",
            "Epoch 386/400, Loss: 1.5246\n",
            "Epoch 387/400, Loss: 1.5053\n",
            "Epoch 388/400, Loss: 1.4984\n",
            "Epoch 389/400, Loss: 1.4829\n",
            "Epoch 390/400, Loss: 1.4833\n",
            "Epoch 391/400, Loss: 1.4733\n",
            "Epoch 392/400, Loss: 1.4375\n",
            "Epoch 393/400, Loss: 1.5336\n",
            "Epoch 394/400, Loss: 1.4364\n",
            "Epoch 395/400, Loss: 1.5380\n",
            "Epoch 396/400, Loss: 1.4588\n",
            "Epoch 397/400, Loss: 1.4873\n",
            "Epoch 398/400, Loss: 1.4813\n",
            "Epoch 399/400, Loss: 1.4965\n",
            "Epoch 400/400, Loss: 1.4708\n",
            "{'0': {'precision': 1.0, 'recall': 0.7142857142857143, 'f1-score': 0.8333333333333334, 'support': 7.0}, '1': {'precision': 0.7777777777777778, 'recall': 1.0, 'f1-score': 0.875, 'support': 7.0}, 'accuracy': 0.8571428571428571, 'macro avg': {'precision': 0.8888888888888888, 'recall': 0.8571428571428572, 'f1-score': 0.8541666666666667, 'support': 14.0}, 'weighted avg': {'precision': 0.888888888888889, 'recall': 0.8571428571428571, 'f1-score': 0.8541666666666667, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 8, attention_layers = 10, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 400, learning_rate = 1e-5)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vtq_fPOYZx6M",
        "outputId": "86731d14-6645-4ec6-c441-85d43e182931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 2.8682\n",
            "Epoch 2/500, Loss: 2.8404\n",
            "Epoch 3/500, Loss: 2.8063\n",
            "Epoch 4/500, Loss: 2.8076\n",
            "Epoch 5/500, Loss: 2.7793\n",
            "Epoch 6/500, Loss: 2.7722\n",
            "Epoch 7/500, Loss: 2.7727\n",
            "Epoch 8/500, Loss: 2.7708\n",
            "Epoch 9/500, Loss: 2.7738\n",
            "Epoch 10/500, Loss: 2.7741\n",
            "Epoch 11/500, Loss: 2.7704\n",
            "Epoch 12/500, Loss: 2.7723\n",
            "Epoch 13/500, Loss: 2.7717\n",
            "Epoch 14/500, Loss: 2.7723\n",
            "Epoch 15/500, Loss: 2.7704\n",
            "Epoch 16/500, Loss: 2.7739\n",
            "Epoch 17/500, Loss: 2.7689\n",
            "Epoch 18/500, Loss: 2.7730\n",
            "Epoch 19/500, Loss: 2.7719\n",
            "Epoch 20/500, Loss: 2.7702\n",
            "Epoch 21/500, Loss: 2.7725\n",
            "Epoch 22/500, Loss: 2.7723\n",
            "Epoch 23/500, Loss: 2.7713\n",
            "Epoch 24/500, Loss: 2.7742\n",
            "Epoch 25/500, Loss: 2.7700\n",
            "Epoch 26/500, Loss: 2.7714\n",
            "Epoch 27/500, Loss: 2.7696\n",
            "Epoch 28/500, Loss: 2.7707\n",
            "Epoch 29/500, Loss: 2.7695\n",
            "Epoch 30/500, Loss: 2.7702\n",
            "Epoch 31/500, Loss: 2.7685\n",
            "Epoch 32/500, Loss: 2.7712\n",
            "Epoch 33/500, Loss: 2.7711\n",
            "Epoch 34/500, Loss: 2.7704\n",
            "Epoch 35/500, Loss: 2.7719\n",
            "Epoch 36/500, Loss: 2.7699\n",
            "Epoch 37/500, Loss: 2.7713\n",
            "Epoch 38/500, Loss: 2.7688\n",
            "Epoch 39/500, Loss: 2.7709\n",
            "Epoch 40/500, Loss: 2.7725\n",
            "Epoch 41/500, Loss: 2.7717\n",
            "Epoch 42/500, Loss: 2.7648\n",
            "Epoch 43/500, Loss: 2.7657\n",
            "Epoch 44/500, Loss: 2.7737\n",
            "Epoch 45/500, Loss: 2.7699\n",
            "Epoch 46/500, Loss: 2.7661\n",
            "Epoch 47/500, Loss: 2.7700\n",
            "Epoch 48/500, Loss: 2.7721\n",
            "Epoch 49/500, Loss: 2.7712\n",
            "Epoch 50/500, Loss: 2.7688\n",
            "Epoch 51/500, Loss: 2.7703\n",
            "Epoch 52/500, Loss: 2.7694\n",
            "Epoch 53/500, Loss: 2.7681\n",
            "Epoch 54/500, Loss: 2.7677\n",
            "Epoch 55/500, Loss: 2.7677\n",
            "Epoch 56/500, Loss: 2.7696\n",
            "Epoch 57/500, Loss: 2.7686\n",
            "Epoch 58/500, Loss: 2.7696\n",
            "Epoch 59/500, Loss: 2.7692\n",
            "Epoch 60/500, Loss: 2.7700\n",
            "Epoch 61/500, Loss: 2.7672\n",
            "Epoch 62/500, Loss: 2.7688\n",
            "Epoch 63/500, Loss: 2.7703\n",
            "Epoch 64/500, Loss: 2.7648\n",
            "Epoch 65/500, Loss: 2.7661\n",
            "Epoch 66/500, Loss: 2.7692\n",
            "Epoch 67/500, Loss: 2.7700\n",
            "Epoch 68/500, Loss: 2.7680\n",
            "Epoch 69/500, Loss: 2.7687\n",
            "Epoch 70/500, Loss: 2.7708\n",
            "Epoch 71/500, Loss: 2.7716\n",
            "Epoch 72/500, Loss: 2.7656\n",
            "Epoch 73/500, Loss: 2.7664\n",
            "Epoch 74/500, Loss: 2.7676\n",
            "Epoch 75/500, Loss: 2.7682\n",
            "Epoch 76/500, Loss: 2.7673\n",
            "Epoch 77/500, Loss: 2.7692\n",
            "Epoch 78/500, Loss: 2.7696\n",
            "Epoch 79/500, Loss: 2.7675\n",
            "Epoch 80/500, Loss: 2.7675\n",
            "Epoch 81/500, Loss: 2.7677\n",
            "Epoch 82/500, Loss: 2.7683\n",
            "Epoch 83/500, Loss: 2.7694\n",
            "Epoch 84/500, Loss: 2.7681\n",
            "Epoch 85/500, Loss: 2.7645\n",
            "Epoch 86/500, Loss: 2.7682\n",
            "Epoch 87/500, Loss: 2.7678\n",
            "Epoch 88/500, Loss: 2.7646\n",
            "Epoch 89/500, Loss: 2.7659\n",
            "Epoch 90/500, Loss: 2.7663\n",
            "Epoch 91/500, Loss: 2.7677\n",
            "Epoch 92/500, Loss: 2.7687\n",
            "Epoch 93/500, Loss: 2.7682\n",
            "Epoch 94/500, Loss: 2.7663\n",
            "Epoch 95/500, Loss: 2.7659\n",
            "Epoch 96/500, Loss: 2.7662\n",
            "Epoch 97/500, Loss: 2.7645\n",
            "Epoch 98/500, Loss: 2.7683\n",
            "Epoch 99/500, Loss: 2.7668\n",
            "Epoch 100/500, Loss: 2.7647\n",
            "Epoch 101/500, Loss: 2.7638\n",
            "Epoch 102/500, Loss: 2.7671\n",
            "Epoch 103/500, Loss: 2.7653\n",
            "Epoch 104/500, Loss: 2.7647\n",
            "Epoch 105/500, Loss: 2.7668\n",
            "Epoch 106/500, Loss: 2.7656\n",
            "Epoch 107/500, Loss: 2.7672\n",
            "Epoch 108/500, Loss: 2.7679\n",
            "Epoch 109/500, Loss: 2.7630\n",
            "Epoch 110/500, Loss: 2.7647\n",
            "Epoch 111/500, Loss: 2.7676\n",
            "Epoch 112/500, Loss: 2.7642\n",
            "Epoch 113/500, Loss: 2.7661\n",
            "Epoch 114/500, Loss: 2.7670\n",
            "Epoch 115/500, Loss: 2.7658\n",
            "Epoch 116/500, Loss: 2.7642\n",
            "Epoch 117/500, Loss: 2.7675\n",
            "Epoch 118/500, Loss: 2.7645\n",
            "Epoch 119/500, Loss: 2.7632\n",
            "Epoch 120/500, Loss: 2.7642\n",
            "Epoch 121/500, Loss: 2.7667\n",
            "Epoch 122/500, Loss: 2.7635\n",
            "Epoch 123/500, Loss: 2.7612\n",
            "Epoch 124/500, Loss: 2.7660\n",
            "Epoch 125/500, Loss: 2.7649\n",
            "Epoch 126/500, Loss: 2.7662\n",
            "Epoch 127/500, Loss: 2.7664\n",
            "Epoch 128/500, Loss: 2.7644\n",
            "Epoch 129/500, Loss: 2.7621\n",
            "Epoch 130/500, Loss: 2.7630\n",
            "Epoch 131/500, Loss: 2.7638\n",
            "Epoch 132/500, Loss: 2.7629\n",
            "Epoch 133/500, Loss: 2.7623\n",
            "Epoch 134/500, Loss: 2.7659\n",
            "Epoch 135/500, Loss: 2.7698\n",
            "Epoch 136/500, Loss: 2.7663\n",
            "Epoch 137/500, Loss: 2.7633\n",
            "Epoch 138/500, Loss: 2.7609\n",
            "Epoch 139/500, Loss: 2.7601\n",
            "Epoch 140/500, Loss: 2.7652\n",
            "Epoch 141/500, Loss: 2.7604\n",
            "Epoch 142/500, Loss: 2.7615\n",
            "Epoch 143/500, Loss: 2.7616\n",
            "Epoch 144/500, Loss: 2.7622\n",
            "Epoch 145/500, Loss: 2.7603\n",
            "Epoch 146/500, Loss: 2.7623\n",
            "Epoch 147/500, Loss: 2.7627\n",
            "Epoch 148/500, Loss: 2.7628\n",
            "Epoch 149/500, Loss: 2.7631\n",
            "Epoch 150/500, Loss: 2.7615\n",
            "Epoch 151/500, Loss: 2.7647\n",
            "Epoch 152/500, Loss: 2.7647\n",
            "Epoch 153/500, Loss: 2.7607\n",
            "Epoch 154/500, Loss: 2.7624\n",
            "Epoch 155/500, Loss: 2.7579\n",
            "Epoch 156/500, Loss: 2.7627\n",
            "Epoch 157/500, Loss: 2.7611\n",
            "Epoch 158/500, Loss: 2.7610\n",
            "Epoch 159/500, Loss: 2.7632\n",
            "Epoch 160/500, Loss: 2.7602\n",
            "Epoch 161/500, Loss: 2.7637\n",
            "Epoch 162/500, Loss: 2.7621\n",
            "Epoch 163/500, Loss: 2.7597\n",
            "Epoch 164/500, Loss: 2.7622\n",
            "Epoch 165/500, Loss: 2.7617\n",
            "Epoch 166/500, Loss: 2.7608\n",
            "Epoch 167/500, Loss: 2.7608\n",
            "Epoch 168/500, Loss: 2.7603\n",
            "Epoch 169/500, Loss: 2.7648\n",
            "Epoch 170/500, Loss: 2.7563\n",
            "Epoch 171/500, Loss: 2.7615\n",
            "Epoch 172/500, Loss: 2.7616\n",
            "Epoch 173/500, Loss: 2.7600\n",
            "Epoch 174/500, Loss: 2.7627\n",
            "Epoch 175/500, Loss: 2.7592\n",
            "Epoch 176/500, Loss: 2.7621\n",
            "Epoch 177/500, Loss: 2.7599\n",
            "Epoch 178/500, Loss: 2.7593\n",
            "Epoch 179/500, Loss: 2.7590\n",
            "Epoch 180/500, Loss: 2.7578\n",
            "Epoch 181/500, Loss: 2.7592\n",
            "Epoch 182/500, Loss: 2.7603\n",
            "Epoch 183/500, Loss: 2.7599\n",
            "Epoch 184/500, Loss: 2.7577\n",
            "Epoch 185/500, Loss: 2.7591\n",
            "Epoch 186/500, Loss: 2.7578\n",
            "Epoch 187/500, Loss: 2.7593\n",
            "Epoch 188/500, Loss: 2.7604\n",
            "Epoch 189/500, Loss: 2.7595\n",
            "Epoch 190/500, Loss: 2.7595\n",
            "Epoch 191/500, Loss: 2.7566\n",
            "Epoch 192/500, Loss: 2.7579\n",
            "Epoch 193/500, Loss: 2.7586\n",
            "Epoch 194/500, Loss: 2.7587\n",
            "Epoch 195/500, Loss: 2.7587\n",
            "Epoch 196/500, Loss: 2.7590\n",
            "Epoch 197/500, Loss: 2.7624\n",
            "Epoch 198/500, Loss: 2.7588\n",
            "Epoch 199/500, Loss: 2.7568\n",
            "Epoch 200/500, Loss: 2.7594\n",
            "Epoch 201/500, Loss: 2.7586\n",
            "Epoch 202/500, Loss: 2.7556\n",
            "Epoch 203/500, Loss: 2.7563\n",
            "Epoch 204/500, Loss: 2.7574\n",
            "Epoch 205/500, Loss: 2.7562\n",
            "Epoch 206/500, Loss: 2.7599\n",
            "Epoch 207/500, Loss: 2.7602\n",
            "Epoch 208/500, Loss: 2.7589\n",
            "Epoch 209/500, Loss: 2.7532\n",
            "Epoch 210/500, Loss: 2.7610\n",
            "Epoch 211/500, Loss: 2.7600\n",
            "Epoch 212/500, Loss: 2.7559\n",
            "Epoch 213/500, Loss: 2.7563\n",
            "Epoch 214/500, Loss: 2.7541\n",
            "Epoch 215/500, Loss: 2.7554\n",
            "Epoch 216/500, Loss: 2.7501\n",
            "Epoch 217/500, Loss: 2.7612\n",
            "Epoch 218/500, Loss: 2.7544\n",
            "Epoch 219/500, Loss: 2.7524\n",
            "Epoch 220/500, Loss: 2.7597\n",
            "Epoch 221/500, Loss: 2.7561\n",
            "Epoch 222/500, Loss: 2.7559\n",
            "Epoch 223/500, Loss: 2.7548\n",
            "Epoch 224/500, Loss: 2.7616\n",
            "Epoch 225/500, Loss: 2.7568\n",
            "Epoch 226/500, Loss: 2.7586\n",
            "Epoch 227/500, Loss: 2.7566\n",
            "Epoch 228/500, Loss: 2.7570\n",
            "Epoch 229/500, Loss: 2.7594\n",
            "Epoch 230/500, Loss: 2.7559\n",
            "Epoch 231/500, Loss: 2.7566\n",
            "Epoch 232/500, Loss: 2.7568\n",
            "Epoch 233/500, Loss: 2.7574\n",
            "Epoch 234/500, Loss: 2.7573\n",
            "Epoch 235/500, Loss: 2.7585\n",
            "Epoch 236/500, Loss: 2.7589\n",
            "Epoch 237/500, Loss: 2.7568\n",
            "Epoch 238/500, Loss: 2.7537\n",
            "Epoch 239/500, Loss: 2.7555\n",
            "Epoch 240/500, Loss: 2.7583\n",
            "Epoch 241/500, Loss: 2.7544\n",
            "Epoch 242/500, Loss: 2.7548\n",
            "Epoch 243/500, Loss: 2.7534\n",
            "Epoch 244/500, Loss: 2.7568\n",
            "Epoch 245/500, Loss: 2.7566\n",
            "Epoch 246/500, Loss: 2.7554\n",
            "Epoch 247/500, Loss: 2.7554\n",
            "Epoch 248/500, Loss: 2.7582\n",
            "Epoch 249/500, Loss: 2.7537\n",
            "Epoch 250/500, Loss: 2.7543\n",
            "Epoch 251/500, Loss: 2.7512\n",
            "Epoch 252/500, Loss: 2.7523\n",
            "Epoch 253/500, Loss: 2.7564\n",
            "Epoch 254/500, Loss: 2.7593\n",
            "Epoch 255/500, Loss: 2.7495\n",
            "Epoch 256/500, Loss: 2.7545\n",
            "Epoch 257/500, Loss: 2.7462\n",
            "Epoch 258/500, Loss: 2.7553\n",
            "Epoch 259/500, Loss: 2.7537\n",
            "Epoch 260/500, Loss: 2.7569\n",
            "Epoch 261/500, Loss: 2.7554\n",
            "Epoch 262/500, Loss: 2.7442\n",
            "Epoch 263/500, Loss: 2.7532\n",
            "Epoch 264/500, Loss: 2.7504\n",
            "Epoch 265/500, Loss: 2.7546\n",
            "Epoch 266/500, Loss: 2.7566\n",
            "Epoch 267/500, Loss: 2.7473\n",
            "Epoch 268/500, Loss: 2.7564\n",
            "Epoch 269/500, Loss: 2.7510\n",
            "Epoch 270/500, Loss: 2.7540\n",
            "Epoch 271/500, Loss: 2.7481\n",
            "Epoch 272/500, Loss: 2.7535\n",
            "Epoch 273/500, Loss: 2.7501\n",
            "Epoch 274/500, Loss: 2.7464\n",
            "Epoch 275/500, Loss: 2.7548\n",
            "Epoch 276/500, Loss: 2.7541\n",
            "Epoch 277/500, Loss: 2.7519\n",
            "Epoch 278/500, Loss: 2.7516\n",
            "Epoch 279/500, Loss: 2.7548\n",
            "Epoch 280/500, Loss: 2.7519\n",
            "Epoch 281/500, Loss: 2.7546\n",
            "Epoch 282/500, Loss: 2.7498\n",
            "Epoch 283/500, Loss: 2.7525\n",
            "Epoch 284/500, Loss: 2.7531\n",
            "Epoch 285/500, Loss: 2.7521\n",
            "Epoch 286/500, Loss: 2.7541\n",
            "Epoch 287/500, Loss: 2.7507\n",
            "Epoch 288/500, Loss: 2.7545\n",
            "Epoch 289/500, Loss: 2.7481\n",
            "Epoch 290/500, Loss: 2.7523\n",
            "Epoch 291/500, Loss: 2.7488\n",
            "Epoch 292/500, Loss: 2.7491\n",
            "Epoch 293/500, Loss: 2.7511\n",
            "Epoch 294/500, Loss: 2.7524\n",
            "Epoch 295/500, Loss: 2.7542\n",
            "Epoch 296/500, Loss: 2.7499\n",
            "Epoch 297/500, Loss: 2.7493\n",
            "Epoch 298/500, Loss: 2.7515\n",
            "Epoch 299/500, Loss: 2.7537\n",
            "Epoch 300/500, Loss: 2.7512\n",
            "Epoch 301/500, Loss: 2.7463\n",
            "Epoch 302/500, Loss: 2.7460\n",
            "Epoch 303/500, Loss: 2.7516\n",
            "Epoch 304/500, Loss: 2.7497\n",
            "Epoch 305/500, Loss: 2.7483\n",
            "Epoch 306/500, Loss: 2.7501\n",
            "Epoch 307/500, Loss: 2.7513\n",
            "Epoch 308/500, Loss: 2.7506\n",
            "Epoch 309/500, Loss: 2.7525\n",
            "Epoch 310/500, Loss: 2.7507\n",
            "Epoch 311/500, Loss: 2.7524\n",
            "Epoch 312/500, Loss: 2.7511\n",
            "Epoch 313/500, Loss: 2.7493\n",
            "Epoch 314/500, Loss: 2.7485\n",
            "Epoch 315/500, Loss: 2.7466\n",
            "Epoch 316/500, Loss: 2.7484\n",
            "Epoch 317/500, Loss: 2.7500\n",
            "Epoch 318/500, Loss: 2.7504\n",
            "Epoch 319/500, Loss: 2.7440\n",
            "Epoch 320/500, Loss: 2.7477\n",
            "Epoch 321/500, Loss: 2.7465\n",
            "Epoch 322/500, Loss: 2.7496\n",
            "Epoch 323/500, Loss: 2.7513\n",
            "Epoch 324/500, Loss: 2.7499\n",
            "Epoch 325/500, Loss: 2.7472\n",
            "Epoch 326/500, Loss: 2.7426\n",
            "Epoch 327/500, Loss: 2.7493\n",
            "Epoch 328/500, Loss: 2.7471\n",
            "Epoch 329/500, Loss: 2.7528\n",
            "Epoch 330/500, Loss: 2.7467\n",
            "Epoch 331/500, Loss: 2.7483\n",
            "Epoch 332/500, Loss: 2.7498\n",
            "Epoch 333/500, Loss: 2.7487\n",
            "Epoch 334/500, Loss: 2.7486\n",
            "Epoch 335/500, Loss: 2.7456\n",
            "Epoch 336/500, Loss: 2.7451\n",
            "Epoch 337/500, Loss: 2.7520\n",
            "Epoch 338/500, Loss: 2.7447\n",
            "Epoch 339/500, Loss: 2.7422\n",
            "Epoch 340/500, Loss: 2.7481\n",
            "Epoch 341/500, Loss: 2.7488\n",
            "Epoch 342/500, Loss: 2.7467\n",
            "Epoch 343/500, Loss: 2.7467\n",
            "Epoch 344/500, Loss: 2.7389\n",
            "Epoch 345/500, Loss: 2.7431\n",
            "Epoch 346/500, Loss: 2.7496\n",
            "Epoch 347/500, Loss: 2.7455\n",
            "Epoch 348/500, Loss: 2.7367\n",
            "Epoch 349/500, Loss: 2.7477\n",
            "Epoch 350/500, Loss: 2.7472\n",
            "Epoch 351/500, Loss: 2.7430\n",
            "Epoch 352/500, Loss: 2.7421\n",
            "Epoch 353/500, Loss: 2.7441\n",
            "Epoch 354/500, Loss: 2.7435\n",
            "Epoch 355/500, Loss: 2.7441\n",
            "Epoch 356/500, Loss: 2.7451\n",
            "Epoch 357/500, Loss: 2.7419\n",
            "Epoch 358/500, Loss: 2.7434\n",
            "Epoch 359/500, Loss: 2.7402\n",
            "Epoch 360/500, Loss: 2.7400\n",
            "Epoch 361/500, Loss: 2.7370\n",
            "Epoch 362/500, Loss: 2.7431\n",
            "Epoch 363/500, Loss: 2.7412\n",
            "Epoch 364/500, Loss: 2.7443\n",
            "Epoch 365/500, Loss: 2.7445\n",
            "Epoch 366/500, Loss: 2.7390\n",
            "Epoch 367/500, Loss: 2.7437\n",
            "Epoch 368/500, Loss: 2.7456\n",
            "Epoch 369/500, Loss: 2.7406\n",
            "Epoch 370/500, Loss: 2.7418\n",
            "Epoch 371/500, Loss: 2.7387\n",
            "Epoch 372/500, Loss: 2.7385\n",
            "Epoch 373/500, Loss: 2.7438\n",
            "Epoch 374/500, Loss: 2.7420\n",
            "Epoch 375/500, Loss: 2.7461\n",
            "Epoch 376/500, Loss: 2.7394\n",
            "Epoch 377/500, Loss: 2.7431\n",
            "Epoch 378/500, Loss: 2.7423\n",
            "Epoch 379/500, Loss: 2.7450\n",
            "Epoch 380/500, Loss: 2.7462\n",
            "Epoch 381/500, Loss: 2.7418\n",
            "Epoch 382/500, Loss: 2.7416\n",
            "Epoch 383/500, Loss: 2.7423\n",
            "Epoch 384/500, Loss: 2.7442\n",
            "Epoch 385/500, Loss: 2.7451\n",
            "Epoch 386/500, Loss: 2.7399\n",
            "Epoch 387/500, Loss: 2.7379\n",
            "Epoch 388/500, Loss: 2.7388\n",
            "Epoch 389/500, Loss: 2.7445\n",
            "Epoch 390/500, Loss: 2.7397\n",
            "Epoch 391/500, Loss: 2.7382\n",
            "Epoch 392/500, Loss: 2.7420\n",
            "Epoch 393/500, Loss: 2.7363\n",
            "Epoch 394/500, Loss: 2.7409\n",
            "Epoch 395/500, Loss: 2.7367\n",
            "Epoch 396/500, Loss: 2.7352\n",
            "Epoch 397/500, Loss: 2.7398\n",
            "Epoch 398/500, Loss: 2.7432\n",
            "Epoch 399/500, Loss: 2.7424\n",
            "Epoch 400/500, Loss: 2.7389\n",
            "Epoch 401/500, Loss: 2.7393\n",
            "Epoch 402/500, Loss: 2.7306\n",
            "Epoch 403/500, Loss: 2.7306\n",
            "Epoch 404/500, Loss: 2.7400\n",
            "Epoch 405/500, Loss: 2.7365\n",
            "Epoch 406/500, Loss: 2.7418\n",
            "Epoch 407/500, Loss: 2.7408\n",
            "Epoch 408/500, Loss: 2.7412\n",
            "Epoch 409/500, Loss: 2.7365\n",
            "Epoch 410/500, Loss: 2.7375\n",
            "Epoch 411/500, Loss: 2.7379\n",
            "Epoch 412/500, Loss: 2.7410\n",
            "Epoch 413/500, Loss: 2.7389\n",
            "Epoch 414/500, Loss: 2.7366\n",
            "Epoch 415/500, Loss: 2.7262\n",
            "Epoch 416/500, Loss: 2.7361\n",
            "Epoch 417/500, Loss: 2.7317\n",
            "Epoch 418/500, Loss: 2.7394\n",
            "Epoch 419/500, Loss: 2.7365\n",
            "Epoch 420/500, Loss: 2.7347\n",
            "Epoch 421/500, Loss: 2.7376\n",
            "Epoch 422/500, Loss: 2.7341\n",
            "Epoch 423/500, Loss: 2.7345\n",
            "Epoch 424/500, Loss: 2.7349\n",
            "Epoch 425/500, Loss: 2.7306\n",
            "Epoch 426/500, Loss: 2.7331\n",
            "Epoch 427/500, Loss: 2.7392\n",
            "Epoch 428/500, Loss: 2.7360\n",
            "Epoch 429/500, Loss: 2.7317\n",
            "Epoch 430/500, Loss: 2.7332\n",
            "Epoch 431/500, Loss: 2.7324\n",
            "Epoch 432/500, Loss: 2.7301\n",
            "Epoch 433/500, Loss: 2.7365\n",
            "Epoch 434/500, Loss: 2.7401\n",
            "Epoch 435/500, Loss: 2.7325\n",
            "Epoch 436/500, Loss: 2.7368\n",
            "Epoch 437/500, Loss: 2.7242\n",
            "Epoch 438/500, Loss: 2.7355\n",
            "Epoch 439/500, Loss: 2.7306\n",
            "Epoch 440/500, Loss: 2.7342\n",
            "Epoch 441/500, Loss: 2.7335\n",
            "Epoch 442/500, Loss: 2.7273\n",
            "Epoch 443/500, Loss: 2.7302\n",
            "Epoch 444/500, Loss: 2.7318\n",
            "Epoch 445/500, Loss: 2.7307\n",
            "Epoch 446/500, Loss: 2.7308\n",
            "Epoch 447/500, Loss: 2.7290\n",
            "Epoch 448/500, Loss: 2.7314\n",
            "Epoch 449/500, Loss: 2.7380\n",
            "Epoch 450/500, Loss: 2.7297\n",
            "Epoch 451/500, Loss: 2.7297\n",
            "Epoch 452/500, Loss: 2.7300\n",
            "Epoch 453/500, Loss: 2.7279\n",
            "Epoch 454/500, Loss: 2.7295\n",
            "Epoch 455/500, Loss: 2.7256\n",
            "Epoch 456/500, Loss: 2.7293\n",
            "Epoch 457/500, Loss: 2.7326\n",
            "Epoch 458/500, Loss: 2.7297\n",
            "Epoch 459/500, Loss: 2.7261\n",
            "Epoch 460/500, Loss: 2.7298\n",
            "Epoch 461/500, Loss: 2.7220\n",
            "Epoch 462/500, Loss: 2.7282\n",
            "Epoch 463/500, Loss: 2.7255\n",
            "Epoch 464/500, Loss: 2.7233\n",
            "Epoch 465/500, Loss: 2.7232\n",
            "Epoch 466/500, Loss: 2.7308\n",
            "Epoch 467/500, Loss: 2.7288\n",
            "Epoch 468/500, Loss: 2.7318\n",
            "Epoch 469/500, Loss: 2.7278\n",
            "Epoch 470/500, Loss: 2.7251\n",
            "Epoch 471/500, Loss: 2.7240\n",
            "Epoch 472/500, Loss: 2.7293\n",
            "Epoch 473/500, Loss: 2.7226\n",
            "Epoch 474/500, Loss: 2.7234\n",
            "Epoch 475/500, Loss: 2.7234\n",
            "Epoch 476/500, Loss: 2.7195\n",
            "Epoch 477/500, Loss: 2.7236\n",
            "Epoch 478/500, Loss: 2.7285\n",
            "Epoch 479/500, Loss: 2.7264\n",
            "Epoch 480/500, Loss: 2.7260\n",
            "Epoch 481/500, Loss: 2.7236\n",
            "Epoch 482/500, Loss: 2.7283\n",
            "Epoch 483/500, Loss: 2.7257\n",
            "Epoch 484/500, Loss: 2.7197\n",
            "Epoch 485/500, Loss: 2.7239\n",
            "Epoch 486/500, Loss: 2.7217\n",
            "Epoch 487/500, Loss: 2.7197\n",
            "Epoch 488/500, Loss: 2.7225\n",
            "Epoch 489/500, Loss: 2.7185\n",
            "Epoch 490/500, Loss: 2.7238\n",
            "Epoch 491/500, Loss: 2.7210\n",
            "Epoch 492/500, Loss: 2.7125\n",
            "Epoch 493/500, Loss: 2.7251\n",
            "Epoch 494/500, Loss: 2.7140\n",
            "Epoch 495/500, Loss: 2.7217\n",
            "Epoch 496/500, Loss: 2.7177\n",
            "Epoch 497/500, Loss: 2.7238\n",
            "Epoch 498/500, Loss: 2.7266\n",
            "Epoch 499/500, Loss: 2.7201\n",
            "Epoch 500/500, Loss: 2.7268\n",
            "{'0': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, '1': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}, 'weighted avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 10, attention_layers = 12, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 500, learning_rate = 1e-6)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KeHfGBHsZx6M",
        "outputId": "8d0867db-fd53-4450-bad8-2db0c4dc48c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 2.7777\n",
            "Epoch 2/500, Loss: 2.7722\n",
            "Epoch 3/500, Loss: 2.7731\n",
            "Epoch 4/500, Loss: 2.7716\n",
            "Epoch 5/500, Loss: 2.7728\n",
            "Epoch 6/500, Loss: 2.7724\n",
            "Epoch 7/500, Loss: 2.7718\n",
            "Epoch 8/500, Loss: 2.7719\n",
            "Epoch 9/500, Loss: 2.7732\n",
            "Epoch 10/500, Loss: 2.7692\n",
            "Epoch 11/500, Loss: 2.7710\n",
            "Epoch 12/500, Loss: 2.7728\n",
            "Epoch 13/500, Loss: 2.7723\n",
            "Epoch 14/500, Loss: 2.7701\n",
            "Epoch 15/500, Loss: 2.7725\n",
            "Epoch 16/500, Loss: 2.7720\n",
            "Epoch 17/500, Loss: 2.7713\n",
            "Epoch 18/500, Loss: 2.7708\n",
            "Epoch 19/500, Loss: 2.7714\n",
            "Epoch 20/500, Loss: 2.7671\n",
            "Epoch 21/500, Loss: 2.7704\n",
            "Epoch 22/500, Loss: 2.7705\n",
            "Epoch 23/500, Loss: 2.7705\n",
            "Epoch 24/500, Loss: 2.7694\n",
            "Epoch 25/500, Loss: 2.7719\n",
            "Epoch 26/500, Loss: 2.7705\n",
            "Epoch 27/500, Loss: 2.7692\n",
            "Epoch 28/500, Loss: 2.7692\n",
            "Epoch 29/500, Loss: 2.7662\n",
            "Epoch 30/500, Loss: 2.7716\n",
            "Epoch 31/500, Loss: 2.7685\n",
            "Epoch 32/500, Loss: 2.7692\n",
            "Epoch 33/500, Loss: 2.7701\n",
            "Epoch 34/500, Loss: 2.7681\n",
            "Epoch 35/500, Loss: 2.7670\n",
            "Epoch 36/500, Loss: 2.7674\n",
            "Epoch 37/500, Loss: 2.7687\n",
            "Epoch 38/500, Loss: 2.7645\n",
            "Epoch 39/500, Loss: 2.7700\n",
            "Epoch 40/500, Loss: 2.7716\n",
            "Epoch 41/500, Loss: 2.7693\n",
            "Epoch 42/500, Loss: 2.7632\n",
            "Epoch 43/500, Loss: 2.7648\n",
            "Epoch 44/500, Loss: 2.7718\n",
            "Epoch 45/500, Loss: 2.7671\n",
            "Epoch 46/500, Loss: 2.7641\n",
            "Epoch 47/500, Loss: 2.7710\n",
            "Epoch 48/500, Loss: 2.7692\n",
            "Epoch 49/500, Loss: 2.7643\n",
            "Epoch 50/500, Loss: 2.7652\n",
            "Epoch 51/500, Loss: 2.7674\n",
            "Epoch 52/500, Loss: 2.7692\n",
            "Epoch 53/500, Loss: 2.7643\n",
            "Epoch 54/500, Loss: 2.7668\n",
            "Epoch 55/500, Loss: 2.7684\n",
            "Epoch 56/500, Loss: 2.7679\n",
            "Epoch 57/500, Loss: 2.7659\n",
            "Epoch 58/500, Loss: 2.7659\n",
            "Epoch 59/500, Loss: 2.7645\n",
            "Epoch 60/500, Loss: 2.7691\n",
            "Epoch 61/500, Loss: 2.7673\n",
            "Epoch 62/500, Loss: 2.7667\n",
            "Epoch 63/500, Loss: 2.7691\n",
            "Epoch 64/500, Loss: 2.7613\n",
            "Epoch 65/500, Loss: 2.7658\n",
            "Epoch 66/500, Loss: 2.7674\n",
            "Epoch 67/500, Loss: 2.7657\n",
            "Epoch 68/500, Loss: 2.7657\n",
            "Epoch 69/500, Loss: 2.7682\n",
            "Epoch 70/500, Loss: 2.7657\n",
            "Epoch 71/500, Loss: 2.7667\n",
            "Epoch 72/500, Loss: 2.7622\n",
            "Epoch 73/500, Loss: 2.7644\n",
            "Epoch 74/500, Loss: 2.7650\n",
            "Epoch 75/500, Loss: 2.7635\n",
            "Epoch 76/500, Loss: 2.7603\n",
            "Epoch 77/500, Loss: 2.7687\n",
            "Epoch 78/500, Loss: 2.7653\n",
            "Epoch 79/500, Loss: 2.7660\n",
            "Epoch 80/500, Loss: 2.7638\n",
            "Epoch 81/500, Loss: 2.7653\n",
            "Epoch 82/500, Loss: 2.7675\n",
            "Epoch 83/500, Loss: 2.7660\n",
            "Epoch 84/500, Loss: 2.7632\n",
            "Epoch 85/500, Loss: 2.7612\n",
            "Epoch 86/500, Loss: 2.7644\n",
            "Epoch 87/500, Loss: 2.7644\n",
            "Epoch 88/500, Loss: 2.7595\n",
            "Epoch 89/500, Loss: 2.7610\n",
            "Epoch 90/500, Loss: 2.7631\n",
            "Epoch 91/500, Loss: 2.7653\n",
            "Epoch 92/500, Loss: 2.7644\n",
            "Epoch 93/500, Loss: 2.7614\n",
            "Epoch 94/500, Loss: 2.7636\n",
            "Epoch 95/500, Loss: 2.7615\n",
            "Epoch 96/500, Loss: 2.7593\n",
            "Epoch 97/500, Loss: 2.7651\n",
            "Epoch 98/500, Loss: 2.7655\n",
            "Epoch 99/500, Loss: 2.7635\n",
            "Epoch 100/500, Loss: 2.7629\n",
            "Epoch 101/500, Loss: 2.7627\n",
            "Epoch 102/500, Loss: 2.7632\n",
            "Epoch 103/500, Loss: 2.7604\n",
            "Epoch 104/500, Loss: 2.7594\n",
            "Epoch 105/500, Loss: 2.7634\n",
            "Epoch 106/500, Loss: 2.7595\n",
            "Epoch 107/500, Loss: 2.7621\n",
            "Epoch 108/500, Loss: 2.7594\n",
            "Epoch 109/500, Loss: 2.7593\n",
            "Epoch 110/500, Loss: 2.7600\n",
            "Epoch 111/500, Loss: 2.7614\n",
            "Epoch 112/500, Loss: 2.7577\n",
            "Epoch 113/500, Loss: 2.7592\n",
            "Epoch 114/500, Loss: 2.7565\n",
            "Epoch 115/500, Loss: 2.7639\n",
            "Epoch 116/500, Loss: 2.7602\n",
            "Epoch 117/500, Loss: 2.7611\n",
            "Epoch 118/500, Loss: 2.7606\n",
            "Epoch 119/500, Loss: 2.7579\n",
            "Epoch 120/500, Loss: 2.7601\n",
            "Epoch 121/500, Loss: 2.7598\n",
            "Epoch 122/500, Loss: 2.7554\n",
            "Epoch 123/500, Loss: 2.7561\n",
            "Epoch 124/500, Loss: 2.7596\n",
            "Epoch 125/500, Loss: 2.7577\n",
            "Epoch 126/500, Loss: 2.7597\n",
            "Epoch 127/500, Loss: 2.7603\n",
            "Epoch 128/500, Loss: 2.7602\n",
            "Epoch 129/500, Loss: 2.7558\n",
            "Epoch 130/500, Loss: 2.7558\n",
            "Epoch 131/500, Loss: 2.7574\n",
            "Epoch 132/500, Loss: 2.7566\n",
            "Epoch 133/500, Loss: 2.7573\n",
            "Epoch 134/500, Loss: 2.7561\n",
            "Epoch 135/500, Loss: 2.7618\n",
            "Epoch 136/500, Loss: 2.7587\n",
            "Epoch 137/500, Loss: 2.7583\n",
            "Epoch 138/500, Loss: 2.7542\n",
            "Epoch 139/500, Loss: 2.7531\n",
            "Epoch 140/500, Loss: 2.7588\n",
            "Epoch 141/500, Loss: 2.7525\n",
            "Epoch 142/500, Loss: 2.7556\n",
            "Epoch 143/500, Loss: 2.7553\n",
            "Epoch 144/500, Loss: 2.7564\n",
            "Epoch 145/500, Loss: 2.7550\n",
            "Epoch 146/500, Loss: 2.7546\n",
            "Epoch 147/500, Loss: 2.7572\n",
            "Epoch 148/500, Loss: 2.7551\n",
            "Epoch 149/500, Loss: 2.7551\n",
            "Epoch 150/500, Loss: 2.7575\n",
            "Epoch 151/500, Loss: 2.7564\n",
            "Epoch 152/500, Loss: 2.7533\n",
            "Epoch 153/500, Loss: 2.7555\n",
            "Epoch 154/500, Loss: 2.7550\n",
            "Epoch 155/500, Loss: 2.7444\n",
            "Epoch 156/500, Loss: 2.7571\n",
            "Epoch 157/500, Loss: 2.7546\n",
            "Epoch 158/500, Loss: 2.7557\n",
            "Epoch 159/500, Loss: 2.7545\n",
            "Epoch 160/500, Loss: 2.7493\n",
            "Epoch 161/500, Loss: 2.7551\n",
            "Epoch 162/500, Loss: 2.7522\n",
            "Epoch 163/500, Loss: 2.7534\n",
            "Epoch 164/500, Loss: 2.7560\n",
            "Epoch 165/500, Loss: 2.7549\n",
            "Epoch 166/500, Loss: 2.7509\n",
            "Epoch 167/500, Loss: 2.7547\n",
            "Epoch 168/500, Loss: 2.7539\n",
            "Epoch 169/500, Loss: 2.7489\n",
            "Epoch 170/500, Loss: 2.7443\n",
            "Epoch 171/500, Loss: 2.7503\n",
            "Epoch 172/500, Loss: 2.7525\n",
            "Epoch 173/500, Loss: 2.7473\n",
            "Epoch 174/500, Loss: 2.7535\n",
            "Epoch 175/500, Loss: 2.7505\n",
            "Epoch 176/500, Loss: 2.7535\n",
            "Epoch 177/500, Loss: 2.7509\n",
            "Epoch 178/500, Loss: 2.7489\n",
            "Epoch 179/500, Loss: 2.7429\n",
            "Epoch 180/500, Loss: 2.7456\n",
            "Epoch 181/500, Loss: 2.7520\n",
            "Epoch 182/500, Loss: 2.7495\n",
            "Epoch 183/500, Loss: 2.7495\n",
            "Epoch 184/500, Loss: 2.7469\n",
            "Epoch 185/500, Loss: 2.7492\n",
            "Epoch 186/500, Loss: 2.7419\n",
            "Epoch 187/500, Loss: 2.7495\n",
            "Epoch 188/500, Loss: 2.7487\n",
            "Epoch 189/500, Loss: 2.7505\n",
            "Epoch 190/500, Loss: 2.7518\n",
            "Epoch 191/500, Loss: 2.7484\n",
            "Epoch 192/500, Loss: 2.7490\n",
            "Epoch 193/500, Loss: 2.7502\n",
            "Epoch 194/500, Loss: 2.7494\n",
            "Epoch 195/500, Loss: 2.7488\n",
            "Epoch 196/500, Loss: 2.7473\n",
            "Epoch 197/500, Loss: 2.7524\n",
            "Epoch 198/500, Loss: 2.7476\n",
            "Epoch 199/500, Loss: 2.7444\n",
            "Epoch 200/500, Loss: 2.7481\n",
            "Epoch 201/500, Loss: 2.7484\n",
            "Epoch 202/500, Loss: 2.7405\n",
            "Epoch 203/500, Loss: 2.7478\n",
            "Epoch 204/500, Loss: 2.7473\n",
            "Epoch 205/500, Loss: 2.7462\n",
            "Epoch 206/500, Loss: 2.7503\n",
            "Epoch 207/500, Loss: 2.7454\n",
            "Epoch 208/500, Loss: 2.7452\n",
            "Epoch 209/500, Loss: 2.7405\n",
            "Epoch 210/500, Loss: 2.7473\n",
            "Epoch 211/500, Loss: 2.7438\n",
            "Epoch 212/500, Loss: 2.7430\n",
            "Epoch 213/500, Loss: 2.7456\n",
            "Epoch 214/500, Loss: 2.7424\n",
            "Epoch 215/500, Loss: 2.7402\n",
            "Epoch 216/500, Loss: 2.7387\n",
            "Epoch 217/500, Loss: 2.7489\n",
            "Epoch 218/500, Loss: 2.7384\n",
            "Epoch 219/500, Loss: 2.7375\n",
            "Epoch 220/500, Loss: 2.7494\n",
            "Epoch 221/500, Loss: 2.7414\n",
            "Epoch 222/500, Loss: 2.7436\n",
            "Epoch 223/500, Loss: 2.7416\n",
            "Epoch 224/500, Loss: 2.7470\n",
            "Epoch 225/500, Loss: 2.7422\n",
            "Epoch 226/500, Loss: 2.7481\n",
            "Epoch 227/500, Loss: 2.7420\n",
            "Epoch 228/500, Loss: 2.7433\n",
            "Epoch 229/500, Loss: 2.7442\n",
            "Epoch 230/500, Loss: 2.7393\n",
            "Epoch 231/500, Loss: 2.7423\n",
            "Epoch 232/500, Loss: 2.7428\n",
            "Epoch 233/500, Loss: 2.7402\n",
            "Epoch 234/500, Loss: 2.7406\n",
            "Epoch 235/500, Loss: 2.7408\n",
            "Epoch 236/500, Loss: 2.7434\n",
            "Epoch 237/500, Loss: 2.7406\n",
            "Epoch 238/500, Loss: 2.7353\n",
            "Epoch 239/500, Loss: 2.7411\n",
            "Epoch 240/500, Loss: 2.7411\n",
            "Epoch 241/500, Loss: 2.7369\n",
            "Epoch 242/500, Loss: 2.7397\n",
            "Epoch 243/500, Loss: 2.7336\n",
            "Epoch 244/500, Loss: 2.7423\n",
            "Epoch 245/500, Loss: 2.7427\n",
            "Epoch 246/500, Loss: 2.7391\n",
            "Epoch 247/500, Loss: 2.7391\n",
            "Epoch 248/500, Loss: 2.7420\n",
            "Epoch 249/500, Loss: 2.7358\n",
            "Epoch 250/500, Loss: 2.7394\n",
            "Epoch 251/500, Loss: 2.7364\n",
            "Epoch 252/500, Loss: 2.7342\n",
            "Epoch 253/500, Loss: 2.7386\n",
            "Epoch 254/500, Loss: 2.7417\n",
            "Epoch 255/500, Loss: 2.7358\n",
            "Epoch 256/500, Loss: 2.7385\n",
            "Epoch 257/500, Loss: 2.7237\n",
            "Epoch 258/500, Loss: 2.7405\n",
            "Epoch 259/500, Loss: 2.7338\n",
            "Epoch 260/500, Loss: 2.7370\n",
            "Epoch 261/500, Loss: 2.7429\n",
            "Epoch 262/500, Loss: 2.7226\n",
            "Epoch 263/500, Loss: 2.7331\n",
            "Epoch 264/500, Loss: 2.7317\n",
            "Epoch 265/500, Loss: 2.7374\n",
            "Epoch 266/500, Loss: 2.7373\n",
            "Epoch 267/500, Loss: 2.7244\n",
            "Epoch 268/500, Loss: 2.7387\n",
            "Epoch 269/500, Loss: 2.7301\n",
            "Epoch 270/500, Loss: 2.7324\n",
            "Epoch 271/500, Loss: 2.7272\n",
            "Epoch 272/500, Loss: 2.7335\n",
            "Epoch 273/500, Loss: 2.7317\n",
            "Epoch 274/500, Loss: 2.7199\n",
            "Epoch 275/500, Loss: 2.7345\n",
            "Epoch 276/500, Loss: 2.7324\n",
            "Epoch 277/500, Loss: 2.7318\n",
            "Epoch 278/500, Loss: 2.7303\n",
            "Epoch 279/500, Loss: 2.7368\n",
            "Epoch 280/500, Loss: 2.7317\n",
            "Epoch 281/500, Loss: 2.7357\n",
            "Epoch 282/500, Loss: 2.7324\n",
            "Epoch 283/500, Loss: 2.7301\n",
            "Epoch 284/500, Loss: 2.7309\n",
            "Epoch 285/500, Loss: 2.7304\n",
            "Epoch 286/500, Loss: 2.7309\n",
            "Epoch 287/500, Loss: 2.7293\n",
            "Epoch 288/500, Loss: 2.7305\n",
            "Epoch 289/500, Loss: 2.7238\n",
            "Epoch 290/500, Loss: 2.7331\n",
            "Epoch 291/500, Loss: 2.7248\n",
            "Epoch 292/500, Loss: 2.7274\n",
            "Epoch 293/500, Loss: 2.7282\n",
            "Epoch 294/500, Loss: 2.7296\n",
            "Epoch 295/500, Loss: 2.7302\n",
            "Epoch 296/500, Loss: 2.7283\n",
            "Epoch 297/500, Loss: 2.7241\n",
            "Epoch 298/500, Loss: 2.7263\n",
            "Epoch 299/500, Loss: 2.7322\n",
            "Epoch 300/500, Loss: 2.7275\n",
            "Epoch 301/500, Loss: 2.7227\n",
            "Epoch 302/500, Loss: 2.7217\n",
            "Epoch 303/500, Loss: 2.7279\n",
            "Epoch 304/500, Loss: 2.7238\n",
            "Epoch 305/500, Loss: 2.7207\n",
            "Epoch 306/500, Loss: 2.7234\n",
            "Epoch 307/500, Loss: 2.7225\n",
            "Epoch 308/500, Loss: 2.7270\n",
            "Epoch 309/500, Loss: 2.7274\n",
            "Epoch 310/500, Loss: 2.7257\n",
            "Epoch 311/500, Loss: 2.7271\n",
            "Epoch 312/500, Loss: 2.7252\n",
            "Epoch 313/500, Loss: 2.7207\n",
            "Epoch 314/500, Loss: 2.7233\n",
            "Epoch 315/500, Loss: 2.7175\n",
            "Epoch 316/500, Loss: 2.7211\n",
            "Epoch 317/500, Loss: 2.7262\n",
            "Epoch 318/500, Loss: 2.7232\n",
            "Epoch 319/500, Loss: 2.7152\n",
            "Epoch 320/500, Loss: 2.7230\n",
            "Epoch 321/500, Loss: 2.7176\n",
            "Epoch 322/500, Loss: 2.7223\n",
            "Epoch 323/500, Loss: 2.7231\n",
            "Epoch 324/500, Loss: 2.7222\n",
            "Epoch 325/500, Loss: 2.7168\n",
            "Epoch 326/500, Loss: 2.7073\n",
            "Epoch 327/500, Loss: 2.7183\n",
            "Epoch 328/500, Loss: 2.7154\n",
            "Epoch 329/500, Loss: 2.7279\n",
            "Epoch 330/500, Loss: 2.7182\n",
            "Epoch 331/500, Loss: 2.7187\n",
            "Epoch 332/500, Loss: 2.7217\n",
            "Epoch 333/500, Loss: 2.7206\n",
            "Epoch 334/500, Loss: 2.7216\n",
            "Epoch 335/500, Loss: 2.7133\n",
            "Epoch 336/500, Loss: 2.7155\n",
            "Epoch 337/500, Loss: 2.7260\n",
            "Epoch 338/500, Loss: 2.7180\n",
            "Epoch 339/500, Loss: 2.7088\n",
            "Epoch 340/500, Loss: 2.7159\n",
            "Epoch 341/500, Loss: 2.7207\n",
            "Epoch 342/500, Loss: 2.7135\n",
            "Epoch 343/500, Loss: 2.7147\n",
            "Epoch 344/500, Loss: 2.7047\n",
            "Epoch 345/500, Loss: 2.7083\n",
            "Epoch 346/500, Loss: 2.7174\n",
            "Epoch 347/500, Loss: 2.7144\n",
            "Epoch 348/500, Loss: 2.6999\n",
            "Epoch 349/500, Loss: 2.7122\n",
            "Epoch 350/500, Loss: 2.7136\n",
            "Epoch 351/500, Loss: 2.7103\n",
            "Epoch 352/500, Loss: 2.7057\n",
            "Epoch 353/500, Loss: 2.7068\n",
            "Epoch 354/500, Loss: 2.7055\n",
            "Epoch 355/500, Loss: 2.7088\n",
            "Epoch 356/500, Loss: 2.7110\n",
            "Epoch 357/500, Loss: 2.7033\n",
            "Epoch 358/500, Loss: 2.7062\n",
            "Epoch 359/500, Loss: 2.6979\n",
            "Epoch 360/500, Loss: 2.7002\n",
            "Epoch 361/500, Loss: 2.6946\n",
            "Epoch 362/500, Loss: 2.7019\n",
            "Epoch 363/500, Loss: 2.7031\n",
            "Epoch 364/500, Loss: 2.7071\n",
            "Epoch 365/500, Loss: 2.7093\n",
            "Epoch 366/500, Loss: 2.7015\n",
            "Epoch 367/500, Loss: 2.7041\n",
            "Epoch 368/500, Loss: 2.7055\n",
            "Epoch 369/500, Loss: 2.7008\n",
            "Epoch 370/500, Loss: 2.7032\n",
            "Epoch 371/500, Loss: 2.6971\n",
            "Epoch 372/500, Loss: 2.6948\n",
            "Epoch 373/500, Loss: 2.7011\n",
            "Epoch 374/500, Loss: 2.7061\n",
            "Epoch 375/500, Loss: 2.7062\n",
            "Epoch 376/500, Loss: 2.6998\n",
            "Epoch 377/500, Loss: 2.7031\n",
            "Epoch 378/500, Loss: 2.7000\n",
            "Epoch 379/500, Loss: 2.7027\n",
            "Epoch 380/500, Loss: 2.7063\n",
            "Epoch 381/500, Loss: 2.6988\n",
            "Epoch 382/500, Loss: 2.7034\n",
            "Epoch 383/500, Loss: 2.7011\n",
            "Epoch 384/500, Loss: 2.7008\n",
            "Epoch 385/500, Loss: 2.7047\n",
            "Epoch 386/500, Loss: 2.6990\n",
            "Epoch 387/500, Loss: 2.6901\n",
            "Epoch 388/500, Loss: 2.6918\n",
            "Epoch 389/500, Loss: 2.7009\n",
            "Epoch 390/500, Loss: 2.6920\n",
            "Epoch 391/500, Loss: 2.6890\n",
            "Epoch 392/500, Loss: 2.6935\n",
            "Epoch 393/500, Loss: 2.6871\n",
            "Epoch 394/500, Loss: 2.6899\n",
            "Epoch 395/500, Loss: 2.6925\n",
            "Epoch 396/500, Loss: 2.6840\n",
            "Epoch 397/500, Loss: 2.6954\n",
            "Epoch 398/500, Loss: 2.6964\n",
            "Epoch 399/500, Loss: 2.6960\n",
            "Epoch 400/500, Loss: 2.6897\n",
            "Epoch 401/500, Loss: 2.6856\n",
            "Epoch 402/500, Loss: 2.6745\n",
            "Epoch 403/500, Loss: 2.6783\n",
            "Epoch 404/500, Loss: 2.6958\n",
            "Epoch 405/500, Loss: 2.6881\n",
            "Epoch 406/500, Loss: 2.6943\n",
            "Epoch 407/500, Loss: 2.6921\n",
            "Epoch 408/500, Loss: 2.6858\n",
            "Epoch 409/500, Loss: 2.6804\n",
            "Epoch 410/500, Loss: 2.6864\n",
            "Epoch 411/500, Loss: 2.6809\n",
            "Epoch 412/500, Loss: 2.6891\n",
            "Epoch 413/500, Loss: 2.6874\n",
            "Epoch 414/500, Loss: 2.6790\n",
            "Epoch 415/500, Loss: 2.6668\n",
            "Epoch 416/500, Loss: 2.6775\n",
            "Epoch 417/500, Loss: 2.6744\n",
            "Epoch 418/500, Loss: 2.6836\n",
            "Epoch 419/500, Loss: 2.6769\n",
            "Epoch 420/500, Loss: 2.6698\n",
            "Epoch 421/500, Loss: 2.6743\n",
            "Epoch 422/500, Loss: 2.6752\n",
            "Epoch 423/500, Loss: 2.6798\n",
            "Epoch 424/500, Loss: 2.6751\n",
            "Epoch 425/500, Loss: 2.6637\n",
            "Epoch 426/500, Loss: 2.6724\n",
            "Epoch 427/500, Loss: 2.6767\n",
            "Epoch 428/500, Loss: 2.6749\n",
            "Epoch 429/500, Loss: 2.6712\n",
            "Epoch 430/500, Loss: 2.6710\n",
            "Epoch 431/500, Loss: 2.6653\n",
            "Epoch 432/500, Loss: 2.6655\n",
            "Epoch 433/500, Loss: 2.6742\n",
            "Epoch 434/500, Loss: 2.6765\n",
            "Epoch 435/500, Loss: 2.6669\n",
            "Epoch 436/500, Loss: 2.6775\n",
            "Epoch 437/500, Loss: 2.6575\n",
            "Epoch 438/500, Loss: 2.6706\n",
            "Epoch 439/500, Loss: 2.6600\n",
            "Epoch 440/500, Loss: 2.6669\n",
            "Epoch 441/500, Loss: 2.6597\n",
            "Epoch 442/500, Loss: 2.6587\n",
            "Epoch 443/500, Loss: 2.6586\n",
            "Epoch 444/500, Loss: 2.6555\n",
            "Epoch 445/500, Loss: 2.6656\n",
            "Epoch 446/500, Loss: 2.6559\n",
            "Epoch 447/500, Loss: 2.6552\n",
            "Epoch 448/500, Loss: 2.6551\n",
            "Epoch 449/500, Loss: 2.6687\n",
            "Epoch 450/500, Loss: 2.6553\n",
            "Epoch 451/500, Loss: 2.6527\n",
            "Epoch 452/500, Loss: 2.6578\n",
            "Epoch 453/500, Loss: 2.6500\n",
            "Epoch 454/500, Loss: 2.6499\n",
            "Epoch 455/500, Loss: 2.6449\n",
            "Epoch 456/500, Loss: 2.6508\n",
            "Epoch 457/500, Loss: 2.6522\n",
            "Epoch 458/500, Loss: 2.6504\n",
            "Epoch 459/500, Loss: 2.6375\n",
            "Epoch 460/500, Loss: 2.6502\n",
            "Epoch 461/500, Loss: 2.6302\n",
            "Epoch 462/500, Loss: 2.6433\n",
            "Epoch 463/500, Loss: 2.6365\n",
            "Epoch 464/500, Loss: 2.6328\n",
            "Epoch 465/500, Loss: 2.6344\n",
            "Epoch 466/500, Loss: 2.6465\n",
            "Epoch 467/500, Loss: 2.6447\n",
            "Epoch 468/500, Loss: 2.6459\n",
            "Epoch 469/500, Loss: 2.6430\n",
            "Epoch 470/500, Loss: 2.6289\n",
            "Epoch 471/500, Loss: 2.6394\n",
            "Epoch 472/500, Loss: 2.6391\n",
            "Epoch 473/500, Loss: 2.6299\n",
            "Epoch 474/500, Loss: 2.6313\n",
            "Epoch 475/500, Loss: 2.6267\n",
            "Epoch 476/500, Loss: 2.6199\n",
            "Epoch 477/500, Loss: 2.6239\n",
            "Epoch 478/500, Loss: 2.6384\n",
            "Epoch 479/500, Loss: 2.6271\n",
            "Epoch 480/500, Loss: 2.6293\n",
            "Epoch 481/500, Loss: 2.6218\n",
            "Epoch 482/500, Loss: 2.6319\n",
            "Epoch 483/500, Loss: 2.6297\n",
            "Epoch 484/500, Loss: 2.6161\n",
            "Epoch 485/500, Loss: 2.6237\n",
            "Epoch 486/500, Loss: 2.6204\n",
            "Epoch 487/500, Loss: 2.6107\n",
            "Epoch 488/500, Loss: 2.6203\n",
            "Epoch 489/500, Loss: 2.6117\n",
            "Epoch 490/500, Loss: 2.6244\n",
            "Epoch 491/500, Loss: 2.6109\n",
            "Epoch 492/500, Loss: 2.5938\n",
            "Epoch 493/500, Loss: 2.6185\n",
            "Epoch 494/500, Loss: 2.5950\n",
            "Epoch 495/500, Loss: 2.6018\n",
            "Epoch 496/500, Loss: 2.6068\n",
            "Epoch 497/500, Loss: 2.6045\n",
            "Epoch 498/500, Loss: 2.6219\n",
            "Epoch 499/500, Loss: 2.6027\n",
            "Epoch 500/500, Loss: 2.6235\n",
            "{'0': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, '1': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}, 'weighted avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 10, attention_layers = 12, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 512, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 500, learning_rate = 1e-6)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ADgwlEjqZx6M",
        "outputId": "203ec762-7379-4f43-c11e-16df87d0effd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 2.7922\n",
            "Epoch 2/500, Loss: 2.7781\n",
            "Epoch 3/500, Loss: 2.7774\n",
            "Epoch 4/500, Loss: 2.7693\n",
            "Epoch 5/500, Loss: 2.7767\n",
            "Epoch 6/500, Loss: 2.7744\n",
            "Epoch 7/500, Loss: 2.7723\n",
            "Epoch 8/500, Loss: 2.7735\n",
            "Epoch 9/500, Loss: 2.7747\n",
            "Epoch 10/500, Loss: 2.7718\n",
            "Epoch 11/500, Loss: 2.7730\n",
            "Epoch 12/500, Loss: 2.7713\n",
            "Epoch 13/500, Loss: 2.7717\n",
            "Epoch 14/500, Loss: 2.7717\n",
            "Epoch 15/500, Loss: 2.7737\n",
            "Epoch 16/500, Loss: 2.7730\n",
            "Epoch 17/500, Loss: 2.7728\n",
            "Epoch 18/500, Loss: 2.7722\n",
            "Epoch 19/500, Loss: 2.7737\n",
            "Epoch 20/500, Loss: 2.7700\n",
            "Epoch 21/500, Loss: 2.7726\n",
            "Epoch 22/500, Loss: 2.7708\n",
            "Epoch 23/500, Loss: 2.7728\n",
            "Epoch 24/500, Loss: 2.7736\n",
            "Epoch 25/500, Loss: 2.7734\n",
            "Epoch 26/500, Loss: 2.7712\n",
            "Epoch 27/500, Loss: 2.7731\n",
            "Epoch 28/500, Loss: 2.7692\n",
            "Epoch 29/500, Loss: 2.7686\n",
            "Epoch 30/500, Loss: 2.7744\n",
            "Epoch 31/500, Loss: 2.7704\n",
            "Epoch 32/500, Loss: 2.7715\n",
            "Epoch 33/500, Loss: 2.7728\n",
            "Epoch 34/500, Loss: 2.7728\n",
            "Epoch 35/500, Loss: 2.7721\n",
            "Epoch 36/500, Loss: 2.7731\n",
            "Epoch 37/500, Loss: 2.7716\n",
            "Epoch 38/500, Loss: 2.7711\n",
            "Epoch 39/500, Loss: 2.7714\n",
            "Epoch 40/500, Loss: 2.7766\n",
            "Epoch 41/500, Loss: 2.7744\n",
            "Epoch 42/500, Loss: 2.7668\n",
            "Epoch 43/500, Loss: 2.7672\n",
            "Epoch 44/500, Loss: 2.7755\n",
            "Epoch 45/500, Loss: 2.7726\n",
            "Epoch 46/500, Loss: 2.7681\n",
            "Epoch 47/500, Loss: 2.7749\n",
            "Epoch 48/500, Loss: 2.7734\n",
            "Epoch 49/500, Loss: 2.7700\n",
            "Epoch 50/500, Loss: 2.7692\n",
            "Epoch 51/500, Loss: 2.7734\n",
            "Epoch 52/500, Loss: 2.7732\n",
            "Epoch 53/500, Loss: 2.7698\n",
            "Epoch 54/500, Loss: 2.7704\n",
            "Epoch 55/500, Loss: 2.7713\n",
            "Epoch 56/500, Loss: 2.7741\n",
            "Epoch 57/500, Loss: 2.7701\n",
            "Epoch 58/500, Loss: 2.7704\n",
            "Epoch 59/500, Loss: 2.7723\n",
            "Epoch 60/500, Loss: 2.7713\n",
            "Epoch 61/500, Loss: 2.7731\n",
            "Epoch 62/500, Loss: 2.7747\n",
            "Epoch 63/500, Loss: 2.7735\n",
            "Epoch 64/500, Loss: 2.7660\n",
            "Epoch 65/500, Loss: 2.7702\n",
            "Epoch 66/500, Loss: 2.7737\n",
            "Epoch 67/500, Loss: 2.7713\n",
            "Epoch 68/500, Loss: 2.7720\n",
            "Epoch 69/500, Loss: 2.7729\n",
            "Epoch 70/500, Loss: 2.7760\n",
            "Epoch 71/500, Loss: 2.7730\n",
            "Epoch 72/500, Loss: 2.7700\n",
            "Epoch 73/500, Loss: 2.7686\n",
            "Epoch 74/500, Loss: 2.7719\n",
            "Epoch 75/500, Loss: 2.7701\n",
            "Epoch 76/500, Loss: 2.7689\n",
            "Epoch 77/500, Loss: 2.7725\n",
            "Epoch 78/500, Loss: 2.7711\n",
            "Epoch 79/500, Loss: 2.7706\n",
            "Epoch 80/500, Loss: 2.7696\n",
            "Epoch 81/500, Loss: 2.7723\n",
            "Epoch 82/500, Loss: 2.7752\n",
            "Epoch 83/500, Loss: 2.7732\n",
            "Epoch 84/500, Loss: 2.7698\n",
            "Epoch 85/500, Loss: 2.7690\n",
            "Epoch 86/500, Loss: 2.7706\n",
            "Epoch 87/500, Loss: 2.7718\n",
            "Epoch 88/500, Loss: 2.7710\n",
            "Epoch 89/500, Loss: 2.7688\n",
            "Epoch 90/500, Loss: 2.7710\n",
            "Epoch 91/500, Loss: 2.7745\n",
            "Epoch 92/500, Loss: 2.7729\n",
            "Epoch 93/500, Loss: 2.7697\n",
            "Epoch 94/500, Loss: 2.7701\n",
            "Epoch 95/500, Loss: 2.7696\n",
            "Epoch 96/500, Loss: 2.7704\n",
            "Epoch 97/500, Loss: 2.7726\n",
            "Epoch 98/500, Loss: 2.7739\n",
            "Epoch 99/500, Loss: 2.7747\n",
            "Epoch 100/500, Loss: 2.7707\n",
            "Epoch 101/500, Loss: 2.7708\n",
            "Epoch 102/500, Loss: 2.7690\n",
            "Epoch 103/500, Loss: 2.7699\n",
            "Epoch 104/500, Loss: 2.7666\n",
            "Epoch 105/500, Loss: 2.7739\n",
            "Epoch 106/500, Loss: 2.7694\n",
            "Epoch 107/500, Loss: 2.7734\n",
            "Epoch 108/500, Loss: 2.7708\n",
            "Epoch 109/500, Loss: 2.7701\n",
            "Epoch 110/500, Loss: 2.7688\n",
            "Epoch 111/500, Loss: 2.7695\n",
            "Epoch 112/500, Loss: 2.7697\n",
            "Epoch 113/500, Loss: 2.7685\n",
            "Epoch 114/500, Loss: 2.7678\n",
            "Epoch 115/500, Loss: 2.7717\n",
            "Epoch 116/500, Loss: 2.7690\n",
            "Epoch 117/500, Loss: 2.7720\n",
            "Epoch 118/500, Loss: 2.7715\n",
            "Epoch 119/500, Loss: 2.7682\n",
            "Epoch 120/500, Loss: 2.7690\n",
            "Epoch 121/500, Loss: 2.7686\n",
            "Epoch 122/500, Loss: 2.7645\n",
            "Epoch 123/500, Loss: 2.7661\n",
            "Epoch 124/500, Loss: 2.7704\n",
            "Epoch 125/500, Loss: 2.7695\n",
            "Epoch 126/500, Loss: 2.7721\n",
            "Epoch 127/500, Loss: 2.7692\n",
            "Epoch 128/500, Loss: 2.7680\n",
            "Epoch 129/500, Loss: 2.7675\n",
            "Epoch 130/500, Loss: 2.7695\n",
            "Epoch 131/500, Loss: 2.7690\n",
            "Epoch 132/500, Loss: 2.7699\n",
            "Epoch 133/500, Loss: 2.7692\n",
            "Epoch 134/500, Loss: 2.7697\n",
            "Epoch 135/500, Loss: 2.7747\n",
            "Epoch 136/500, Loss: 2.7739\n",
            "Epoch 137/500, Loss: 2.7702\n",
            "Epoch 138/500, Loss: 2.7662\n",
            "Epoch 139/500, Loss: 2.7662\n",
            "Epoch 140/500, Loss: 2.7721\n",
            "Epoch 141/500, Loss: 2.7674\n",
            "Epoch 142/500, Loss: 2.7702\n",
            "Epoch 143/500, Loss: 2.7700\n",
            "Epoch 144/500, Loss: 2.7686\n",
            "Epoch 145/500, Loss: 2.7690\n",
            "Epoch 146/500, Loss: 2.7679\n",
            "Epoch 147/500, Loss: 2.7698\n",
            "Epoch 148/500, Loss: 2.7690\n",
            "Epoch 149/500, Loss: 2.7674\n",
            "Epoch 150/500, Loss: 2.7716\n",
            "Epoch 151/500, Loss: 2.7672\n",
            "Epoch 152/500, Loss: 2.7683\n",
            "Epoch 153/500, Loss: 2.7668\n",
            "Epoch 154/500, Loss: 2.7693\n",
            "Epoch 155/500, Loss: 2.7595\n",
            "Epoch 156/500, Loss: 2.7699\n",
            "Epoch 157/500, Loss: 2.7696\n",
            "Epoch 158/500, Loss: 2.7672\n",
            "Epoch 159/500, Loss: 2.7689\n",
            "Epoch 160/500, Loss: 2.7659\n",
            "Epoch 161/500, Loss: 2.7692\n",
            "Epoch 162/500, Loss: 2.7677\n",
            "Epoch 163/500, Loss: 2.7683\n",
            "Epoch 164/500, Loss: 2.7707\n",
            "Epoch 165/500, Loss: 2.7694\n",
            "Epoch 166/500, Loss: 2.7669\n",
            "Epoch 167/500, Loss: 2.7692\n",
            "Epoch 168/500, Loss: 2.7680\n",
            "Epoch 169/500, Loss: 2.7681\n",
            "Epoch 170/500, Loss: 2.7586\n",
            "Epoch 171/500, Loss: 2.7663\n",
            "Epoch 172/500, Loss: 2.7690\n",
            "Epoch 173/500, Loss: 2.7609\n",
            "Epoch 174/500, Loss: 2.7703\n",
            "Epoch 175/500, Loss: 2.7676\n",
            "Epoch 176/500, Loss: 2.7676\n",
            "Epoch 177/500, Loss: 2.7669\n",
            "Epoch 178/500, Loss: 2.7610\n",
            "Epoch 179/500, Loss: 2.7627\n",
            "Epoch 180/500, Loss: 2.7631\n",
            "Epoch 181/500, Loss: 2.7692\n",
            "Epoch 182/500, Loss: 2.7662\n",
            "Epoch 183/500, Loss: 2.7673\n",
            "Epoch 184/500, Loss: 2.7661\n",
            "Epoch 185/500, Loss: 2.7673\n",
            "Epoch 186/500, Loss: 2.7613\n",
            "Epoch 187/500, Loss: 2.7689\n",
            "Epoch 188/500, Loss: 2.7695\n",
            "Epoch 189/500, Loss: 2.7670\n",
            "Epoch 190/500, Loss: 2.7692\n",
            "Epoch 191/500, Loss: 2.7680\n",
            "Epoch 192/500, Loss: 2.7669\n",
            "Epoch 193/500, Loss: 2.7676\n",
            "Epoch 194/500, Loss: 2.7673\n",
            "Epoch 195/500, Loss: 2.7676\n",
            "Epoch 196/500, Loss: 2.7658\n",
            "Epoch 197/500, Loss: 2.7677\n",
            "Epoch 198/500, Loss: 2.7643\n",
            "Epoch 199/500, Loss: 2.7655\n",
            "Epoch 200/500, Loss: 2.7644\n",
            "Epoch 201/500, Loss: 2.7681\n",
            "Epoch 202/500, Loss: 2.7593\n",
            "Epoch 203/500, Loss: 2.7635\n",
            "Epoch 204/500, Loss: 2.7655\n",
            "Epoch 205/500, Loss: 2.7668\n",
            "Epoch 206/500, Loss: 2.7699\n",
            "Epoch 207/500, Loss: 2.7660\n",
            "Epoch 208/500, Loss: 2.7664\n",
            "Epoch 209/500, Loss: 2.7626\n",
            "Epoch 210/500, Loss: 2.7677\n",
            "Epoch 211/500, Loss: 2.7621\n",
            "Epoch 212/500, Loss: 2.7614\n",
            "Epoch 213/500, Loss: 2.7656\n",
            "Epoch 214/500, Loss: 2.7644\n",
            "Epoch 215/500, Loss: 2.7583\n",
            "Epoch 216/500, Loss: 2.7596\n",
            "Epoch 217/500, Loss: 2.7713\n",
            "Epoch 218/500, Loss: 2.7597\n",
            "Epoch 219/500, Loss: 2.7594\n",
            "Epoch 220/500, Loss: 2.7714\n",
            "Epoch 221/500, Loss: 2.7665\n",
            "Epoch 222/500, Loss: 2.7659\n",
            "Epoch 223/500, Loss: 2.7602\n",
            "Epoch 224/500, Loss: 2.7695\n",
            "Epoch 225/500, Loss: 2.7642\n",
            "Epoch 226/500, Loss: 2.7696\n",
            "Epoch 227/500, Loss: 2.7623\n",
            "Epoch 228/500, Loss: 2.7648\n",
            "Epoch 229/500, Loss: 2.7673\n",
            "Epoch 230/500, Loss: 2.7618\n",
            "Epoch 231/500, Loss: 2.7656\n",
            "Epoch 232/500, Loss: 2.7665\n",
            "Epoch 233/500, Loss: 2.7646\n",
            "Epoch 234/500, Loss: 2.7663\n",
            "Epoch 235/500, Loss: 2.7658\n",
            "Epoch 236/500, Loss: 2.7686\n",
            "Epoch 237/500, Loss: 2.7609\n",
            "Epoch 238/500, Loss: 2.7623\n",
            "Epoch 239/500, Loss: 2.7641\n",
            "Epoch 240/500, Loss: 2.7625\n",
            "Epoch 241/500, Loss: 2.7630\n",
            "Epoch 242/500, Loss: 2.7641\n",
            "Epoch 243/500, Loss: 2.7580\n",
            "Epoch 244/500, Loss: 2.7656\n",
            "Epoch 245/500, Loss: 2.7622\n",
            "Epoch 246/500, Loss: 2.7655\n",
            "Epoch 247/500, Loss: 2.7640\n",
            "Epoch 248/500, Loss: 2.7671\n",
            "Epoch 249/500, Loss: 2.7620\n",
            "Epoch 250/500, Loss: 2.7682\n",
            "Epoch 251/500, Loss: 2.7611\n",
            "Epoch 252/500, Loss: 2.7613\n",
            "Epoch 253/500, Loss: 2.7672\n",
            "Epoch 254/500, Loss: 2.7657\n",
            "Epoch 255/500, Loss: 2.7618\n",
            "Epoch 256/500, Loss: 2.7656\n",
            "Epoch 257/500, Loss: 2.7520\n",
            "Epoch 258/500, Loss: 2.7666\n",
            "Epoch 259/500, Loss: 2.7601\n",
            "Epoch 260/500, Loss: 2.7662\n",
            "Epoch 261/500, Loss: 2.7695\n",
            "Epoch 262/500, Loss: 2.7520\n",
            "Epoch 263/500, Loss: 2.7578\n",
            "Epoch 264/500, Loss: 2.7589\n",
            "Epoch 265/500, Loss: 2.7636\n",
            "Epoch 266/500, Loss: 2.7675\n",
            "Epoch 267/500, Loss: 2.7475\n",
            "Epoch 268/500, Loss: 2.7690\n",
            "Epoch 269/500, Loss: 2.7640\n",
            "Epoch 270/500, Loss: 2.7618\n",
            "Epoch 271/500, Loss: 2.7568\n",
            "Epoch 272/500, Loss: 2.7613\n",
            "Epoch 273/500, Loss: 2.7606\n",
            "Epoch 274/500, Loss: 2.7507\n",
            "Epoch 275/500, Loss: 2.7636\n",
            "Epoch 276/500, Loss: 2.7650\n",
            "Epoch 277/500, Loss: 2.7634\n",
            "Epoch 278/500, Loss: 2.7614\n",
            "Epoch 279/500, Loss: 2.7640\n",
            "Epoch 280/500, Loss: 2.7619\n",
            "Epoch 281/500, Loss: 2.7644\n",
            "Epoch 282/500, Loss: 2.7621\n",
            "Epoch 283/500, Loss: 2.7619\n",
            "Epoch 284/500, Loss: 2.7630\n",
            "Epoch 285/500, Loss: 2.7642\n",
            "Epoch 286/500, Loss: 2.7628\n",
            "Epoch 287/500, Loss: 2.7632\n",
            "Epoch 288/500, Loss: 2.7633\n",
            "Epoch 289/500, Loss: 2.7576\n",
            "Epoch 290/500, Loss: 2.7695\n",
            "Epoch 291/500, Loss: 2.7559\n",
            "Epoch 292/500, Loss: 2.7595\n",
            "Epoch 293/500, Loss: 2.7655\n",
            "Epoch 294/500, Loss: 2.7678\n",
            "Epoch 295/500, Loss: 2.7645\n",
            "Epoch 296/500, Loss: 2.7632\n",
            "Epoch 297/500, Loss: 2.7588\n",
            "Epoch 298/500, Loss: 2.7644\n",
            "Epoch 299/500, Loss: 2.7671\n",
            "Epoch 300/500, Loss: 2.7641\n",
            "Epoch 301/500, Loss: 2.7593\n",
            "Epoch 302/500, Loss: 2.7589\n",
            "Epoch 303/500, Loss: 2.7635\n",
            "Epoch 304/500, Loss: 2.7595\n",
            "Epoch 305/500, Loss: 2.7600\n",
            "Epoch 306/500, Loss: 2.7607\n",
            "Epoch 307/500, Loss: 2.7571\n",
            "Epoch 308/500, Loss: 2.7657\n",
            "Epoch 309/500, Loss: 2.7672\n",
            "Epoch 310/500, Loss: 2.7624\n",
            "Epoch 311/500, Loss: 2.7627\n",
            "Epoch 312/500, Loss: 2.7634\n",
            "Epoch 313/500, Loss: 2.7575\n",
            "Epoch 314/500, Loss: 2.7635\n",
            "Epoch 315/500, Loss: 2.7612\n",
            "Epoch 316/500, Loss: 2.7626\n",
            "Epoch 317/500, Loss: 2.7631\n",
            "Epoch 318/500, Loss: 2.7637\n",
            "Epoch 319/500, Loss: 2.7574\n",
            "Epoch 320/500, Loss: 2.7601\n",
            "Epoch 321/500, Loss: 2.7589\n",
            "Epoch 322/500, Loss: 2.7634\n",
            "Epoch 323/500, Loss: 2.7654\n",
            "Epoch 324/500, Loss: 2.7633\n",
            "Epoch 325/500, Loss: 2.7595\n",
            "Epoch 326/500, Loss: 2.7507\n",
            "Epoch 327/500, Loss: 2.7644\n",
            "Epoch 328/500, Loss: 2.7586\n",
            "Epoch 329/500, Loss: 2.7714\n",
            "Epoch 330/500, Loss: 2.7608\n",
            "Epoch 331/500, Loss: 2.7601\n",
            "Epoch 332/500, Loss: 2.7625\n",
            "Epoch 333/500, Loss: 2.7628\n",
            "Epoch 334/500, Loss: 2.7660\n",
            "Epoch 335/500, Loss: 2.7606\n",
            "Epoch 336/500, Loss: 2.7625\n",
            "Epoch 337/500, Loss: 2.7672\n",
            "Epoch 338/500, Loss: 2.7583\n",
            "Epoch 339/500, Loss: 2.7569\n",
            "Epoch 340/500, Loss: 2.7629\n",
            "Epoch 341/500, Loss: 2.7629\n",
            "Epoch 342/500, Loss: 2.7550\n",
            "Epoch 343/500, Loss: 2.7634\n",
            "Epoch 344/500, Loss: 2.7527\n",
            "Epoch 345/500, Loss: 2.7514\n",
            "Epoch 346/500, Loss: 2.7679\n",
            "Epoch 347/500, Loss: 2.7624\n",
            "Epoch 348/500, Loss: 2.7394\n",
            "Epoch 349/500, Loss: 2.7618\n",
            "Epoch 350/500, Loss: 2.7608\n",
            "Epoch 351/500, Loss: 2.7585\n",
            "Epoch 352/500, Loss: 2.7562\n",
            "Epoch 353/500, Loss: 2.7584\n",
            "Epoch 354/500, Loss: 2.7580\n",
            "Epoch 355/500, Loss: 2.7556\n",
            "Epoch 356/500, Loss: 2.7592\n",
            "Epoch 357/500, Loss: 2.7577\n",
            "Epoch 358/500, Loss: 2.7588\n",
            "Epoch 359/500, Loss: 2.7482\n",
            "Epoch 360/500, Loss: 2.7531\n",
            "Epoch 361/500, Loss: 2.7467\n",
            "Epoch 362/500, Loss: 2.7601\n",
            "Epoch 363/500, Loss: 2.7574\n",
            "Epoch 364/500, Loss: 2.7608\n",
            "Epoch 365/500, Loss: 2.7633\n",
            "Epoch 366/500, Loss: 2.7518\n",
            "Epoch 367/500, Loss: 2.7556\n",
            "Epoch 368/500, Loss: 2.7587\n",
            "Epoch 369/500, Loss: 2.7489\n",
            "Epoch 370/500, Loss: 2.7560\n",
            "Epoch 371/500, Loss: 2.7525\n",
            "Epoch 372/500, Loss: 2.7438\n",
            "Epoch 373/500, Loss: 2.7597\n",
            "Epoch 374/500, Loss: 2.7606\n",
            "Epoch 375/500, Loss: 2.7638\n",
            "Epoch 376/500, Loss: 2.7556\n",
            "Epoch 377/500, Loss: 2.7567\n",
            "Epoch 378/500, Loss: 2.7605\n",
            "Epoch 379/500, Loss: 2.7619\n",
            "Epoch 380/500, Loss: 2.7651\n",
            "Epoch 381/500, Loss: 2.7595\n",
            "Epoch 382/500, Loss: 2.7552\n",
            "Epoch 383/500, Loss: 2.7561\n",
            "Epoch 384/500, Loss: 2.7629\n",
            "Epoch 385/500, Loss: 2.7645\n",
            "Epoch 386/500, Loss: 2.7609\n",
            "Epoch 387/500, Loss: 2.7507\n",
            "Epoch 388/500, Loss: 2.7555\n",
            "Epoch 389/500, Loss: 2.7643\n",
            "Epoch 390/500, Loss: 2.7578\n",
            "Epoch 391/500, Loss: 2.7525\n",
            "Epoch 392/500, Loss: 2.7549\n",
            "Epoch 393/500, Loss: 2.7577\n",
            "Epoch 394/500, Loss: 2.7599\n",
            "Epoch 395/500, Loss: 2.7506\n",
            "Epoch 396/500, Loss: 2.7515\n",
            "Epoch 397/500, Loss: 2.7611\n",
            "Epoch 398/500, Loss: 2.7617\n",
            "Epoch 399/500, Loss: 2.7593\n",
            "Epoch 400/500, Loss: 2.7546\n",
            "Epoch 401/500, Loss: 2.7571\n",
            "Epoch 402/500, Loss: 2.7499\n",
            "Epoch 403/500, Loss: 2.7411\n",
            "Epoch 404/500, Loss: 2.7658\n",
            "Epoch 405/500, Loss: 2.7545\n",
            "Epoch 406/500, Loss: 2.7664\n",
            "Epoch 407/500, Loss: 2.7643\n",
            "Epoch 408/500, Loss: 2.7612\n",
            "Epoch 409/500, Loss: 2.7520\n",
            "Epoch 410/500, Loss: 2.7645\n",
            "Epoch 411/500, Loss: 2.7571\n",
            "Epoch 412/500, Loss: 2.7636\n",
            "Epoch 413/500, Loss: 2.7589\n",
            "Epoch 414/500, Loss: 2.7573\n",
            "Epoch 415/500, Loss: 2.7402\n",
            "Epoch 416/500, Loss: 2.7538\n",
            "Epoch 417/500, Loss: 2.7463\n",
            "Epoch 418/500, Loss: 2.7590\n",
            "Epoch 419/500, Loss: 2.7538\n",
            "Epoch 420/500, Loss: 2.7531\n",
            "Epoch 421/500, Loss: 2.7585\n",
            "Epoch 422/500, Loss: 2.7499\n",
            "Epoch 423/500, Loss: 2.7487\n",
            "Epoch 424/500, Loss: 2.7549\n",
            "Epoch 425/500, Loss: 2.7548\n",
            "Epoch 426/500, Loss: 2.7550\n",
            "Epoch 427/500, Loss: 2.7633\n",
            "Epoch 428/500, Loss: 2.7590\n",
            "Epoch 429/500, Loss: 2.7526\n",
            "Epoch 430/500, Loss: 2.7478\n",
            "Epoch 431/500, Loss: 2.7553\n",
            "Epoch 432/500, Loss: 2.7453\n",
            "Epoch 433/500, Loss: 2.7593\n",
            "Epoch 434/500, Loss: 2.7634\n",
            "Epoch 435/500, Loss: 2.7525\n",
            "Epoch 436/500, Loss: 2.7601\n",
            "Epoch 437/500, Loss: 2.7432\n",
            "Epoch 438/500, Loss: 2.7590\n",
            "Epoch 439/500, Loss: 2.7447\n",
            "Epoch 440/500, Loss: 2.7565\n",
            "Epoch 441/500, Loss: 2.7527\n",
            "Epoch 442/500, Loss: 2.7412\n",
            "Epoch 443/500, Loss: 2.7505\n",
            "Epoch 444/500, Loss: 2.7521\n",
            "Epoch 445/500, Loss: 2.7513\n",
            "Epoch 446/500, Loss: 2.7541\n",
            "Epoch 447/500, Loss: 2.7452\n",
            "Epoch 448/500, Loss: 2.7540\n",
            "Epoch 449/500, Loss: 2.7651\n",
            "Epoch 450/500, Loss: 2.7476\n",
            "Epoch 451/500, Loss: 2.7516\n",
            "Epoch 452/500, Loss: 2.7478\n",
            "Epoch 453/500, Loss: 2.7459\n",
            "Epoch 454/500, Loss: 2.7552\n",
            "Epoch 455/500, Loss: 2.7497\n",
            "Epoch 456/500, Loss: 2.7510\n",
            "Epoch 457/500, Loss: 2.7547\n",
            "Epoch 458/500, Loss: 2.7526\n",
            "Epoch 459/500, Loss: 2.7488\n",
            "Epoch 460/500, Loss: 2.7499\n",
            "Epoch 461/500, Loss: 2.7432\n",
            "Epoch 462/500, Loss: 2.7544\n",
            "Epoch 463/500, Loss: 2.7480\n",
            "Epoch 464/500, Loss: 2.7453\n",
            "Epoch 465/500, Loss: 2.7435\n",
            "Epoch 466/500, Loss: 2.7596\n",
            "Epoch 467/500, Loss: 2.7532\n",
            "Epoch 468/500, Loss: 2.7564\n",
            "Epoch 469/500, Loss: 2.7554\n",
            "Epoch 470/500, Loss: 2.7488\n",
            "Epoch 471/500, Loss: 2.7494\n",
            "Epoch 472/500, Loss: 2.7520\n",
            "Epoch 473/500, Loss: 2.7532\n",
            "Epoch 474/500, Loss: 2.7539\n",
            "Epoch 475/500, Loss: 2.7496\n",
            "Epoch 476/500, Loss: 2.7417\n",
            "Epoch 477/500, Loss: 2.7500\n",
            "Epoch 478/500, Loss: 2.7537\n",
            "Epoch 479/500, Loss: 2.7506\n",
            "Epoch 480/500, Loss: 2.7507\n",
            "Epoch 481/500, Loss: 2.7522\n",
            "Epoch 482/500, Loss: 2.7539\n",
            "Epoch 483/500, Loss: 2.7503\n",
            "Epoch 484/500, Loss: 2.7432\n",
            "Epoch 485/500, Loss: 2.7530\n",
            "Epoch 486/500, Loss: 2.7423\n",
            "Epoch 487/500, Loss: 2.7454\n",
            "Epoch 488/500, Loss: 2.7431\n",
            "Epoch 489/500, Loss: 2.7446\n",
            "Epoch 490/500, Loss: 2.7486\n",
            "Epoch 491/500, Loss: 2.7521\n",
            "Epoch 492/500, Loss: 2.7378\n",
            "Epoch 493/500, Loss: 2.7555\n",
            "Epoch 494/500, Loss: 2.7426\n",
            "Epoch 495/500, Loss: 2.7477\n",
            "Epoch 496/500, Loss: 2.7481\n",
            "Epoch 497/500, Loss: 2.7492\n",
            "Epoch 498/500, Loss: 2.7560\n",
            "Epoch 499/500, Loss: 2.7456\n",
            "Epoch 500/500, Loss: 2.7550\n",
            "{'0': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, '1': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 7.0}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}, 'weighted avg': {'precision': 0.7142857142857143, 'recall': 0.7142857142857143, 'f1-score': 0.7142857142857143, 'support': 14.0}}\n"
          ]
        }
      ],
      "source": [
        "custom_model = Transformer(k_mers_type = 10, attention_layers = 16, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 512, dim_feedforward = 2048, seed = global_seed)\n",
        "custom_model.train(epochs = 500, learning_rate = 1e-6)\n",
        "model_score = custom_model.evaluate()\n",
        "if (model_score > best_model_score):\n",
        "    best_model = custom_model\n",
        "    best_model_score = model_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "20QsTCq1Zx6M",
        "outputId": "03c5e1ad-1c6e-486d-940c-325095991ef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model score in Validation:  0.8571428571428571\n",
            "Best model score in Test: \n",
            "\n",
            "Labels:      [1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
            "Predictions: [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.57      0.73         7\n",
            "           1       0.70      1.00      0.82         7\n",
            "\n",
            "    accuracy                           0.79        14\n",
            "   macro avg       0.85      0.79      0.78        14\n",
            "weighted avg       0.85      0.79      0.78        14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Best model score in Validation: \", best_model_score)\n",
        "print(\"Best model score in Test: \\n\")\n",
        "best_model.test()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}