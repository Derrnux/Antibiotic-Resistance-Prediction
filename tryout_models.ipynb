{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Out and Compare Models based on predefined Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup Overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_manager import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training and Comparison of Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:      [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.80      1.00      0.89        12\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.40      0.50      0.44        15\n",
      "weighted avg       0.64      0.80      0.71        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 1, attention_layers = 3, batch_size = 32,  balancing_method = 'none', embed_dim = 128, dim_feedforward = 2048)\n",
    "custom_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 3.5630\n",
      "Epoch 2/200, Loss: 3.5135\n",
      "Epoch 3/200, Loss: 3.3918\n",
      "Epoch 4/200, Loss: 3.3958\n",
      "Epoch 5/200, Loss: 3.6548\n",
      "Epoch 6/200, Loss: 3.4099\n",
      "Epoch 7/200, Loss: 3.4073\n",
      "Epoch 8/200, Loss: 3.3015\n",
      "Epoch 9/200, Loss: 3.3918\n",
      "Epoch 10/200, Loss: 3.3309\n",
      "Epoch 11/200, Loss: 3.3724\n",
      "Epoch 12/200, Loss: 3.2595\n",
      "Epoch 13/200, Loss: 3.3288\n",
      "Epoch 14/200, Loss: 3.2759\n",
      "Epoch 15/200, Loss: 3.3171\n",
      "Epoch 16/200, Loss: 3.3259\n",
      "Epoch 17/200, Loss: 3.2013\n",
      "Epoch 18/200, Loss: 3.4275\n",
      "Epoch 19/200, Loss: 3.4637\n",
      "Epoch 20/200, Loss: 3.5258\n",
      "Epoch 21/200, Loss: 3.3518\n",
      "Epoch 22/200, Loss: 3.3772\n",
      "Epoch 23/200, Loss: 3.2067\n",
      "Epoch 24/200, Loss: 3.4361\n",
      "Epoch 25/200, Loss: 3.4865\n",
      "Epoch 26/200, Loss: 3.3078\n",
      "Epoch 27/200, Loss: 3.3419\n",
      "Epoch 28/200, Loss: 3.4024\n",
      "Epoch 29/200, Loss: 3.2317\n",
      "Epoch 30/200, Loss: 3.3791\n",
      "Epoch 31/200, Loss: 3.4171\n",
      "Epoch 32/200, Loss: 3.4222\n",
      "Epoch 33/200, Loss: 3.2943\n",
      "Epoch 34/200, Loss: 3.3123\n",
      "Epoch 35/200, Loss: 3.3886\n",
      "Epoch 36/200, Loss: 3.5707\n",
      "Epoch 37/200, Loss: 3.3230\n",
      "Epoch 38/200, Loss: 3.3271\n",
      "Epoch 39/200, Loss: 3.4619\n",
      "Epoch 40/200, Loss: 3.4717\n",
      "Epoch 41/200, Loss: 3.2371\n",
      "Epoch 42/200, Loss: 3.3663\n",
      "Epoch 43/200, Loss: 3.2717\n",
      "Epoch 44/200, Loss: 3.3247\n",
      "Epoch 45/200, Loss: 3.3666\n",
      "Epoch 46/200, Loss: 3.2432\n",
      "Epoch 47/200, Loss: 3.3424\n",
      "Epoch 48/200, Loss: 3.4485\n",
      "Epoch 49/200, Loss: 3.3473\n",
      "Epoch 50/200, Loss: 3.4259\n",
      "Epoch 51/200, Loss: 3.5171\n",
      "Epoch 52/200, Loss: 3.3323\n",
      "Epoch 53/200, Loss: 3.4303\n",
      "Epoch 54/200, Loss: 3.4905\n",
      "Epoch 55/200, Loss: 3.4451\n",
      "Epoch 56/200, Loss: 3.3748\n",
      "Epoch 57/200, Loss: 3.3784\n",
      "Epoch 58/200, Loss: 3.3285\n",
      "Epoch 59/200, Loss: 3.2770\n",
      "Epoch 60/200, Loss: 3.4016\n",
      "Epoch 61/200, Loss: 3.2707\n",
      "Epoch 62/200, Loss: 3.2465\n",
      "Epoch 63/200, Loss: 3.3909\n",
      "Epoch 64/200, Loss: 3.2192\n",
      "Epoch 65/200, Loss: 3.2624\n",
      "Epoch 66/200, Loss: 3.2193\n",
      "Epoch 67/200, Loss: 3.2748\n",
      "Epoch 68/200, Loss: 3.2241\n",
      "Epoch 69/200, Loss: 3.1709\n",
      "Epoch 70/200, Loss: 3.3015\n",
      "Epoch 71/200, Loss: 3.3728\n",
      "Epoch 72/200, Loss: 3.4774\n",
      "Epoch 73/200, Loss: 3.3300\n",
      "Epoch 74/200, Loss: 3.3576\n",
      "Epoch 75/200, Loss: 3.2730\n",
      "Epoch 76/200, Loss: 3.3897\n",
      "Epoch 77/200, Loss: 3.3873\n",
      "Epoch 78/200, Loss: 3.2326\n",
      "Epoch 79/200, Loss: 3.3027\n",
      "Epoch 80/200, Loss: 3.3728\n",
      "Epoch 81/200, Loss: 3.3186\n",
      "Epoch 82/200, Loss: 3.3254\n",
      "Epoch 83/200, Loss: 3.3318\n",
      "Epoch 84/200, Loss: 3.2164\n",
      "Epoch 85/200, Loss: 3.3714\n",
      "Epoch 86/200, Loss: 3.3180\n",
      "Epoch 87/200, Loss: 3.3505\n",
      "Epoch 88/200, Loss: 3.3107\n",
      "Epoch 89/200, Loss: 3.3444\n",
      "Epoch 90/200, Loss: 3.4775\n",
      "Epoch 91/200, Loss: 3.2724\n",
      "Epoch 92/200, Loss: 3.2368\n",
      "Epoch 93/200, Loss: 3.3474\n",
      "Epoch 94/200, Loss: 3.4715\n",
      "Epoch 95/200, Loss: 3.3494\n",
      "Epoch 96/200, Loss: 3.3042\n",
      "Epoch 97/200, Loss: 3.2299\n",
      "Epoch 98/200, Loss: 3.2373\n",
      "Epoch 99/200, Loss: 3.4230\n",
      "Epoch 100/200, Loss: 3.2781\n",
      "Epoch 101/200, Loss: 3.3721\n",
      "Epoch 102/200, Loss: 3.4319\n",
      "Epoch 103/200, Loss: 3.2887\n",
      "Epoch 104/200, Loss: 3.2778\n",
      "Epoch 105/200, Loss: 3.2454\n",
      "Epoch 106/200, Loss: 3.3894\n",
      "Epoch 107/200, Loss: 3.3089\n",
      "Epoch 108/200, Loss: 3.2727\n",
      "Epoch 109/200, Loss: 3.2737\n",
      "Epoch 110/200, Loss: 3.2770\n",
      "Epoch 111/200, Loss: 3.1727\n",
      "Epoch 112/200, Loss: 3.3111\n",
      "Epoch 113/200, Loss: 3.3658\n",
      "Epoch 114/200, Loss: 3.3793\n",
      "Epoch 115/200, Loss: 3.2882\n",
      "Epoch 116/200, Loss: 3.3494\n",
      "Epoch 117/200, Loss: 3.2370\n",
      "Epoch 118/200, Loss: 3.4065\n",
      "Epoch 119/200, Loss: 3.4844\n",
      "Epoch 120/200, Loss: 3.2306\n",
      "Epoch 121/200, Loss: 3.1387\n",
      "Epoch 122/200, Loss: 3.4126\n",
      "Epoch 123/200, Loss: 3.3407\n",
      "Epoch 124/200, Loss: 3.2615\n",
      "Epoch 125/200, Loss: 3.3592\n",
      "Epoch 126/200, Loss: 3.1416\n",
      "Epoch 127/200, Loss: 3.2484\n",
      "Epoch 128/200, Loss: 3.2840\n",
      "Epoch 129/200, Loss: 3.3352\n",
      "Epoch 130/200, Loss: 3.3279\n",
      "Epoch 131/200, Loss: 3.3627\n",
      "Epoch 132/200, Loss: 3.5165\n",
      "Epoch 133/200, Loss: 3.2542\n",
      "Epoch 134/200, Loss: 3.2782\n",
      "Epoch 135/200, Loss: 3.3273\n",
      "Epoch 136/200, Loss: 3.3515\n",
      "Epoch 137/200, Loss: 3.3574\n",
      "Epoch 138/200, Loss: 3.3641\n",
      "Epoch 139/200, Loss: 3.3484\n",
      "Epoch 140/200, Loss: 3.3239\n",
      "Epoch 141/200, Loss: 3.3369\n",
      "Epoch 142/200, Loss: 3.3341\n",
      "Epoch 143/200, Loss: 3.4683\n",
      "Epoch 144/200, Loss: 3.3798\n",
      "Epoch 145/200, Loss: 3.3337\n",
      "Epoch 146/200, Loss: 3.3774\n",
      "Epoch 147/200, Loss: 3.2957\n",
      "Epoch 148/200, Loss: 3.4425\n",
      "Epoch 149/200, Loss: 3.3267\n",
      "Epoch 150/200, Loss: 3.3021\n",
      "Epoch 151/200, Loss: 3.3167\n",
      "Epoch 152/200, Loss: 3.2382\n",
      "Epoch 153/200, Loss: 3.3146\n",
      "Epoch 154/200, Loss: 3.3884\n",
      "Epoch 155/200, Loss: 3.2920\n",
      "Epoch 156/200, Loss: 3.3739\n",
      "Epoch 157/200, Loss: 3.3649\n",
      "Epoch 158/200, Loss: 3.2943\n",
      "Epoch 159/200, Loss: 3.3093\n",
      "Epoch 160/200, Loss: 3.4387\n",
      "Epoch 161/200, Loss: 3.3360\n",
      "Epoch 162/200, Loss: 3.3744\n",
      "Epoch 163/200, Loss: 3.3145\n",
      "Epoch 164/200, Loss: 3.4066\n",
      "Epoch 165/200, Loss: 3.2966\n",
      "Epoch 166/200, Loss: 3.3736\n",
      "Epoch 167/200, Loss: 3.5004\n",
      "Epoch 168/200, Loss: 3.4206\n",
      "Epoch 169/200, Loss: 3.2403\n",
      "Epoch 170/200, Loss: 3.2494\n",
      "Epoch 171/200, Loss: 3.4095\n",
      "Epoch 172/200, Loss: 3.2968\n",
      "Epoch 173/200, Loss: 3.3834\n",
      "Epoch 174/200, Loss: 3.4599\n",
      "Epoch 175/200, Loss: 3.3941\n",
      "Epoch 176/200, Loss: 3.2458\n",
      "Epoch 177/200, Loss: 3.3563\n",
      "Epoch 178/200, Loss: 3.3989\n",
      "Epoch 179/200, Loss: 3.3347\n",
      "Epoch 180/200, Loss: 3.2905\n",
      "Epoch 181/200, Loss: 3.3466\n",
      "Epoch 182/200, Loss: 3.2815\n",
      "Epoch 183/200, Loss: 3.4446\n",
      "Epoch 184/200, Loss: 3.3478\n",
      "Epoch 185/200, Loss: 3.4187\n",
      "Epoch 186/200, Loss: 3.2472\n",
      "Epoch 187/200, Loss: 3.1868\n",
      "Epoch 188/200, Loss: 3.4064\n",
      "Epoch 189/200, Loss: 3.3705\n",
      "Epoch 190/200, Loss: 3.3103\n",
      "Epoch 191/200, Loss: 3.2784\n",
      "Epoch 192/200, Loss: 3.3189\n",
      "Epoch 193/200, Loss: 3.3501\n",
      "Epoch 194/200, Loss: 3.2635\n",
      "Epoch 195/200, Loss: 3.2538\n",
      "Epoch 196/200, Loss: 3.4607\n",
      "Epoch 197/200, Loss: 3.3136\n",
      "Epoch 198/200, Loss: 3.3006\n",
      "Epoch 199/200, Loss: 3.2474\n",
      "Epoch 200/200, Loss: 3.1029\n",
      "Labels:      [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Predictions: [0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60         3\n",
      "           1       1.00      0.67      0.80        12\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.71      0.83      0.70        15\n",
      "weighted avg       0.89      0.73      0.76        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 1, attention_layers = 3, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 128, dim_feedforward = 2048)\n",
    "custom_model.train(epochs = 200, learning_rate = 1e-4)\n",
    "custom_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 3.8213\n",
      "Epoch 2/200, Loss: 3.5032\n",
      "Epoch 3/200, Loss: 3.6014\n",
      "Epoch 4/200, Loss: 3.5356\n",
      "Epoch 5/200, Loss: 3.3738\n",
      "Epoch 6/200, Loss: 3.4324\n",
      "Epoch 7/200, Loss: 3.3749\n",
      "Epoch 8/200, Loss: 3.6145\n",
      "Epoch 9/200, Loss: 3.3466\n",
      "Epoch 10/200, Loss: 3.3891\n",
      "Epoch 11/200, Loss: 3.4107\n",
      "Epoch 12/200, Loss: 3.3023\n",
      "Epoch 13/200, Loss: 3.3290\n",
      "Epoch 14/200, Loss: 3.3729\n",
      "Epoch 15/200, Loss: 3.2864\n",
      "Epoch 16/200, Loss: 3.4778\n",
      "Epoch 17/200, Loss: 3.4787\n",
      "Epoch 18/200, Loss: 3.2914\n",
      "Epoch 19/200, Loss: 3.2321\n",
      "Epoch 20/200, Loss: 3.2649\n",
      "Epoch 21/200, Loss: 3.2761\n",
      "Epoch 22/200, Loss: 3.3250\n",
      "Epoch 23/200, Loss: 3.3778\n",
      "Epoch 24/200, Loss: 3.2743\n",
      "Epoch 25/200, Loss: 3.2145\n",
      "Epoch 26/200, Loss: 3.4594\n",
      "Epoch 27/200, Loss: 3.2892\n",
      "Epoch 28/200, Loss: 3.2926\n",
      "Epoch 29/200, Loss: 3.3259\n",
      "Epoch 30/200, Loss: 3.1822\n",
      "Epoch 31/200, Loss: 3.3845\n",
      "Epoch 32/200, Loss: 3.4410\n",
      "Epoch 33/200, Loss: 3.2675\n",
      "Epoch 34/200, Loss: 3.2479\n",
      "Epoch 35/200, Loss: 3.3310\n",
      "Epoch 36/200, Loss: 3.1966\n",
      "Epoch 37/200, Loss: 3.3066\n",
      "Epoch 38/200, Loss: 3.2246\n",
      "Epoch 39/200, Loss: 3.1547\n",
      "Epoch 40/200, Loss: 3.5687\n",
      "Epoch 41/200, Loss: 3.3247\n",
      "Epoch 42/200, Loss: 3.3607\n",
      "Epoch 43/200, Loss: 3.1570\n",
      "Epoch 44/200, Loss: 3.4110\n",
      "Epoch 45/200, Loss: 3.1944\n",
      "Epoch 46/200, Loss: 3.1917\n",
      "Epoch 47/200, Loss: 3.1159\n",
      "Epoch 48/200, Loss: 3.2524\n",
      "Epoch 49/200, Loss: 3.0510\n",
      "Epoch 50/200, Loss: 3.1879\n",
      "Epoch 51/200, Loss: 3.4106\n",
      "Epoch 52/200, Loss: 3.1972\n",
      "Epoch 53/200, Loss: 3.1683\n",
      "Epoch 54/200, Loss: 3.1687\n",
      "Epoch 55/200, Loss: 3.2014\n",
      "Epoch 56/200, Loss: 2.9293\n",
      "Epoch 57/200, Loss: 3.1248\n",
      "Epoch 58/200, Loss: 2.9673\n",
      "Epoch 59/200, Loss: 3.3333\n",
      "Epoch 60/200, Loss: 3.1698\n",
      "Epoch 61/200, Loss: 3.1203\n",
      "Epoch 62/200, Loss: 3.0807\n",
      "Epoch 63/200, Loss: 3.1145\n",
      "Epoch 64/200, Loss: 3.0692\n",
      "Epoch 65/200, Loss: 3.0638\n",
      "Epoch 66/200, Loss: 2.8695\n",
      "Epoch 67/200, Loss: 3.2970\n",
      "Epoch 68/200, Loss: 3.1569\n",
      "Epoch 69/200, Loss: 2.9728\n",
      "Epoch 70/200, Loss: 2.8905\n",
      "Epoch 71/200, Loss: 2.9835\n",
      "Epoch 72/200, Loss: 3.2269\n",
      "Epoch 73/200, Loss: 3.0363\n",
      "Epoch 74/200, Loss: 2.8691\n",
      "Epoch 75/200, Loss: 2.8517\n",
      "Epoch 76/200, Loss: 2.8964\n",
      "Epoch 77/200, Loss: 2.9524\n",
      "Epoch 78/200, Loss: 2.9481\n",
      "Epoch 79/200, Loss: 3.1315\n",
      "Epoch 80/200, Loss: 2.9626\n",
      "Epoch 81/200, Loss: 2.9230\n",
      "Epoch 82/200, Loss: 2.8554\n",
      "Epoch 83/200, Loss: 2.8633\n",
      "Epoch 84/200, Loss: 3.0077\n",
      "Epoch 85/200, Loss: 2.9456\n",
      "Epoch 86/200, Loss: 2.9232\n",
      "Epoch 87/200, Loss: 2.9776\n",
      "Epoch 88/200, Loss: 3.0819\n",
      "Epoch 89/200, Loss: 2.8751\n",
      "Epoch 90/200, Loss: 2.8913\n",
      "Epoch 91/200, Loss: 2.8801\n",
      "Epoch 92/200, Loss: 3.1638\n",
      "Epoch 93/200, Loss: 3.0193\n",
      "Epoch 94/200, Loss: 2.8048\n",
      "Epoch 95/200, Loss: 2.9355\n",
      "Epoch 96/200, Loss: 2.8697\n",
      "Epoch 97/200, Loss: 2.7933\n",
      "Epoch 98/200, Loss: 2.9040\n",
      "Epoch 99/200, Loss: 2.9533\n",
      "Epoch 100/200, Loss: 3.0310\n",
      "Epoch 101/200, Loss: 2.9282\n",
      "Epoch 102/200, Loss: 2.7324\n",
      "Epoch 103/200, Loss: 2.6587\n",
      "Epoch 104/200, Loss: 2.9060\n",
      "Epoch 105/200, Loss: 2.8189\n",
      "Epoch 106/200, Loss: 2.9907\n",
      "Epoch 107/200, Loss: 2.7221\n",
      "Epoch 108/200, Loss: 2.8339\n",
      "Epoch 109/200, Loss: 2.7693\n",
      "Epoch 110/200, Loss: 3.0367\n",
      "Epoch 111/200, Loss: 2.7456\n",
      "Epoch 112/200, Loss: 2.8006\n",
      "Epoch 113/200, Loss: 2.7888\n",
      "Epoch 114/200, Loss: 2.7905\n",
      "Epoch 115/200, Loss: 2.7106\n",
      "Epoch 116/200, Loss: 2.8032\n",
      "Epoch 117/200, Loss: 2.7033\n",
      "Epoch 118/200, Loss: 2.9212\n",
      "Epoch 119/200, Loss: 2.7830\n",
      "Epoch 120/200, Loss: 2.9003\n",
      "Epoch 121/200, Loss: 2.7122\n",
      "Epoch 122/200, Loss: 2.6784\n",
      "Epoch 123/200, Loss: 2.8406\n",
      "Epoch 124/200, Loss: 2.6720\n",
      "Epoch 125/200, Loss: 2.6559\n",
      "Epoch 126/200, Loss: 2.9385\n",
      "Epoch 127/200, Loss: 2.8630\n",
      "Epoch 128/200, Loss: 2.6827\n",
      "Epoch 129/200, Loss: 2.7262\n",
      "Epoch 130/200, Loss: 3.3064\n",
      "Epoch 131/200, Loss: 2.6787\n",
      "Epoch 132/200, Loss: 2.7222\n",
      "Epoch 133/200, Loss: 2.6907\n",
      "Epoch 134/200, Loss: 2.9098\n",
      "Epoch 135/200, Loss: 2.8001\n",
      "Epoch 136/200, Loss: 2.6981\n",
      "Epoch 137/200, Loss: 2.8253\n",
      "Epoch 138/200, Loss: 2.8172\n",
      "Epoch 139/200, Loss: 2.6990\n",
      "Epoch 140/200, Loss: 2.6657\n",
      "Epoch 141/200, Loss: 2.7361\n",
      "Epoch 142/200, Loss: 2.6248\n",
      "Epoch 143/200, Loss: 2.5560\n",
      "Epoch 144/200, Loss: 2.8119\n",
      "Epoch 145/200, Loss: 2.7361\n",
      "Epoch 146/200, Loss: 2.5421\n",
      "Epoch 147/200, Loss: 2.9111\n",
      "Epoch 148/200, Loss: 2.7389\n",
      "Epoch 149/200, Loss: 2.6050\n",
      "Epoch 150/200, Loss: 2.7095\n",
      "Epoch 151/200, Loss: 2.8283\n",
      "Epoch 152/200, Loss: 2.7021\n",
      "Epoch 153/200, Loss: 2.5922\n",
      "Epoch 154/200, Loss: 2.4689\n",
      "Epoch 155/200, Loss: 2.5293\n",
      "Epoch 156/200, Loss: 2.5056\n",
      "Epoch 157/200, Loss: 2.6323\n",
      "Epoch 158/200, Loss: 2.5687\n",
      "Epoch 159/200, Loss: 2.6475\n",
      "Epoch 160/200, Loss: 2.8006\n",
      "Epoch 161/200, Loss: 2.6724\n",
      "Epoch 162/200, Loss: 2.6623\n",
      "Epoch 163/200, Loss: 2.6386\n",
      "Epoch 164/200, Loss: 2.6741\n",
      "Epoch 165/200, Loss: 2.5608\n",
      "Epoch 166/200, Loss: 2.5705\n",
      "Epoch 167/200, Loss: 2.7776\n",
      "Epoch 168/200, Loss: 2.5551\n",
      "Epoch 169/200, Loss: 2.6659\n",
      "Epoch 170/200, Loss: 2.6145\n",
      "Epoch 171/200, Loss: 2.5808\n",
      "Epoch 172/200, Loss: 2.7357\n",
      "Epoch 173/200, Loss: 2.6044\n",
      "Epoch 174/200, Loss: 3.1734\n",
      "Epoch 175/200, Loss: 2.5772\n",
      "Epoch 176/200, Loss: 2.4080\n",
      "Epoch 177/200, Loss: 2.7886\n",
      "Epoch 178/200, Loss: 2.5757\n",
      "Epoch 179/200, Loss: 2.7539\n",
      "Epoch 180/200, Loss: 2.6366\n",
      "Epoch 181/200, Loss: 2.7348\n",
      "Epoch 182/200, Loss: 2.5105\n",
      "Epoch 183/200, Loss: 2.7307\n",
      "Epoch 184/200, Loss: 2.7448\n",
      "Epoch 185/200, Loss: 2.8223\n",
      "Epoch 186/200, Loss: 2.6038\n",
      "Epoch 187/200, Loss: 2.5676\n",
      "Epoch 188/200, Loss: 2.4910\n",
      "Epoch 189/200, Loss: 2.5717\n",
      "Epoch 190/200, Loss: 2.5447\n",
      "Epoch 191/200, Loss: 2.8707\n",
      "Epoch 192/200, Loss: 2.5181\n",
      "Epoch 193/200, Loss: 2.5786\n",
      "Epoch 194/200, Loss: 2.5172\n",
      "Epoch 195/200, Loss: 2.4703\n",
      "Epoch 196/200, Loss: 2.4267\n",
      "Epoch 197/200, Loss: 2.5776\n",
      "Epoch 198/200, Loss: 2.4627\n",
      "Epoch 199/200, Loss: 2.5262\n",
      "Epoch 200/200, Loss: 2.4992\n",
      "Labels:      [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Predictions: [1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       1.00      0.83      0.91        12\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.80      0.92      0.83        15\n",
      "weighted avg       0.92      0.87      0.88        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 3, attention_layers = 4, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048)\n",
    "custom_model.train(epochs = 200, learning_rate = 1e-4)\n",
    "custom_model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 3.6883\n",
      "Epoch 2/200, Loss: 3.4492\n",
      "Epoch 3/200, Loss: 3.5966\n",
      "Epoch 4/200, Loss: 3.4078\n",
      "Epoch 5/200, Loss: 3.4731\n",
      "Epoch 6/200, Loss: 3.4384\n",
      "Epoch 7/200, Loss: 3.4301\n",
      "Epoch 8/200, Loss: 3.4785\n",
      "Epoch 9/200, Loss: 3.3286\n",
      "Epoch 10/200, Loss: 3.3018\n",
      "Epoch 11/200, Loss: 3.5200\n",
      "Epoch 12/200, Loss: 3.4336\n",
      "Epoch 13/200, Loss: 3.3465\n",
      "Epoch 14/200, Loss: 3.2969\n",
      "Epoch 15/200, Loss: 3.2729\n",
      "Epoch 16/200, Loss: 3.3041\n",
      "Epoch 17/200, Loss: 3.1343\n",
      "Epoch 18/200, Loss: 3.3020\n",
      "Epoch 19/200, Loss: 3.2092\n",
      "Epoch 20/200, Loss: 3.1174\n",
      "Epoch 21/200, Loss: 3.2499\n",
      "Epoch 22/200, Loss: 3.4127\n",
      "Epoch 23/200, Loss: 3.2259\n",
      "Epoch 24/200, Loss: 3.2397\n",
      "Epoch 25/200, Loss: 3.3253\n",
      "Epoch 26/200, Loss: 3.0959\n",
      "Epoch 27/200, Loss: 3.1340\n",
      "Epoch 28/200, Loss: 3.0587\n",
      "Epoch 29/200, Loss: 2.9718\n",
      "Epoch 30/200, Loss: 3.1000\n",
      "Epoch 31/200, Loss: 2.8612\n",
      "Epoch 32/200, Loss: 2.7610\n",
      "Epoch 33/200, Loss: 2.9166\n",
      "Epoch 34/200, Loss: 2.8281\n",
      "Epoch 35/200, Loss: 2.8047\n",
      "Epoch 36/200, Loss: 2.7453\n",
      "Epoch 37/200, Loss: 2.4402\n",
      "Epoch 38/200, Loss: 2.6246\n",
      "Epoch 39/200, Loss: 2.4353\n",
      "Epoch 40/200, Loss: 2.4689\n",
      "Epoch 41/200, Loss: 2.4225\n",
      "Epoch 42/200, Loss: 2.5554\n",
      "Epoch 43/200, Loss: 2.5458\n",
      "Epoch 44/200, Loss: 2.3852\n",
      "Epoch 45/200, Loss: 2.4217\n",
      "Epoch 46/200, Loss: 2.3131\n",
      "Epoch 47/200, Loss: 2.5674\n",
      "Epoch 48/200, Loss: 2.2835\n",
      "Epoch 49/200, Loss: 2.2413\n",
      "Epoch 50/200, Loss: 2.3597\n",
      "Epoch 51/200, Loss: 2.3888\n",
      "Epoch 52/200, Loss: 2.5154\n",
      "Epoch 53/200, Loss: 2.1751\n",
      "Epoch 54/200, Loss: 2.2533\n",
      "Epoch 55/200, Loss: 2.1352\n",
      "Epoch 56/200, Loss: 2.1450\n",
      "Epoch 57/200, Loss: 2.2178\n",
      "Epoch 58/200, Loss: 2.2272\n",
      "Epoch 59/200, Loss: 2.1511\n",
      "Epoch 60/200, Loss: 2.3652\n",
      "Epoch 61/200, Loss: 2.3790\n",
      "Epoch 62/200, Loss: 2.2208\n",
      "Epoch 63/200, Loss: 2.2041\n",
      "Epoch 64/200, Loss: 2.1937\n",
      "Epoch 65/200, Loss: 2.0933\n",
      "Epoch 66/200, Loss: 2.2223\n",
      "Epoch 67/200, Loss: 2.1443\n",
      "Epoch 68/200, Loss: 2.1900\n",
      "Epoch 69/200, Loss: 2.0005\n",
      "Epoch 70/200, Loss: 2.1299\n",
      "Epoch 71/200, Loss: 1.8495\n",
      "Epoch 72/200, Loss: 2.0950\n",
      "Epoch 73/200, Loss: 2.0869\n",
      "Epoch 74/200, Loss: 2.0285\n",
      "Epoch 75/200, Loss: 1.9552\n",
      "Epoch 76/200, Loss: 2.0816\n",
      "Epoch 77/200, Loss: 1.9995\n",
      "Epoch 78/200, Loss: 2.3873\n",
      "Epoch 79/200, Loss: 2.0355\n",
      "Epoch 80/200, Loss: 2.1355\n",
      "Epoch 81/200, Loss: 1.9606\n",
      "Epoch 82/200, Loss: 2.1240\n",
      "Epoch 83/200, Loss: 1.9357\n",
      "Epoch 84/200, Loss: 2.1123\n",
      "Epoch 85/200, Loss: 2.3787\n",
      "Epoch 86/200, Loss: 2.1351\n",
      "Epoch 87/200, Loss: 2.0135\n",
      "Epoch 88/200, Loss: 2.1599\n",
      "Epoch 89/200, Loss: 1.9278\n",
      "Epoch 90/200, Loss: 1.9080\n",
      "Epoch 91/200, Loss: 1.9252\n",
      "Epoch 92/200, Loss: 1.9194\n",
      "Epoch 93/200, Loss: 2.0580\n",
      "Epoch 94/200, Loss: 1.9492\n",
      "Epoch 95/200, Loss: 1.8293\n",
      "Epoch 96/200, Loss: 1.9963\n",
      "Epoch 97/200, Loss: 1.8890\n",
      "Epoch 98/200, Loss: 1.8994\n",
      "Epoch 99/200, Loss: 1.8127\n",
      "Epoch 100/200, Loss: 2.0180\n",
      "Epoch 101/200, Loss: 1.8824\n",
      "Epoch 102/200, Loss: 1.8739\n",
      "Epoch 103/200, Loss: 1.8310\n",
      "Epoch 104/200, Loss: 1.7987\n",
      "Epoch 105/200, Loss: 1.8502\n",
      "Epoch 106/200, Loss: 1.9823\n",
      "Epoch 107/200, Loss: 2.1411\n",
      "Epoch 108/200, Loss: 1.7565\n",
      "Epoch 109/200, Loss: 1.8724\n",
      "Epoch 110/200, Loss: 2.0607\n",
      "Epoch 111/200, Loss: 1.8261\n",
      "Epoch 112/200, Loss: 1.7557\n",
      "Epoch 113/200, Loss: 1.7625\n",
      "Epoch 114/200, Loss: 1.7027\n",
      "Epoch 115/200, Loss: 1.7764\n",
      "Epoch 116/200, Loss: 1.7329\n",
      "Epoch 117/200, Loss: 2.0330\n",
      "Epoch 118/200, Loss: 2.0573\n",
      "Epoch 119/200, Loss: 1.8936\n",
      "Epoch 120/200, Loss: 1.7992\n",
      "Epoch 121/200, Loss: 1.6859\n",
      "Epoch 122/200, Loss: 1.6504\n",
      "Epoch 123/200, Loss: 2.0035\n",
      "Epoch 124/200, Loss: 1.6584\n",
      "Epoch 125/200, Loss: 1.7841\n",
      "Epoch 126/200, Loss: 1.6960\n",
      "Epoch 127/200, Loss: 1.8256\n",
      "Epoch 128/200, Loss: 1.6329\n",
      "Epoch 129/200, Loss: 1.8355\n",
      "Epoch 130/200, Loss: 1.6836\n",
      "Epoch 131/200, Loss: 1.7174\n",
      "Epoch 132/200, Loss: 1.6496\n",
      "Epoch 133/200, Loss: 1.6340\n",
      "Epoch 134/200, Loss: 1.8477\n",
      "Epoch 135/200, Loss: 1.8273\n",
      "Epoch 136/200, Loss: 1.8281\n",
      "Epoch 137/200, Loss: 1.6091\n",
      "Epoch 138/200, Loss: 1.5639\n",
      "Epoch 139/200, Loss: 1.5148\n",
      "Epoch 140/200, Loss: 1.7488\n",
      "Epoch 141/200, Loss: 1.7131\n",
      "Epoch 142/200, Loss: 1.5590\n",
      "Epoch 143/200, Loss: 1.8049\n",
      "Epoch 144/200, Loss: 1.5193\n",
      "Epoch 145/200, Loss: 1.7663\n",
      "Epoch 146/200, Loss: 1.6794\n",
      "Epoch 147/200, Loss: 1.7503\n",
      "Epoch 148/200, Loss: 1.7626\n",
      "Epoch 149/200, Loss: 1.7593\n",
      "Epoch 150/200, Loss: 1.6773\n",
      "Epoch 151/200, Loss: 1.6880\n",
      "Epoch 152/200, Loss: 1.7094\n",
      "Epoch 153/200, Loss: 1.8906\n",
      "Epoch 154/200, Loss: 1.4499\n",
      "Epoch 155/200, Loss: 1.5624\n",
      "Epoch 156/200, Loss: 1.8090\n",
      "Epoch 157/200, Loss: 1.5297\n",
      "Epoch 158/200, Loss: 1.6177\n",
      "Epoch 159/200, Loss: 1.6715\n",
      "Epoch 160/200, Loss: 1.4395\n",
      "Epoch 161/200, Loss: 1.3459\n",
      "Epoch 162/200, Loss: 1.4060\n",
      "Epoch 163/200, Loss: 1.6821\n",
      "Epoch 164/200, Loss: 1.5189\n",
      "Epoch 165/200, Loss: 1.4466\n",
      "Epoch 166/200, Loss: 1.4560\n",
      "Epoch 167/200, Loss: 1.9122\n",
      "Epoch 168/200, Loss: 1.9339\n",
      "Epoch 169/200, Loss: 1.9547\n",
      "Epoch 170/200, Loss: 1.6001\n",
      "Epoch 171/200, Loss: 1.6104\n",
      "Epoch 172/200, Loss: 1.8547\n",
      "Epoch 173/200, Loss: 1.5048\n",
      "Epoch 174/200, Loss: 1.6406\n",
      "Epoch 175/200, Loss: 1.6746\n",
      "Epoch 176/200, Loss: 1.4369\n",
      "Epoch 177/200, Loss: 1.5857\n",
      "Epoch 178/200, Loss: 1.5695\n",
      "Epoch 179/200, Loss: 1.9609\n",
      "Epoch 180/200, Loss: 1.7210\n",
      "Epoch 181/200, Loss: 1.5278\n",
      "Epoch 182/200, Loss: 1.4909\n",
      "Epoch 183/200, Loss: 1.6859\n",
      "Epoch 184/200, Loss: 1.6753\n",
      "Epoch 185/200, Loss: 1.5017\n",
      "Epoch 186/200, Loss: 1.7061\n",
      "Epoch 187/200, Loss: 1.5238\n",
      "Epoch 188/200, Loss: 1.6357\n",
      "Epoch 189/200, Loss: 2.0418\n",
      "Epoch 190/200, Loss: 1.7090\n",
      "Epoch 191/200, Loss: 1.6997\n",
      "Epoch 192/200, Loss: 1.7840\n",
      "Epoch 193/200, Loss: 1.4781\n",
      "Epoch 194/200, Loss: 1.6290\n",
      "Epoch 195/200, Loss: 1.8998\n",
      "Epoch 196/200, Loss: 1.6854\n",
      "Epoch 197/200, Loss: 1.5958\n",
      "Epoch 198/200, Loss: 1.6114\n",
      "Epoch 199/200, Loss: 1.4536\n",
      "Epoch 200/200, Loss: 1.5767\n",
      "Labels:      [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "Predictions: [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.88      0.96      0.91        15\n",
      "weighted avg       0.95      0.93      0.94        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 6, attention_layers = 4, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048)\n",
    "custom_model.train(epochs = 200, learning_rate = 1e-4)\n",
    "custom_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daghz\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.5425\n",
      "Epoch 2/100, Loss: 3.9409\n",
      "Epoch 3/100, Loss: 3.9304\n",
      "Epoch 4/100, Loss: 3.6035\n",
      "Epoch 5/100, Loss: 3.4470\n",
      "Epoch 6/100, Loss: 3.4411\n",
      "Epoch 7/100, Loss: 3.4681\n",
      "Epoch 8/100, Loss: 3.4516\n",
      "Epoch 9/100, Loss: 3.4571\n",
      "Epoch 10/100, Loss: 3.3716\n",
      "Epoch 11/100, Loss: 3.4621\n",
      "Epoch 12/100, Loss: 3.4398\n",
      "Epoch 13/100, Loss: 3.3875\n",
      "Epoch 14/100, Loss: 3.4761\n",
      "Epoch 15/100, Loss: 3.3380\n",
      "Epoch 16/100, Loss: 3.3131\n",
      "Epoch 17/100, Loss: 3.3555\n",
      "Epoch 18/100, Loss: 3.4014\n",
      "Epoch 19/100, Loss: 3.2036\n",
      "Epoch 20/100, Loss: 3.1749\n",
      "Epoch 21/100, Loss: 3.1235\n",
      "Epoch 22/100, Loss: 3.2479\n",
      "Epoch 23/100, Loss: 3.2620\n",
      "Epoch 24/100, Loss: 3.2958\n",
      "Epoch 25/100, Loss: 3.3307\n",
      "Epoch 26/100, Loss: 3.1613\n",
      "Epoch 27/100, Loss: 3.1352\n",
      "Epoch 28/100, Loss: 2.9909\n",
      "Epoch 29/100, Loss: 3.3164\n",
      "Epoch 30/100, Loss: 3.0837\n",
      "Epoch 31/100, Loss: 2.9993\n",
      "Epoch 32/100, Loss: 2.9874\n",
      "Epoch 33/100, Loss: 2.9325\n",
      "Epoch 34/100, Loss: 2.9317\n",
      "Epoch 35/100, Loss: 2.6441\n",
      "Epoch 36/100, Loss: 2.5901\n",
      "Epoch 37/100, Loss: 2.5286\n",
      "Epoch 38/100, Loss: 2.5213\n",
      "Epoch 39/100, Loss: 2.5787\n",
      "Epoch 40/100, Loss: 2.5383\n",
      "Epoch 41/100, Loss: 2.6157\n",
      "Epoch 42/100, Loss: 2.5421\n",
      "Epoch 43/100, Loss: 2.6629\n",
      "Epoch 44/100, Loss: 2.5799\n",
      "Epoch 45/100, Loss: 2.5661\n",
      "Epoch 46/100, Loss: 2.6048\n",
      "Epoch 47/100, Loss: 2.5264\n",
      "Epoch 48/100, Loss: 2.5344\n",
      "Epoch 49/100, Loss: 2.6174\n",
      "Epoch 50/100, Loss: 2.7070\n",
      "Epoch 51/100, Loss: 2.3935\n",
      "Epoch 52/100, Loss: 2.3373\n",
      "Epoch 53/100, Loss: 2.4134\n",
      "Epoch 54/100, Loss: 2.2521\n",
      "Epoch 55/100, Loss: 2.4342\n",
      "Epoch 56/100, Loss: 2.6158\n",
      "Epoch 57/100, Loss: 2.3898\n",
      "Epoch 58/100, Loss: 2.3145\n",
      "Epoch 59/100, Loss: 2.5245\n",
      "Epoch 60/100, Loss: 2.4300\n",
      "Epoch 61/100, Loss: 2.3844\n",
      "Epoch 62/100, Loss: 2.2922\n",
      "Epoch 63/100, Loss: 2.2825\n",
      "Epoch 64/100, Loss: 2.4995\n",
      "Epoch 65/100, Loss: 2.3717\n",
      "Epoch 66/100, Loss: 2.4796\n",
      "Epoch 67/100, Loss: 2.2115\n",
      "Epoch 68/100, Loss: 2.4217\n",
      "Epoch 69/100, Loss: 2.2015\n",
      "Epoch 70/100, Loss: 2.2526\n",
      "Epoch 71/100, Loss: 2.3798\n",
      "Epoch 72/100, Loss: 2.3771\n",
      "Epoch 73/100, Loss: 2.2250\n",
      "Epoch 74/100, Loss: 2.3817\n",
      "Epoch 75/100, Loss: 2.3137\n",
      "Epoch 76/100, Loss: 2.1650\n",
      "Epoch 77/100, Loss: 2.1087\n",
      "Epoch 78/100, Loss: 2.3564\n",
      "Epoch 79/100, Loss: 2.3782\n",
      "Epoch 80/100, Loss: 2.1758\n",
      "Epoch 81/100, Loss: 2.2060\n",
      "Epoch 82/100, Loss: 2.2119\n",
      "Epoch 83/100, Loss: 2.2176\n",
      "Epoch 84/100, Loss: 2.1025\n",
      "Epoch 85/100, Loss: 2.3466\n",
      "Epoch 86/100, Loss: 2.2222\n",
      "Epoch 87/100, Loss: 2.4822\n",
      "Epoch 88/100, Loss: 2.2878\n",
      "Epoch 89/100, Loss: 2.1865\n",
      "Epoch 90/100, Loss: 2.1425\n",
      "Epoch 91/100, Loss: 2.1181\n",
      "Epoch 92/100, Loss: 2.4676\n",
      "Epoch 93/100, Loss: 2.4134\n",
      "Epoch 94/100, Loss: 2.2523\n",
      "Epoch 95/100, Loss: 2.1070\n",
      "Epoch 96/100, Loss: 2.1494\n",
      "Epoch 97/100, Loss: 2.3168\n",
      "Epoch 98/100, Loss: 2.2262\n",
      "Epoch 99/100, Loss: 2.4279\n",
      "Epoch 100/100, Loss: 2.0830\n",
      "Labels:      [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Predictions: [1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.88      0.96      0.91        15\n",
      "weighted avg       0.95      0.93      0.94        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_model = Transformer(k_mers_type = 6, attention_layers = 6, batch_size = 32,  balancing_method = 'class_weights', embed_dim = 256, dim_feedforward = 2048)\n",
    "custom_model.train(epochs = 100, learning_rate = 1e-4)\n",
    "custom_model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
